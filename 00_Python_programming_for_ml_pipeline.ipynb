{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce4b88dd",
   "metadata": {},
   "source": [
    "# Python Programming for the Machine Learning Pipeline (Lecture Notebook)\n",
    "\n",
    "**Course:** Python Programming\n",
    "**Focus:** How core Python skills power each step of an ML workflow  \n",
    "\n",
    "This notebook emphasizes **programming practice** over deep ML theory.  \n",
    "We will follow a typical ML pipeline:\n",
    "\n",
    "1. Define strategy  \n",
    "2. Data collection / ingestion  \n",
    "3. Data preprocessing  \n",
    "4. Data modeling  \n",
    "5. Training and evaluation  \n",
    "6. Optimization  \n",
    "7. Deployment  \n",
    "8. Performance monitoring  \n",
    "\n",
    "You will see **two styles** of solutions where appropriate:\n",
    "- A more “manual Python” approach (loops, lists, basic NumPy)\n",
    "- A cleaner “library-first” approach (pandas, scikit-learn)\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Outcomes\n",
    "By the end of this notebook, you should be able to:\n",
    "1. Identify which Python tools match each ML pipeline step.\n",
    "2. Read and inspect data using pandas and scikit-learn datasets.\n",
    "3. Apply basic EDA and preprocessing with both manual and library approaches.\n",
    "4. Split datasets manually and with `train_test_split`.\n",
    "5. Train and evaluate a simple model using clean, reusable code.\n",
    "6. Compare manual tuning loops with `GridSearchCV`.\n",
    "7. Save and load models for simple deployment demos.\n",
    "8. Sketch a minimal monitoring workflow using Python."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59fde797",
   "metadata": {},
   "source": [
    "## 0. Imports and Setup\n",
    "\n",
    "We will use a small set of standard libraries:\n",
    "- `numpy` and `pandas` for data handling\n",
    "- `matplotlib` for basic visualization\n",
    "- `scikit-learn` for clean ML utilities\n",
    "\n",
    "The goal is not to memorize every function but to learn a reusable **pattern** for writing ML-ready Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772a7768",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "import joblib\n",
    "\n",
    "RNG = 42\n",
    "np.random.seed(RNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f8a7f0",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Define Strategy (Programming View)\n",
    "\n",
    "In real projects, “define strategy” means more than stating a goal.  \n",
    "From a programming standpoint, you want to make your intent **explicit** and **configurable**.\n",
    "\n",
    "Good habits include:\n",
    "- Defining constants for random seeds\n",
    "- Keeping dataset and model choices in a dictionary\n",
    "- Writing small helper functions for repeatable steps\n",
    "\n",
    "This avoids “magic numbers” scattered across your notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4594a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A simple configuration pattern\n",
    "CONFIG = {\n",
    "    \"random_state\": 42,\n",
    "    \"test_size\": 0.25,\n",
    "    \"knn_default_k\": 5,\n",
    "    \"grid_k_values\": [1, 3, 5, 7, 9]\n",
    "}\n",
    "\n",
    "CONFIG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f02b94e7",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Data Collection / Ingestion\n",
    "\n",
    "In practice, “data collection” often becomes **data ingestion** in Python:\n",
    "- Reading CSV files\n",
    "- Loading Excel sheets\n",
    "- Pulling from APIs or databases (covered later in the course)\n",
    "- Using benchmark datasets for learning\n",
    "\n",
    "For classroom-safe demonstrations, we will:\n",
    "1. Load the Iris dataset from scikit-learn.\n",
    "2. Convert it to a pandas DataFrame.\n",
    "3. Save it to CSV and read it back to simulate a real file workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3da58d",
   "metadata": {},
   "source": [
    "### 2.1 Load a Built-in Dataset (Iris)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3e77f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris(as_frame=True)\n",
    "df = iris.frame.copy()  # includes features + target\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c68c635",
   "metadata": {},
   "source": [
    "### 2.2 Simulate “Real-World” File Reading\n",
    "\n",
    "Even when using known datasets, it's helpful to practice file I/O."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80427be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to a local CSV (in the same working directory)\n",
    "csv_path = \"iris_demo.csv\"\n",
    "df.to_csv(csv_path, index=False)\n",
    "\n",
    "# Read it back\n",
    "df_csv = pd.read_csv(csv_path)\n",
    "\n",
    "print(\"Loaded from CSV:\", df_csv.shape)\n",
    "df_csv.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2747f48d",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Data Understanding / EDA (Programming Essentials)\n",
    "\n",
    "Before modeling, Python helps you answer:\n",
    "- What columns do we have?\n",
    "- Are there missing values?\n",
    "- What types are our features?\n",
    "- Are classes balanced?\n",
    "\n",
    "You don't need fancy plots at this stage.  \n",
    "Good **first-pass** EDA uses:\n",
    "- `head()`, `info()`, `describe()`\n",
    "- simple counts\n",
    "- lightweight plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134d7812",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_csv.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a51e73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_csv.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4ba585",
   "metadata": {},
   "source": [
    "### 3.1 Target Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df9902c",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_counts = df_csv[\"target\"].value_counts().sort_index()\n",
    "target_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f737d162",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "target_counts.plot(kind=\"bar\")\n",
    "plt.title(\"Iris Target Distribution\")\n",
    "plt.xlabel(\"Class label\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617c0980",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Data Preprocessing: Manual vs Library\n",
    "\n",
    "Many ML errors come from weak preprocessing.  \n",
    "Programming-wise, you should recognize **when to write a loop** and when to use a **library**.\n",
    "\n",
    "We will demonstrate:\n",
    "1. Handling missing values (simulated)\n",
    "2. Scaling numerical features\n",
    "3. Encoding categorical features (small synthetic example)\n",
    "\n",
    "The goal is to learn patterns you can reuse across datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab4b9844",
   "metadata": {},
   "source": [
    "### 4.1 Simulate Missing Values\n",
    "\n",
    "The Iris dataset is clean.  \n",
    "To practice preprocessing, we'll intentionally insert a few missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14ba061",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_miss = df_csv.copy()\n",
    "rng = np.random.default_rng(RNG)\n",
    "\n",
    "# Randomly set ~2% of numeric cells to NaN\n",
    "num_cols = iris.feature_names\n",
    "mask = rng.random((df_miss.shape[0], len(num_cols))) < 0.02\n",
    "\n",
    "for j, col in enumerate(num_cols):\n",
    "    df_miss.loc[mask[:, j], col] = np.nan\n",
    "\n",
    "df_miss.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e1f81d",
   "metadata": {},
   "source": [
    "### 4.2 Missing Value Handling — Manual Loop Approach\n",
    "\n",
    "We'll fill missing numeric values using the **column mean** manually.  \n",
    "This approach is fine for learning, but not ideal for production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8661bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_manual = df_miss.copy()\n",
    "\n",
    "for col in num_cols:\n",
    "    mean_val = df_manual[col].mean()\n",
    "    # loop-style fill\n",
    "    values = []\n",
    "    for v in df_manual[col].tolist():\n",
    "        values.append(mean_val if pd.isna(v) else v)\n",
    "    df_manual[col] = values\n",
    "\n",
    "df_manual.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e147db5f",
   "metadata": {},
   "source": [
    "### 4.3 Missing Value Handling — pandas / sklearn Approach\n",
    "\n",
    "Here is the cleaner “library-first” version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737971bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pandas = df_miss.copy()\n",
    "df_pandas[num_cols] = df_pandas[num_cols].fillna(df_pandas[num_cols].mean())\n",
    "\n",
    "df_pandas.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54286cc3",
   "metadata": {},
   "source": [
    "### 4.4 Scaling — Manual Standardization\n",
    "\n",
    "Standardization transforms each feature to:\n",
    "\n",
    "\\[\n",
    "z = \\frac{x - \\mu}{\\sigma}\n",
    "\\]\n",
    "\n",
    "We'll do this manually to see the mechanics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b98d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_num = df_pandas[num_cols].to_numpy()\n",
    "\n",
    "mu = X_num.mean(axis=0)\n",
    "sigma = X_num.std(axis=0, ddof=0)\n",
    "\n",
    "X_scaled_manual = (X_num - mu) / sigma\n",
    "\n",
    "X_scaled_manual[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff8b66a0",
   "metadata": {},
   "source": [
    "### 4.5 Scaling — StandardScaler\n",
    "\n",
    "This is the preferred approach for clean pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b7183b",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_scaled_lib = scaler.fit_transform(df_pandas[num_cols])\n",
    "\n",
    "X_scaled_lib[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4387a0e",
   "metadata": {},
   "source": [
    "### 4.6 Encoding Categorical Features (Small Synthetic Demo)\n",
    "\n",
    "Iris is purely numeric.  \n",
    "To practice encoding, let's create a tiny toy dataset with a categorical column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc39edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "toy = pd.DataFrame({\n",
    "    \"height_cm\": [160, 170, 165, 180, 175],\n",
    "    \"diet_type\": [\"A\", \"B\", \"A\", \"C\", \"B\"],\n",
    "    \"target\": [0, 1, 0, 1, 1]\n",
    "})\n",
    "\n",
    "toy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b5db39",
   "metadata": {},
   "source": [
    "#### Manual Encoding with a Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1f270a",
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_manual = toy.copy()\n",
    "mapping = {\"A\": 0, \"B\": 1, \"C\": 2}\n",
    "toy_manual[\"diet_type_enc\"] = toy_manual[\"diet_type\"].map(mapping)\n",
    "\n",
    "toy_manual"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea0ae28",
   "metadata": {},
   "source": [
    "#### OneHotEncoder (Preferred for Many Cases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85ba9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe = OneHotEncoder(sparse_output=False, handle_unknown=\"ignore\")\n",
    "diet_ohe = ohe.fit_transform(toy[[\"diet_type\"]])\n",
    "\n",
    "diet_cols = [f\"diet_{c}\" for c in ohe.categories_[0]]\n",
    "diet_df = pd.DataFrame(diet_ohe, columns=diet_cols)\n",
    "\n",
    "pd.concat([toy[[\"height_cm\"]], diet_df, toy[[\"target\"]]], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83267a74",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Train/Test Split — Manual vs Library\n",
    "\n",
    "In ML, we split data to estimate how well our model generalizes.  \n",
    "You *can* implement splitting manually, but in practice we prefer:\n",
    "\n",
    "- `train_test_split` for simplicity and correctness\n",
    "- Stratified splitting when class balance matters\n",
    "\n",
    "We will show both approaches using the Iris dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "178ebaf1",
   "metadata": {},
   "source": [
    "### 5.1 Prepare Features and Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15934ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ready = df_pandas.copy()  # cleaned version\n",
    "\n",
    "X = df_ready[num_cols].to_numpy()\n",
    "y = df_ready[\"target\"].to_numpy()\n",
    "\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63dd8f7",
   "metadata": {},
   "source": [
    "### 5.2 Manual Split with Loops\n",
    "\n",
    "This demonstrates the idea of:\n",
    "- shuffling indices\n",
    "- slicing into train/test\n",
    "- building lists/arrays\n",
    "\n",
    "This is educational—but not recommended for real projects unless you have a special constraint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03810b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual split\n",
    "indices = list(range(len(X)))\n",
    "\n",
    "rng = np.random.default_rng(CONFIG[\"random_state\"])\n",
    "rng.shuffle(indices)\n",
    "\n",
    "split_point = int(len(indices) * (1 - CONFIG[\"test_size\"]))\n",
    "\n",
    "train_idx = indices[:split_point]\n",
    "test_idx = indices[split_point:]\n",
    "\n",
    "# Loop-based assembly\n",
    "X_train_manual, y_train_manual = [], []\n",
    "X_test_manual, y_test_manual = [], []\n",
    "\n",
    "for i in train_idx:\n",
    "    X_train_manual.append(X[i])\n",
    "    y_train_manual.append(y[i])\n",
    "\n",
    "for i in test_idx:\n",
    "    X_test_manual.append(X[i])\n",
    "    y_test_manual.append(y[i])\n",
    "\n",
    "X_train_manual = np.array(X_train_manual)\n",
    "y_train_manual = np.array(y_train_manual)\n",
    "X_test_manual = np.array(X_test_manual)\n",
    "y_test_manual = np.array(y_test_manual)\n",
    "\n",
    "X_train_manual.shape, X_test_manual.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f84cbe1",
   "metadata": {},
   "source": [
    "### 5.3 Library Split with train_test_split\n",
    "\n",
    "Cleaner and safer, with optional stratification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8d7d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=CONFIG[\"test_size\"],\n",
    "    random_state=CONFIG[\"random_state\"],\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c4ea8c",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Data Modeling (Programming View)\n",
    "\n",
    "At this stage, Python shifts from data manipulation to object-oriented usage of ML estimators.  \n",
    "With scikit-learn, models follow a consistent API:\n",
    "\n",
    "- `model = Estimator(**params)`\n",
    "- `model.fit(X_train, y_train)`\n",
    "- `model.predict(X_test)`\n",
    "\n",
    "This uniformity is one reason scikit-learn is so effective for learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3fba5f",
   "metadata": {},
   "source": [
    "### 6.1 Train a Simple KNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f66a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=CONFIG[\"knn_default_k\"])\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "y_pred = knn.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86e29f6",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Training & Evaluation — Manual vs Library Metrics\n",
    "\n",
    "Evaluation is both a programming and scientific habit.  \n",
    "We'll compute accuracy in two ways:\n",
    "1. Manual computation\n",
    "2. Using `accuracy_score`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573c5f2a",
   "metadata": {},
   "source": [
    "### 7.1 Manual Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7870b20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "for yt, yp in zip(y_test, y_pred):\n",
    "    if yt == yp:\n",
    "        correct += 1\n",
    "\n",
    "acc_manual = correct / len(y_test)\n",
    "acc_manual"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c9b948c",
   "metadata": {},
   "source": [
    "### 7.2 accuracy_score + classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9414bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_lib = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"Manual accuracy:\", acc_manual)\n",
    "print(\"Library accuracy:\", acc_lib)\n",
    "print()\n",
    "print(classification_report(y_test, y_pred, target_names=iris.target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b375bb",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Optimization — Manual Hyperparameter Loop vs GridSearchCV\n",
    "\n",
    "Optimization in ML often means choosing hyperparameters that improve generalization.  \n",
    "Programming-wise, this is a great place to compare:\n",
    "\n",
    "- **Manual loops** for transparency\n",
    "- **Library tools** for reliability and speed\n",
    "\n",
    "We'll tune \\(k\\) for KNN."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c1d651",
   "metadata": {},
   "source": [
    "### 8.1 Manual Loop Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dde7629",
   "metadata": {},
   "outputs": [],
   "source": [
    "k_values = CONFIG[\"grid_k_values\"]\n",
    "\n",
    "manual_rows = []\n",
    "for k in k_values:\n",
    "    model = KNeighborsClassifier(n_neighbors=k)\n",
    "    model.fit(X_train, y_train)\n",
    "    pred = model.predict(X_test)\n",
    "    acc = accuracy_score(y_test, pred)\n",
    "    manual_rows.append({\"k\": k, \"accuracy\": acc})\n",
    "\n",
    "manual_results = pd.DataFrame(manual_rows).sort_values(\"accuracy\", ascending=False)\n",
    "manual_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c02c454",
   "metadata": {},
   "source": [
    "### 8.2 GridSearchCV\n",
    "\n",
    "GridSearchCV automates the search using cross-validation.  \n",
    "It is typically more reliable than tuning on the test set directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f40cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\"n_neighbors\": k_values}\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    KNeighborsClassifier(),\n",
    "    param_grid=param_grid,\n",
    "    cv=5,\n",
    "    scoring=\"accuracy\"\n",
    ")\n",
    "\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters:\", grid.best_params_)\n",
    "print(\"Best CV score:\", grid.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c1e2dc",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Preprocessing + Modeling with Pipelines\n",
    "\n",
    "When projects grow, you should avoid scattered preprocessing steps.  \n",
    "A **Pipeline** helps you:\n",
    "- keep steps in order\n",
    "- prevent data leakage\n",
    "- simplify experimentation\n",
    "\n",
    "We'll build a minimal numeric pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4acd3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline(steps=[\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"model\", KNeighborsClassifier(n_neighbors=5))\n",
    "])\n",
    "\n",
    "# Here we go back to DataFrame-style features for clarity\n",
    "X_df = df_ready[num_cols]\n",
    "y_series = df_ready[\"target\"]\n",
    "\n",
    "X_train_df, X_test_df, y_train_s, y_test_s = train_test_split(\n",
    "    X_df, y_series,\n",
    "    test_size=CONFIG[\"test_size\"],\n",
    "    random_state=CONFIG[\"random_state\"],\n",
    "    stratify=y_series\n",
    ")\n",
    "\n",
    "pipe.fit(X_train_df, y_train_s)\n",
    "pred = pipe.predict(X_test_df)\n",
    "\n",
    "print(\"Pipeline accuracy:\", accuracy_score(y_test_s, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5a9f9e",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. Deployment (Intro-Level Programming Demo)\n",
    "\n",
    "Deployment can mean many things—from a simple script to a production service.  \n",
    "For a first programming demonstration, we focus on:\n",
    "\n",
    "- saving a trained model to disk\n",
    "- loading it later\n",
    "- using it in a small prediction function\n",
    "\n",
    "This introduces reproducibility and modular code design."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6594e49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the pipeline\n",
    "model_path = \"iris_knn_pipeline.joblib\"\n",
    "joblib.dump(pipe, model_path)\n",
    "\n",
    "# Load it back\n",
    "loaded_pipe = joblib.load(model_path)\n",
    "\n",
    "# Predict the first 3 rows of the test set\n",
    "loaded_pipe.predict(X_test_df.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcadc07a",
   "metadata": {},
   "source": [
    "---\n",
    "## 11. Performance Monitoring (Simple Classroom Pattern)\n",
    "\n",
    "In professional ML, monitoring ensures that models remain reliable after deployment.  \n",
    "A minimal monitoring script might:\n",
    "\n",
    "- collect new data batches\n",
    "- generate predictions\n",
    "- compare against ground truth when available\n",
    "- log metrics over time\n",
    "\n",
    "Below is a simplified example that logs accuracy for a “new batch” (simulated)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c483b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate a \"new batch\" by sampling from the existing test set\n",
    "new_batch_X = X_test_df.sample(20, random_state=RNG)\n",
    "new_batch_y = y_test_s.loc[new_batch_X.index]\n",
    "\n",
    "new_pred = loaded_pipe.predict(new_batch_X)\n",
    "new_acc = accuracy_score(new_batch_y, new_pred)\n",
    "\n",
    "log_row = {\n",
    "    \"timestamp\": pd.Timestamp.now(),\n",
    "    \"batch_size\": len(new_batch_X),\n",
    "    \"accuracy\": new_acc\n",
    "}\n",
    "\n",
    "log_path = \"monitoring_log.csv\"\n",
    "\n",
    "# Append or create\n",
    "try:\n",
    "    existing = pd.read_csv(log_path)\n",
    "    updated = pd.concat([existing, pd.DataFrame([log_row])], ignore_index=True)\n",
    "except FileNotFoundError:\n",
    "    updated = pd.DataFrame([log_row])\n",
    "\n",
    "updated.to_csv(log_path, index=False)\n",
    "updated.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21482ad6",
   "metadata": {},
   "source": [
    "---\n",
    "## 12. Summary: Python Skills Mapped to the ML Pipeline\n",
    "\n",
    "**Define strategy**\n",
    "- Use configuration dictionaries, constants, and helper functions.\n",
    "\n",
    "**Data collection / ingestion**\n",
    "- Read CSV/Excel; load benchmark datasets; validate schema.\n",
    "\n",
    "**Data understanding / EDA**\n",
    "- Use pandas inspection methods and simple plots.\n",
    "\n",
    "**Preprocessing**\n",
    "- Handle missingness, encode categories, scale numerics.\n",
    "- Prefer reusable transformers and pipelines.\n",
    "\n",
    "**Splitting**\n",
    "- Understand manual splitting logic, but use `train_test_split`.\n",
    "\n",
    "**Modeling**\n",
    "- Learn the estimator API: `fit`, `predict`, `score`.\n",
    "\n",
    "**Training & evaluation**\n",
    "- Compute metrics manually to understand them, then use library tools.\n",
    "\n",
    "**Optimization**\n",
    "- Start with simple loops; progress to `GridSearchCV`.\n",
    "\n",
    "**Deployment**\n",
    "- Save/load models with `joblib`; wrap prediction into functions.\n",
    "\n",
    "**Monitoring**\n",
    "- Log batch metrics; watch for drift in real systems.\n",
    "\n",
    "The big idea: strong ML work depends on **strong Python fundamentals** plus disciplined use of libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b09e515c",
   "metadata": {},
   "source": [
    "---\n",
    "## Suggested Student Practice (Optional)\n",
    "\n",
    "1. Replace KNN with Logistic Regression and repeat the manual vs library comparisons.  \n",
    "2. Create a small synthetic dataset with noise and observe how accuracy changes with different splits.  \n",
    "3. Extend the monitoring log to also record:\n",
    "   - class distribution in new batches\n",
    "   - predicted probability summaries (if using probabilistic models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b70b329",
   "metadata": {},
   "source": [
    "## References (Programming-Oriented)\n",
    "- Géron, A. *Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow*.  \n",
    "- VanderPlas, J. *Python Data Science Handbook*.  \n",
    "- McKinney, W. *Python for Data Analysis*.  \n",
    "- James, G., Witten, D., Hastie, T., & Tibshirani, R. *An Introduction to Statistical Learning*.  \n",
    "- scikit-learn, pandas, and NumPy official documentation."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
