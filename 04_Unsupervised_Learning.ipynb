{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised Learning Models\n",
    "## Lecture Notebook Part 4 of 6\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "1. Understand unsupervised learning and its applications\n",
    "2. Implement and evaluate clustering algorithms (K-Means, Hierarchical, DBSCAN)\n",
    "3. Understand dimensionality reduction techniques (PCA, t-SNE)\n",
    "4. Evaluate unsupervised learning models\n",
    "5. Choose appropriate unsupervised learning methods for different problems\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import silhouette_score, adjusted_rand_score\n",
    "from sklearn.datasets import make_blobs, make_moons\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(\"Ready to explore unsupervised learning!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1: K-Means Clustering\n",
    "\n",
    "Demonstrating K-Means clustering with different k values:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sample data with clear clusters\n",
    "X_kmeans, y_true = make_blobs(n_samples=300, centers=4, n_features=2, \n",
    "                               random_state=42, cluster_std=0.60)\n",
    "\n",
    "# Visualize true clusters\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.scatter(X_kmeans[:, 0], X_kmeans[:, 1], c=y_true, cmap='viridis', s=50, alpha=0.6)\n",
    "plt.title('True Clusters')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# K-Means with different k values\n",
    "k_values = [2, 4, 6]\n",
    "for idx, k in enumerate(k_values):\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    y_pred = kmeans.fit_predict(X_kmeans)\n",
    "    \n",
    "    plt.subplot(1, 3, idx+2)\n",
    "    plt.scatter(X_kmeans[:, 0], X_kmeans[:, 1], c=y_pred, cmap='viridis', s=50, alpha=0.6)\n",
    "    plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1],\n",
    "               c='red', marker='x', s=200, linewidths=3, label='Centroids')\n",
    "    plt.title(f'K-Means (k={k})\\nSilhouette: {silhouette_score(X_kmeans, y_pred):.3f}')\n",
    "    plt.xlabel('Feature 1')\n",
    "    plt.ylabel('Feature 2')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Elbow Method\n",
    "inertias = []\n",
    "k_range = range(1, 11)\n",
    "for k in k_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    kmeans.fit(X_kmeans)\n",
    "    inertias.append(kmeans.inertia_)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(k_range, inertias, 'bo-', linewidth=2, markersize=8)\n",
    "plt.xlabel('Number of Clusters (k)')\n",
    "plt.ylabel('Within-Cluster Sum of Squares (Inertia)')\n",
    "plt.title('Elbow Method for Optimal k')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.axvline(x=4, color='r', linestyle='--', label='Optimal k=4')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Silhouette Analysis\n",
    "silhouette_scores = []\n",
    "for k in k_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    y_pred = kmeans.fit_predict(X_kmeans)\n",
    "    silhouette_scores.append(silhouette_score(X_kmeans, y_pred))\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(k_range, silhouette_scores, 'go-', linewidth=2, markersize=8)\n",
    "plt.xlabel('Number of Clusters (k)')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.title('Silhouette Analysis')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.axvline(x=4, color='r', linestyle='--', label='Optimal k=4')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nOptimal k based on silhouette score: {k_range[np.argmax(silhouette_scores)]}\")\n",
    "print(f\"Best silhouette score: {max(silhouette_scores):.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: Hierarchical Clustering\n",
    "\n",
    "Demonstrating hierarchical clustering with dendrogram:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sample data\n",
    "X_hier, y_hier = make_blobs(n_samples=50, centers=3, n_features=2, \n",
    "                            random_state=42, cluster_std=0.8)\n",
    "\n",
    "# Create dendrogram\n",
    "linkage_types = ['ward', 'complete', 'average']\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for idx, linkage_type in enumerate(linkage_types):\n",
    "    # Perform hierarchical clustering\n",
    "    linkage_matrix = linkage(X_hier, method=linkage_type)\n",
    "    \n",
    "    # Plot dendrogram\n",
    "    axes[idx].set_title(f'{linkage_type.capitalize()} Linkage')\n",
    "    dendrogram(linkage_matrix, ax=axes[idx], leaf_rotation=90, leaf_font_size=8)\n",
    "    axes[idx].set_xlabel('Sample Index')\n",
    "    axes[idx].set_ylabel('Distance')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Compare clustering results\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for idx, linkage_type in enumerate(linkage_types):\n",
    "    hierarchical = AgglomerativeClustering(n_clusters=3, linkage=linkage_type)\n",
    "    y_pred = hierarchical.fit_predict(X_hier)\n",
    "    \n",
    "    axes[idx].scatter(X_hier[:, 0], X_hier[:, 1], c=y_pred, cmap='viridis', s=50, alpha=0.6)\n",
    "    axes[idx].set_title(f'{linkage_type.capitalize()} Linkage\\nSilhouette: {silhouette_score(X_hier, y_pred):.3f}')\n",
    "    axes[idx].set_xlabel('Feature 1')\n",
    "    axes[idx].set_ylabel('Feature 2')\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nObservations:\")\n",
    "print(\"- Ward linkage minimizes within-cluster variance (similar to K-Means)\")\n",
    "print(\"- Complete linkage creates compact clusters\")\n",
    "print(\"- Average linkage is a compromise between single and complete\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3: DBSCAN Clustering\n",
    "\n",
    "Demonstrating DBSCAN for non-spherical clusters:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate non-spherical clusters (moons)\n",
    "X_dbscan, y_dbscan_true = make_moons(n_samples=300, noise=0.1, random_state=42)\n",
    "\n",
    "# Compare K-Means vs DBSCAN\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# True clusters\n",
    "axes[0].scatter(X_dbscan[:, 0], X_dbscan[:, 1], c=y_dbscan_true, cmap='viridis', s=50, alpha=0.6)\n",
    "axes[0].set_title('True Clusters (Moons)')\n",
    "axes[0].set_xlabel('Feature 1')\n",
    "axes[0].set_ylabel('Feature 2')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# K-Means (struggles with non-spherical)\n",
    "kmeans_moons = KMeans(n_clusters=2, random_state=42, n_init=10)\n",
    "y_kmeans = kmeans_moons.fit_predict(X_dbscan)\n",
    "axes[1].scatter(X_dbscan[:, 0], X_dbscan[:, 1], c=y_kmeans, cmap='viridis', s=50, alpha=0.6)\n",
    "axes[1].scatter(kmeans_moons.cluster_centers_[:, 0], kmeans_moons.cluster_centers_[:, 1],\n",
    "               c='red', marker='x', s=200, linewidths=3)\n",
    "axes[1].set_title(f'K-Means\\nARI: {adjusted_rand_score(y_dbscan_true, y_kmeans):.3f}')\n",
    "axes[1].set_xlabel('Feature 1')\n",
    "axes[1].set_ylabel('Feature 2')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# DBSCAN (handles non-spherical)\n",
    "dbscan = DBSCAN(eps=0.3, min_samples=10)\n",
    "y_dbscan = dbscan.fit_predict(X_dbscan)\n",
    "n_clusters = len(set(y_dbscan)) - (1 if -1 in y_dbscan else 0)\n",
    "n_noise = list(y_dbscan).count(-1)\n",
    "\n",
    "axes[2].scatter(X_dbscan[:, 0], X_dbscan[:, 1], c=y_dbscan, cmap='viridis', s=50, alpha=0.6)\n",
    "axes[2].set_title(f'DBSCAN\\nClusters: {n_clusters}, Noise: {n_noise}\\nARI: {adjusted_rand_score(y_dbscan_true, y_dbscan):.3f}')\n",
    "axes[2].set_xlabel('Feature 1')\n",
    "axes[2].set_ylabel('Feature 2')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Test different DBSCAN parameters\n",
    "eps_values = [0.2, 0.3, 0.5]\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for idx, eps in enumerate(eps_values):\n",
    "    dbscan = DBSCAN(eps=eps, min_samples=10)\n",
    "    y_pred = dbscan.fit_predict(X_dbscan)\n",
    "    n_clusters = len(set(y_pred)) - (1 if -1 in y_pred else 0)\n",
    "    n_noise = list(y_pred).count(-1)\n",
    "    \n",
    "    axes[idx].scatter(X_dbscan[:, 0], X_dbscan[:, 1], c=y_pred, cmap='viridis', s=50, alpha=0.6)\n",
    "    axes[idx].set_title(f'DBSCAN (eps={eps})\\nClusters: {n_clusters}, Noise: {n_noise}')\n",
    "    axes[idx].set_xlabel('Feature 1')\n",
    "    axes[idx].set_ylabel('Feature 2')\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nObservations:\")\n",
    "print(\"- K-Means struggles with non-spherical clusters (assumes spherical)\")\n",
    "print(\"- DBSCAN can find clusters of arbitrary shape\")\n",
    "print(\"- DBSCAN automatically identifies outliers as noise\")\n",
    "print(\"- eps parameter controls cluster size and density\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 4: Principal Component Analysis (PCA)\n",
    "\n",
    "Demonstrating PCA for dimensionality reduction:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate high-dimensional data\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "X_pca, y_pca = make_classification(n_samples=200, n_features=10, n_informative=5,\n",
    "                                   n_redundant=3, n_clusters_per_class=1,\n",
    "                                   random_state=42)\n",
    "\n",
    "# Standardize data (important for PCA)\n",
    "scaler_pca = StandardScaler()\n",
    "X_pca_scaled = scaler_pca.fit_transform(X_pca)\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA()\n",
    "X_pca_transformed = pca.fit_transform(X_pca_scaled)\n",
    "\n",
    "# Variance explained\n",
    "cumulative_variance = np.cumsum(pca.explained_variance_ratio_)\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Scree plot\n",
    "axes[0].bar(range(1, 11), pca.explained_variance_ratio_, alpha=0.7, label='Individual')\n",
    "axes[0].plot(range(1, 11), cumulative_variance, 'ro-', label='Cumulative')\n",
    "axes[0].axhline(y=0.95, color='g', linestyle='--', label='95% threshold')\n",
    "axes[0].set_xlabel('Principal Component')\n",
    "axes[0].set_ylabel('Variance Explained')\n",
    "axes[0].set_title('Scree Plot')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2D projection\n",
    "pca_2d = PCA(n_components=2)\n",
    "X_pca_2d = pca_2d.fit_transform(X_pca_scaled)\n",
    "axes[1].scatter(X_pca_2d[:, 0], X_pca_2d[:, 1], c=y_pca, cmap='viridis', s=50, alpha=0.6)\n",
    "axes[1].set_xlabel(f'PC1 ({pca_2d.explained_variance_ratio_[0]:.2%} variance)')\n",
    "axes[1].set_ylabel(f'PC2 ({pca_2d.explained_variance_ratio_[1]:.2%} variance)')\n",
    "axes[1].set_title('2D PCA Projection')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Cumulative variance\n",
    "axes[2].plot(range(1, 11), cumulative_variance, 'bo-', linewidth=2, markersize=8)\n",
    "axes[2].axhline(y=0.95, color='r', linestyle='--', label='95% threshold')\n",
    "axes[2].set_xlabel('Number of Components')\n",
    "axes[2].set_ylabel('Cumulative Variance Explained')\n",
    "axes[2].set_title('Cumulative Variance Explained')\n",
    "axes[2].legend()\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Find number of components for 95% variance\n",
    "n_components_95 = np.argmax(cumulative_variance >= 0.95) + 1\n",
    "print(f\"\\nNumber of components explaining 95% variance: {n_components_95}\")\n",
    "print(f\"Variance explained by first {n_components_95} components: {cumulative_variance[n_components_95-1]:.2%}\")\n",
    "print(f\"\\nOriginal dimensions: {X_pca_scaled.shape[1]}\")\n",
    "print(f\"Reduced dimensions (95% variance): {n_components_95}\")\n",
    "print(f\"Dimensionality reduction: {(1 - n_components_95/X_pca_scaled.shape[1])*100:.1f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 5: t-SNE Visualization\n",
    "\n",
    "Demonstrating t-SNE for non-linear dimensionality reduction:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate complex high-dimensional data\n",
    "X_tsne, y_tsne = make_classification(n_samples=300, n_features=20, n_informative=10,\n",
    "                                     n_redundant=5, n_clusters_per_class=1,\n",
    "                                     random_state=42)\n",
    "\n",
    "# Standardize\n",
    "scaler_tsne = StandardScaler()\n",
    "X_tsne_scaled = scaler_tsne.fit_transform(X_tsne)\n",
    "\n",
    "# Compare PCA vs t-SNE\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# PCA 2D\n",
    "pca_tsne = PCA(n_components=2, random_state=42)\n",
    "X_pca_2d_tsne = pca_tsne.fit_transform(X_tsne_scaled)\n",
    "axes[0].scatter(X_pca_2d_tsne[:, 0], X_pca_2d_tsne[:, 1], c=y_tsne, cmap='viridis', s=50, alpha=0.6)\n",
    "axes[0].set_xlabel(f'PC1 ({pca_tsne.explained_variance_ratio_[0]:.2%} variance)')\n",
    "axes[0].set_ylabel(f'PC2 ({pca_tsne.explained_variance_ratio_[1]:.2%} variance)')\n",
    "axes[0].set_title('PCA (Linear)')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# t-SNE 2D\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=30)\n",
    "X_tsne_2d = tsne.fit_transform(X_tsne_scaled)\n",
    "axes[1].scatter(X_tsne_2d[:, 0], X_tsne_2d[:, 1], c=y_tsne, cmap='viridis', s=50, alpha=0.6)\n",
    "axes[1].set_xlabel('t-SNE 1')\n",
    "axes[1].set_ylabel('t-SNE 2')\n",
    "axes[1].set_title('t-SNE (Non-linear)')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Test different perplexity values\n",
    "perplexities = [5, 30, 50]\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for idx, perp in enumerate(perplexities):\n",
    "    tsne = TSNE(n_components=2, random_state=42, perplexity=perp)\n",
    "    X_tsne_perp = tsne.fit_transform(X_tsne_scaled)\n",
    "    \n",
    "    axes[idx].scatter(X_tsne_perp[:, 0], X_tsne_perp[:, 1], c=y_tsne, cmap='viridis', s=50, alpha=0.6)\n",
    "    axes[idx].set_xlabel('t-SNE 1')\n",
    "    axes[idx].set_ylabel('t-SNE 2')\n",
    "    axes[idx].set_title(f't-SNE (perplexity={perp})')\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nObservations:\")\n",
    "print(\"- PCA preserves global structure but may miss non-linear patterns\")\n",
    "print(\"- t-SNE preserves local neighborhoods, revealing cluster structure\")\n",
    "print(\"- t-SNE is excellent for visualization but computationally expensive\")\n",
    "print(\"- Perplexity controls balance between local and global structure\")\n",
    "print(\"- t-SNE results can vary between runs (stochastic)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Introduction to Unsupervised Learning\n",
    "\n",
    "### What is Unsupervised Learning?\n",
    "\n",
    "**Unsupervised learning** is a type of machine learning where algorithms discover hidden patterns, structures, or relationships in data without labeled examples. Unlike supervised learning where we provide the algorithm with correct answers (labels) during training, unsupervised learning algorithms must find patterns on their own by exploring the data.\n",
    "\n",
    "The \"unsupervised\" aspect means there's no teacher providing correct answers - the algorithm must learn from the data's inherent structure. This makes unsupervised learning more challenging but also more flexible, as it can discover patterns that humans might not have anticipated.\n",
    "\n",
    "Unsupervised learning is particularly valuable for exploratory data analysis, where the goal is to understand the data rather than make specific predictions. It's often used as a preprocessing step before applying supervised learning, or when labeled data is expensive or impossible to obtain.\n",
    "\n",
    "### Key Characteristics\n",
    "\n",
    "**No Labels**: Unsupervised algorithms work with unlabeled data, meaning they don't have target variables to predict. The algorithm must infer structure from the features alone.\n",
    "\n",
    "**Pattern Discovery**: The focus is on finding inherent structures, relationships, or groupings in the data. This includes identifying clusters, reducing dimensionality, or finding associations.\n",
    "\n",
    "**Exploratory Analysis**: Unsupervised learning is often used for data exploration and understanding. It helps answer questions like \"What groups exist in my data?\" or \"What are the main patterns?\"\n",
    "\n",
    "**Dimensionality Reduction**: Many unsupervised techniques reduce the number of features while preserving important information, making data easier to visualize and process.\n",
    "\n",
    "### Common Types of Unsupervised Learning\n",
    "\n",
    "**Clustering**: Grouping similar data points together based on their features. The goal is to find natural groupings in the data where points within a group are more similar to each other than to points in other groups. Examples include customer segmentation, image segmentation, and gene sequence analysis.\n",
    "\n",
    "**Dimensionality Reduction**: Reducing the number of features while preserving as much information as possible. This helps with visualization, removes noise, and can improve performance of downstream machine learning tasks. Principal Component Analysis (PCA) and t-SNE are common techniques.\n",
    "\n",
    "**Association Rules**: Finding relationships between variables, such as \"customers who buy X also tend to buy Y.\" Market basket analysis is a classic application.\n",
    "\n",
    "**Anomaly Detection**: Identifying unusual patterns or outliers in data. While sometimes considered a separate category, it's often approached using unsupervised methods.\n",
    "\n",
    "### Uses and Applications\n",
    "\n",
    "Unsupervised learning has numerous practical applications:\n",
    "\n",
    "- **Customer Segmentation**: Grouping customers based on purchasing behavior, demographics, or preferences to enable targeted marketing strategies.\n",
    "\n",
    "- **Image Compression and Feature Extraction**: Reducing image dimensionality while preserving important visual information, or extracting meaningful features for downstream tasks.\n",
    "\n",
    "- **Recommendation Systems**: Finding similar users or items to make personalized recommendations, even without explicit ratings.\n",
    "\n",
    "- **Data Visualization**: Reducing high-dimensional data to 2D or 3D for visualization and exploration, helping humans understand complex datasets.\n",
    "\n",
    "- **Anomaly Detection**: Identifying fraudulent transactions, network intrusions, or manufacturing defects by finding data points that don't fit normal patterns.\n",
    "\n",
    "- **Market Basket Analysis**: Discovering which products are frequently bought together to optimize store layouts and cross-selling strategies.\n",
    "\n",
    "- **Gene Sequence Analysis**: Clustering genes with similar expression patterns to understand biological functions and relationships.\n",
    "\n",
    "- **Topic Modeling**: Discovering hidden topics in large collections of documents without manual labeling.\n",
    "\n",
    "The exploratory and pattern-discovery nature of unsupervised learning makes it an essential tool for understanding data before applying supervised learning techniques.\n",
    "\n",
    "---\n",
    "\n",
    "## 4.2 Clustering Algorithms\n",
    "\n",
    "### 4.2.1 K-Means Clustering\n",
    "\n",
    "#### What is K-Means?\n",
    "\n",
    "**K-Means** is a centroid-based clustering algorithm that partitions data into k clusters by minimizing within-cluster variance. It's one of the most popular and widely-used clustering algorithms due to its simplicity and efficiency.\n",
    "\n",
    "The algorithm works by iteratively assigning data points to the nearest cluster centroid and then updating centroids based on the assigned points. The \"k\" refers to the number of clusters, which must be specified in advance.\n",
    "\n",
    "K-Means assumes that clusters are spherical (circular in 2D) and of similar size, which works well for many real-world datasets but may struggle with clusters of irregular shapes or varying densities.\n",
    "\n",
    "#### Key Components\n",
    "\n",
    "**Centroids**: Center points of clusters. Initially chosen randomly, then updated iteratively as the mean of all points assigned to that cluster. Each centroid represents the \"center\" of its cluster.\n",
    "\n",
    "**k Parameter**: The number of clusters to create. This must be specified before running the algorithm, making it a critical hyperparameter that significantly affects results.\n",
    "\n",
    "**Distance Metric**: Typically Euclidean distance, measuring straight-line distance between points in feature space. The algorithm assigns each point to the nearest centroid.\n",
    "\n",
    "**Cluster Assignment**: Each data point is assigned to exactly one cluster (hard clustering), based on which centroid is closest.\n",
    "\n",
    "#### The K-Means Algorithm\n",
    "\n",
    "The algorithm follows these steps:\n",
    "\n",
    "1. **Initialize k centroids**: Randomly select k data points as initial centroids, or use more sophisticated initialization methods like K-Means++.\n",
    "\n",
    "2. **Assign each data point to nearest centroid**: Calculate distance from each point to all centroids, assign point to closest centroid. This creates k clusters.\n",
    "\n",
    "3. **Recalculate centroids**: For each cluster, compute the mean of all points in that cluster. This becomes the new centroid position.\n",
    "\n",
    "4. **Repeat steps 2-3**: Continue assigning points and updating centroids until convergence (centroids stop moving significantly) or maximum iterations reached.\n",
    "\n",
    "5. **Final clusters**: The algorithm converges when assignments no longer change, meaning each point is stably assigned to its nearest centroid.\n",
    "\n",
    "The objective function being minimized is:\n",
    "\n",
    "**Minimize: Σᵢ Σⱼ ||xᵢ - μⱼ||²**\n",
    "\n",
    "Where:\n",
    "- xᵢ are data points\n",
    "- μⱼ are cluster centroids\n",
    "- The goal is to minimize the sum of squared distances within clusters\n",
    "\n",
    "#### Choosing k\n",
    "\n",
    "Selecting the right number of clusters is crucial:\n",
    "\n",
    "**Elbow Method**: Plot the within-cluster sum of squares (WCSS) against k. Look for an \"elbow\" - a point where the rate of decrease sharply changes. The elbow indicates the optimal k.\n",
    "\n",
    "**Silhouette Analysis**: Measures how similar objects are to their own cluster compared to other clusters. Silhouette score ranges from -1 to 1, with higher values indicating better clustering. Plot silhouette scores for different k values.\n",
    "\n",
    "**Domain Knowledge**: Use business understanding or domain expertise to determine the appropriate number of clusters. For example, if segmenting customers, you might know there are typically 3-5 distinct customer types.\n",
    "\n",
    "**Practical Considerations**: \n",
    "- Too few clusters: May miss important distinctions in data\n",
    "- Too many clusters: May overfit to noise, creating meaningless small clusters\n",
    "- Consider computational cost: More clusters mean more computation\n",
    "\n",
    "#### Advantages and Limitations\n",
    "\n",
    "**Advantages**:\n",
    "- **Simple and Fast**: Easy to understand and implement, computationally efficient\n",
    "- **Works Well with Spherical Clusters**: Excellent when clusters are roughly circular/spherical\n",
    "- **Scales to Large Datasets**: Efficient even with many data points\n",
    "- **Easy to Interpret**: Centroids provide clear cluster representatives\n",
    "- **Guaranteed Convergence**: Algorithm will converge (though may find local optimum)\n",
    "\n",
    "**Limitations**:\n",
    "- **Requires Pre-specifying k**: Must know or guess number of clusters beforehand\n",
    "- **Assumes Spherical Clusters**: Struggles with non-convex or irregularly shaped clusters\n",
    "- **Sensitive to Initialization**: Different random starts can yield different results\n",
    "- **Struggles with Varying Cluster Sizes**: Works best when clusters are similar in size\n",
    "- **Sensitive to Outliers**: Outliers can significantly affect centroid positions\n",
    "- **Local Optima**: May converge to suboptimal solutions depending on initialization\n",
    "\n",
    "#### Initialization Methods\n",
    "\n",
    "**Random Initialization**: Simple but can lead to poor results. Often requires multiple runs with different random seeds.\n",
    "\n",
    "**K-Means++**: Smart initialization that spreads initial centroids apart:\n",
    "1. Choose first centroid randomly\n",
    "2. Choose subsequent centroids with probability proportional to distance from nearest existing centroid\n",
    "3. This ensures centroids are well-distributed\n",
    "\n",
    "**Multiple Runs**: Run algorithm multiple times with different initializations and choose the best result (lowest within-cluster variance).\n",
    "\n",
    "---\n",
    "\n",
    "### 4.2.2 Hierarchical Clustering\n",
    "\n",
    "#### What is Hierarchical Clustering?\n",
    "\n",
    "**Hierarchical Clustering** creates a tree-like structure (dendrogram) of clusters, allowing exploration at different levels of granularity. Unlike K-Means which produces a flat set of clusters, hierarchical clustering produces a hierarchy where clusters can be merged or split at different levels.\n",
    "\n",
    "The dendrogram visualization shows how clusters are formed at different scales, making it easy to understand the data structure and choose an appropriate number of clusters by \"cutting\" the dendrogram at different heights.\n",
    "\n",
    "Hierarchical clustering doesn't require pre-specifying the number of clusters, making it useful when you're unsure about the optimal number of clusters. You can examine the dendrogram and choose the number of clusters that makes sense for your problem.\n",
    "\n",
    "#### Key Concepts\n",
    "\n",
    "**Dendrogram**: A tree diagram showing cluster relationships. The height of branches indicates the distance at which clusters merge. Cutting the dendrogram at different heights produces different numbers of clusters.\n",
    "\n",
    "**Linkage**: The method for measuring distance between clusters. Different linkage methods produce different cluster structures:\n",
    "- **Single Linkage**: Minimum distance between any two points in clusters (can create long chains)\n",
    "- **Complete Linkage**: Maximum distance between any two points (creates compact clusters)\n",
    "- **Average Linkage**: Average distance between all pairs of points\n",
    "- **Ward Linkage**: Minimizes within-cluster variance (similar to K-Means)\n",
    "\n",
    "**Agglomerative**: Bottom-up approach - start with each point as its own cluster, then merge clusters iteratively. Most common approach.\n",
    "\n",
    "**Divisive**: Top-down approach - start with all points in one cluster, then split recursively. Less common due to computational complexity.\n",
    "\n",
    "#### How Hierarchical Clustering Works\n",
    "\n",
    "The agglomerative algorithm follows these steps:\n",
    "\n",
    "1. **Start with individual points**: Each data point begins as its own cluster (n clusters for n points).\n",
    "\n",
    "2. **Calculate distance matrix**: Compute distances between all pairs of clusters using the chosen linkage method.\n",
    "\n",
    "3. **Merge two closest clusters**: Identify the two clusters with minimum distance and merge them into a single cluster.\n",
    "\n",
    "4. **Update distance matrix**: Recalculate distances from the new cluster to all other clusters.\n",
    "\n",
    "5. **Repeat until one cluster**: Continue merging until all points are in a single cluster, or until desired number of clusters reached.\n",
    "\n",
    "6. **Create dendrogram**: Visualize the merging process as a tree structure.\n",
    "\n",
    "The process creates a complete hierarchy, allowing you to extract clusters at any desired level by cutting the dendrogram.\n",
    "\n",
    "#### Advantages and Limitations\n",
    "\n",
    "**Advantages**:\n",
    "- **No Need to Pre-specify k**: Can explore different numbers of clusters\n",
    "- **Dendrogram Provides Visual Interpretation**: Easy to understand cluster structure\n",
    "- **Deterministic Results**: Same data always produces same dendrogram (for given linkage)\n",
    "- **Works with Any Distance Metric**: Flexible choice of distance measures\n",
    "- **Handles Non-spherical Clusters**: Can find clusters of various shapes\n",
    "\n",
    "**Limitations**:\n",
    "- **Computationally Expensive**: Time complexity O(n³) makes it slow for large datasets\n",
    "- **Sensitive to Noise and Outliers**: Can create spurious clusters from outliers\n",
    "- **Difficult to Handle Large Datasets**: Not practical for datasets with thousands of points\n",
    "- **Once Clusters Merge, They Can't Split**: Greedy algorithm may miss optimal global structure\n",
    "- **Memory Intensive**: Must store and update full distance matrix\n",
    "\n",
    "#### When to Use\n",
    "\n",
    "Hierarchical clustering is preferred when:\n",
    "- You want to explore cluster structure at multiple levels\n",
    "- You don't know the number of clusters beforehand\n",
    "- Dataset is small to medium size (few hundred to few thousand points)\n",
    "- You need interpretable, visual cluster structure\n",
    "- Clusters may have non-spherical shapes\n",
    "\n",
    "---\n",
    "\n",
    "### 4.2.3 DBSCAN (Density-Based Clustering)\n",
    "\n",
    "#### What is DBSCAN?\n",
    "\n",
    "**DBSCAN** (Density-Based Spatial Clustering of Applications with Noise) is a density-based clustering algorithm that groups together points that are closely packed, identifying outliers as noise. Unlike K-Means and hierarchical clustering, DBSCAN doesn't require pre-specifying the number of clusters and can find clusters of arbitrary shapes.\n",
    "\n",
    "The key insight of DBSCAN is that clusters are dense regions separated by sparse regions. Points in dense regions belong to clusters, while points in sparse regions are considered noise. This makes DBSCAN excellent for datasets with irregular cluster shapes and outliers.\n",
    "\n",
    "DBSCAN is particularly powerful because it can automatically determine the number of clusters and identify outliers without requiring them to be specified in advance.\n",
    "\n",
    "#### Key Concepts\n",
    "\n",
    "**Core Points**: Points that have at least min_samples neighbors within eps distance. Core points are always part of clusters and can have other points clustered around them.\n",
    "\n",
    "**Border Points**: Points within eps distance of a core point but don't have enough neighbors themselves to be core points. Border points belong to clusters but don't expand clusters.\n",
    "\n",
    "**Noise Points**: Points that are neither core nor border points. These are outliers that don't belong to any cluster. DBSCAN explicitly identifies and labels these as noise.\n",
    "\n",
    "**Density-Reachable**: Points connected through a chain of core points. If point A is a core point and point B is within eps of A, then B is density-reachable from A. All density-reachable points form a cluster.\n",
    "\n",
    "#### Parameters\n",
    "\n",
    "**eps (ε)**: Maximum distance for two points to be considered neighbors. This defines the neighborhood radius. Smaller eps creates more, smaller clusters; larger eps creates fewer, larger clusters.\n",
    "\n",
    "**min_samples**: Minimum number of points required to form a dense region (core point). Higher values require denser regions to form clusters, making the algorithm more conservative.\n",
    "\n",
    "Choosing these parameters:\n",
    "- **eps**: Use k-distance graph - plot distance to kth nearest neighbor, look for \"knee\" point\n",
    "- **min_samples**: Start with 2× number of dimensions, adjust based on results\n",
    "- Domain knowledge about expected cluster density can guide parameter selection\n",
    "\n",
    "#### How DBSCAN Works\n",
    "\n",
    "The algorithm follows these steps:\n",
    "\n",
    "1. **Randomly select unvisited point**: Start with any point that hasn't been processed.\n",
    "\n",
    "2. **Check if point is core**: Count neighbors within eps distance. If count ≥ min_samples, it's a core point.\n",
    "\n",
    "3. **If core point, create new cluster**: Start a new cluster and add this point.\n",
    "\n",
    "4. **Expand cluster**: Add all density-reachable points (neighbors of core point, and their neighbors recursively) to the cluster.\n",
    "\n",
    "5. **Mark all points in cluster as visited**: Prevent reprocessing.\n",
    "\n",
    "6. **Repeat**: Continue with next unvisited point until all points processed.\n",
    "\n",
    "7. **Label noise**: Points not assigned to any cluster are marked as noise (outliers).\n",
    "\n",
    "The algorithm naturally handles clusters of different shapes and sizes, as long as they meet the density requirements.\n",
    "\n",
    "#### Advantages and Limitations\n",
    "\n",
    "**Advantages**:\n",
    "- **Finds Clusters of Arbitrary Shape**: Not limited to spherical clusters like K-Means\n",
    "- **Automatic Outlier Detection**: Explicitly identifies and labels noise points\n",
    "- **No Need to Pre-specify Number of Clusters**: Automatically determines number based on data density\n",
    "- **Robust to Noise**: Outliers don't significantly affect cluster formation\n",
    "- **Handles Varying Densities**: Can find clusters with different densities (with appropriate parameters)\n",
    "\n",
    "**Limitations**:\n",
    "- **Struggles with Varying Cluster Densities**: Single eps parameter may not work for all clusters\n",
    "- **Sensitive to Parameters**: eps and min_samples significantly affect results\n",
    "- **Border Points May Be Misclassified**: Points near cluster boundaries may be assigned to wrong cluster\n",
    "- **Performance Degrades with High Dimensions**: Curse of dimensionality affects distance calculations\n",
    "- **Difficult Parameter Tuning**: Choosing optimal eps and min_samples can be challenging\n",
    "\n",
    "#### When to Use\n",
    "\n",
    "DBSCAN excels when:\n",
    "- Clusters have irregular, non-spherical shapes\n",
    "- You want automatic outlier detection\n",
    "- Number of clusters is unknown\n",
    "- Data contains noise/outliers\n",
    "- Clusters have similar densities\n",
    "- You need to identify anomalies\n",
    "\n",
    "---\n",
    "\n",
    "## 4.3 Dimensionality Reduction\n",
    "\n",
    "### 4.3.1 Principal Component Analysis (PCA)\n",
    "\n",
    "#### What is PCA?\n",
    "\n",
    "**Principal Component Analysis (PCA)** is a linear dimensionality reduction technique that transforms data to a lower-dimensional space while preserving maximum variance. PCA finds new orthogonal axes (principal components) that capture the most variation in the data.\n",
    "\n",
    "The key idea is that not all features contribute equally to the variance in the data. PCA identifies directions (principal components) where the data varies most, allowing us to represent the data in fewer dimensions while losing minimal information.\n",
    "\n",
    "PCA is widely used for:\n",
    "- Data visualization (reducing to 2D/3D)\n",
    "- Feature extraction for machine learning\n",
    "- Noise reduction\n",
    "- Removing multicollinearity\n",
    "- Data compression\n",
    "\n",
    "#### Key Concepts\n",
    "\n",
    "**Principal Components**: New orthogonal axes that capture maximum variance. The first principal component captures the most variance, the second captures the second most (while being orthogonal to the first), and so on.\n",
    "\n",
    "**Eigenvalues**: Measure the variance explained by each principal component. Larger eigenvalues indicate components that capture more information.\n",
    "\n",
    "**Eigenvectors**: Directions of principal components. These define the new coordinate system.\n",
    "\n",
    "**Variance Preservation**: PCA aims to retain as much variance as possible in fewer dimensions. The proportion of variance explained by each component indicates its importance.\n",
    "\n",
    "#### Mathematical Foundation\n",
    "\n",
    "PCA works through eigendecomposition:\n",
    "\n",
    "1. **Standardize Data**: Mean-center and scale features (mean=0, std=1). This is crucial because PCA is sensitive to feature scales.\n",
    "\n",
    "2. **Calculate Covariance Matrix**: Compute covariance between all pairs of features. This captures relationships between features.\n",
    "\n",
    "3. **Eigendecomposition**: Find eigenvalues and eigenvectors of the covariance matrix. Eigenvectors are principal components, eigenvalues indicate variance explained.\n",
    "\n",
    "4. **Sort by Eigenvalues**: Order components by decreasing eigenvalues (variance explained).\n",
    "\n",
    "5. **Select Top k Components**: Choose components that explain sufficient variance (e.g., 95% cumulative variance).\n",
    "\n",
    "6. **Transform Data**: Project original data onto selected principal components using matrix multiplication.\n",
    "\n",
    "The transformation is: **Y = XW**\n",
    "\n",
    "Where:\n",
    "- X is original data (n×m)\n",
    "- W is matrix of selected eigenvectors (m×k)\n",
    "- Y is transformed data (n×k)\n",
    "\n",
    "#### How PCA Works\n",
    "\n",
    "Step-by-step process:\n",
    "\n",
    "1. **Standardize the data**: Transform features to have mean=0 and standard deviation=1. This ensures all features contribute equally.\n",
    "\n",
    "2. **Calculate covariance matrix**: Compute how features vary together. For m features, this is an m×m matrix.\n",
    "\n",
    "3. **Compute eigenvalues and eigenvectors**: Solve the eigenvalue problem for the covariance matrix. Each eigenvector is a principal component direction.\n",
    "\n",
    "4. **Sort eigenvectors by eigenvalues**: Order components from highest to lowest variance explained.\n",
    "\n",
    "5. **Select top k eigenvectors**: Choose number of components based on:\n",
    "   - Cumulative variance explained (e.g., retain components explaining 95% variance)\n",
    "   - Scree plot analysis (look for \"elbow\")\n",
    "   - Specific number needed for downstream task\n",
    "\n",
    "6. **Transform data**: Project original data onto selected principal components to get reduced-dimensional representation.\n",
    "\n",
    "#### Variance Explained\n",
    "\n",
    "**Cumulative Variance**: Sum of variance explained by components up to a given point. Often plotted to determine how many components to retain.\n",
    "\n",
    "**Scree Plot**: Plot of eigenvalues (variance) vs. component number. Look for \"elbow\" where additional components add little variance.\n",
    "\n",
    "**Choosing Number of Components**:\n",
    "- **Variance Threshold**: Retain components explaining ≥95% cumulative variance\n",
    "- **Elbow Method**: Use scree plot to find point of diminishing returns\n",
    "- **Specific Requirements**: Use fixed number needed for visualization (2D/3D) or downstream tasks\n",
    "\n",
    "#### Advantages and Limitations\n",
    "\n",
    "**Advantages**:\n",
    "- **Reduces Dimensionality**: Can dramatically reduce number of features\n",
    "- **Removes Correlation**: Principal components are uncorrelated\n",
    "- **Can Improve Model Performance**: Removing noise and redundancy can help downstream models\n",
    "- **Helps with Visualization**: Enables visualization of high-dimensional data\n",
    "- **Reduces Overfitting Risk**: Fewer features mean simpler models\n",
    "- **Interpretable**: Components can sometimes be interpreted (though this gets harder with many features)\n",
    "\n",
    "**Limitations**:\n",
    "- **Assumes Linear Relationships**: Cannot capture non-linear patterns\n",
    "- **Loses Interpretability**: Principal components are linear combinations, harder to interpret than original features\n",
    "- **May Lose Important Information**: If important variance is in later components, it may be discarded\n",
    "- **Sensitive to Feature Scaling**: Requires standardization, and results depend on scaling method\n",
    "- **Not Suitable for All Data**: Works best when variance is concentrated in first few components\n",
    "\n",
    "#### Uses\n",
    "\n",
    "Common applications:\n",
    "- **Data Visualization**: Reduce to 2D/3D for plotting and exploration\n",
    "- **Feature Extraction**: Create new features for machine learning models\n",
    "- **Noise Reduction**: Remove components with low variance (often noise)\n",
    "- **Compression**: Represent data more compactly\n",
    "- **Removing Multicollinearity**: Eliminate correlated features before regression\n",
    "- **Preprocessing**: Prepare data for other algorithms that struggle with high dimensions\n",
    "\n",
    "---\n",
    "\n",
    "### 4.3.2 t-SNE (t-Distributed Stochastic Neighbor Embedding)\n",
    "\n",
    "#### What is t-SNE?\n",
    "\n",
    "**t-SNE** (t-Distributed Stochastic Neighbor Embedding) is a non-linear dimensionality reduction technique primarily used for visualization. Unlike PCA which preserves global structure, t-SNE focuses on preserving local neighborhood structure, making it excellent for revealing cluster structure in data.\n",
    "\n",
    "t-SNE is particularly powerful for exploratory data analysis because it can reveal patterns and clusters that aren't visible in the original high-dimensional space. It's widely used for visualizing high-dimensional data in 2D or 3D.\n",
    "\n",
    "The \"t-distributed\" aspect refers to the use of Student's t-distribution (instead of normal distribution) in the low-dimensional space, which has heavier tails and helps prevent points from crowding together.\n",
    "\n",
    "#### Key Concepts\n",
    "\n",
    "**Stochastic**: Uses random initialization, meaning results can vary between runs. The algorithm starts with random positions and optimizes iteratively.\n",
    "\n",
    "**Neighbor Preservation**: Focuses on keeping similar points close together in the low-dimensional space. Points that are neighbors in high dimensions should remain neighbors in low dimensions.\n",
    "\n",
    "**t-Distribution**: Uses Student's t-distribution for low-dimensional probabilities (heavier tails than normal distribution). This helps spread out points and prevents crowding in the center.\n",
    "\n",
    "**Perplexity**: Hyperparameter controlling balance between local and global structure. Roughly represents the number of neighbors each point considers. Typical values: 5-50. Lower perplexity focuses on local structure, higher on global.\n",
    "\n",
    "#### How t-SNE Works\n",
    "\n",
    "Conceptual explanation of the algorithm:\n",
    "\n",
    "1. **Construct Probability Distribution in High Dimensions**: For each point, create a probability distribution over all other points based on distances. Similar points have high probability of being neighbors.\n",
    "\n",
    "2. **Initialize Low-Dimensional Embedding**: Randomly place points in low-dimensional space (2D or 3D).\n",
    "\n",
    "3. **Construct Similar Probability Distribution in Low Dimensions**: Create probability distribution in low-dimensional space using t-distribution.\n",
    "\n",
    "4. **Minimize Divergence**: Use gradient descent to minimize Kullback-Leibler divergence between high-dimensional and low-dimensional distributions. This moves points so that neighborhoods are preserved.\n",
    "\n",
    "5. **Iterate**: Continue optimizing until convergence or maximum iterations.\n",
    "\n",
    "The algorithm preserves local structure (neighbors stay neighbors) but may distort global structure (distances between far-apart points may not be preserved).\n",
    "\n",
    "#### Advantages and Limitations\n",
    "\n",
    "**Advantages**:\n",
    "- **Excellent for Visualization**: Creates beautiful, interpretable 2D/3D visualizations\n",
    "- **Captures Non-linear Relationships**: Can reveal non-linear patterns that PCA misses\n",
    "- **Reveals Cluster Structure**: Excellent for identifying groups and clusters in data\n",
    "- **Good for Exploratory Data Analysis**: Helps understand data structure before modeling\n",
    "- **Handles Complex Manifolds**: Can unfold complex high-dimensional structures\n",
    "\n",
    "**Limitations**:\n",
    "- **Computationally Expensive**: Much slower than PCA, especially for large datasets\n",
    "- **Non-Deterministic**: Different runs can produce different results (due to random initialization)\n",
    "- **Cannot Be Applied to New Data**: Must rerun entire algorithm for new points (unlike PCA which can transform new data)\n",
    "- **Hyperparameter Sensitive**: Perplexity significantly affects results\n",
    "- **Primarily for Visualization**: Not recommended for feature reduction for downstream ML (use PCA instead)\n",
    "- **May Create False Clusters**: Can create apparent clusters that don't exist in high dimensions\n",
    "- **Global Structure Not Preserved**: Distances between far-apart points may not be meaningful\n",
    "\n",
    "#### When to Use\n",
    "\n",
    "t-SNE is best for:\n",
    "- **Data Exploration**: Understanding structure of high-dimensional data\n",
    "- **Visualization**: Creating 2D/3D plots for presentations or analysis\n",
    "- **Cluster Discovery**: Identifying groups and patterns in data\n",
    "- **Before Supervised Learning**: Understanding data structure before building models\n",
    "- **Non-linear Patterns**: When you suspect non-linear relationships\n",
    "\n",
    "**Do NOT use t-SNE for**:\n",
    "- Feature reduction for machine learning models (use PCA)\n",
    "- When you need to transform new data points\n",
    "- When computational time is critical\n",
    "- When you need reproducible, deterministic results\n",
    "\n",
    "---\n",
    "\n",
    "## 4.4 Evaluation of Unsupervised Learning\n",
    "\n",
    "### Challenges\n",
    "\n",
    "Evaluating unsupervised learning is more challenging than supervised learning because there are no ground truth labels to compare against. We can't simply calculate accuracy or error - we must assess whether discovered patterns are meaningful and useful.\n",
    "\n",
    "The evaluation depends on the goal:\n",
    "- **Clustering**: Are clusters meaningful? Do they represent real groups?\n",
    "- **Dimensionality Reduction**: Is important information preserved? Do reduced features work well for downstream tasks?\n",
    "\n",
    "### Clustering Evaluation Metrics\n",
    "\n",
    "#### Internal Metrics (No Ground Truth Needed)\n",
    "\n",
    "**Silhouette Score**: Measures how similar objects are to their own cluster compared to other clusters.\n",
    "\n",
    "- Range: -1 to 1\n",
    "- Higher is better\n",
    "- Formula: s(i) = (b(i) - a(i)) / max(a(i), b(i))\n",
    "  - a(i): average distance to points in same cluster\n",
    "  - b(i): average distance to points in nearest other cluster\n",
    "- Interpretation:\n",
    "  - Close to 1: Well-clustered\n",
    "  - Close to 0: On cluster boundary\n",
    "  - Negative: Assigned to wrong cluster\n",
    "\n",
    "**Davies-Bouldin Index**: Average similarity ratio of clusters.\n",
    "\n",
    "- Lower is better\n",
    "- Measures average similarity between each cluster and its most similar cluster\n",
    "- Good clusters have low similarity to other clusters\n",
    "\n",
    "**Calinski-Harabasz Index**: Ratio of between-cluster to within-cluster variance.\n",
    "\n",
    "- Higher is better\n",
    "- Also called Variance Ratio Criterion\n",
    "- Measures separation between clusters relative to compactness within clusters\n",
    "\n",
    "#### External Metrics (Requires Ground Truth)\n",
    "\n",
    "**Adjusted Rand Index (ARI)**: Measures similarity between clusterings, accounting for chance.\n",
    "\n",
    "- Range: -1 to 1 (1 = perfect match, 0 = random, negative = worse than random)\n",
    "- Accounts for chance agreement, making it more reliable than simple accuracy\n",
    "\n",
    "**Normalized Mutual Information (NMI)**: Measures mutual information between clusterings.\n",
    "\n",
    "- Range: 0 to 1\n",
    "- Higher indicates better agreement\n",
    "- Normalized to account for different numbers of clusters\n",
    "\n",
    "**Homogeneity, Completeness, V-measure**: Three related metrics:\n",
    "- **Homogeneity**: Each cluster contains only members of a single class\n",
    "- **Completeness**: All members of a given class are assigned to same cluster\n",
    "- **V-measure**: Harmonic mean of homogeneity and completeness\n",
    "\n",
    "### Dimensionality Reduction Evaluation\n",
    "\n",
    "**Variance Explained**: For PCA, cumulative variance explained by retained components. Higher variance explained means less information lost.\n",
    "\n",
    "**Reconstruction Error**: How well original data can be reconstructed from reduced dimensions. Lower error means better preservation of information.\n",
    "\n",
    "**Downstream Task Performance**: Use reduced features in supervised learning tasks. If performance is maintained or improved, reduction was successful.\n",
    "\n",
    "**Visual Inspection**: For visualization techniques like t-SNE, visual inspection of plots can reveal whether meaningful structure is preserved.\n",
    "\n",
    "---\n",
    "\n",
    "## Summary and Key Takeaways\n",
    "\n",
    "1. **Unsupervised Learning** discovers patterns in unlabeled data through exploration and structure discovery.\n",
    "\n",
    "2. **K-Means** is simple and efficient for spherical clusters but requires pre-specifying k and assumes similar cluster sizes.\n",
    "\n",
    "3. **Hierarchical Clustering** creates interpretable dendrograms and doesn't require k, but is computationally expensive.\n",
    "\n",
    "4. **DBSCAN** finds arbitrary-shaped clusters and identifies outliers automatically, but is sensitive to parameters.\n",
    "\n",
    "5. **PCA** reduces dimensionality linearly while preserving maximum variance, useful for visualization and feature extraction.\n",
    "\n",
    "6. **t-SNE** excels at non-linear visualization and cluster discovery but is computationally expensive and non-deterministic.\n",
    "\n",
    "7. **Evaluation** of unsupervised learning requires different metrics than supervised learning, focusing on cluster quality or information preservation.\n",
    "\n",
    "8. **Choosing Methods** depends on data characteristics, computational resources, and goals (clustering vs. visualization vs. feature reduction).\n",
    "\n",
    "---\n",
    "\n",
    "## Further Reading\n",
    "\n",
    "- James, G., et al. (2013). *An Introduction to Statistical Learning*. Springer.\n",
    "- Scikit-learn Clustering Documentation: https://scikit-learn.org/stable/modules/clustering.html\n",
    "- Scikit-learn Decomposition Documentation: https://scikit-learn.org/stable/modules/decomposition.html\n",
    "\n",
    "---\n",
    "\n",
    "## Practice Exercises\n",
    "\n",
    "1. When would you choose K-Means over DBSCAN? When would you choose DBSCAN?\n",
    "\n",
    "2. Explain the difference between PCA and t-SNE. When would you use each?\n",
    "\n",
    "3. How do you choose the optimal number of clusters for K-Means?\n",
    "\n",
    "4. What are the advantages and disadvantages of hierarchical clustering?\n",
    "\n",
    "5. How would you evaluate the quality of a clustering result when you don't have ground truth labels?\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
