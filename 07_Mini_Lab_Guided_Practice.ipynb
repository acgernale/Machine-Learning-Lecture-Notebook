{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Mini-Lab: Supervised and Unsupervised Learning Practice\n",
        "## Guided Practice Notebook for BS Data Science Students\n",
        "\n",
        "---\n",
        "\n",
        "## Learning Objectives\n",
        "\n",
        "By completing this mini-lab, you will be able to:\n",
        "1. Apply supervised and unsupervised learning techniques to real datasets\n",
        "2. Frame appropriate machine learning tasks from data\n",
        "3. Implement data cleaning and preprocessing pipelines\n",
        "4. Train and evaluate multiple supervised learning models\n",
        "5. Apply clustering (K-Means) and dimensionality reduction (PCA)\n",
        "6. Interpret results and extract meaningful insights\n",
        "\n",
        "---\n",
        "\n",
        "## Instructions\n",
        "\n",
        "This is a **guided practice** notebook where you will:\n",
        "- Choose one dataset from the provided options\n",
        "- Frame both a **supervised learning task** and an **unsupervised learning task**\n",
        "- Complete code cells marked with `# TODO:` comments\n",
        "- Train models, evaluate performance, and interpret results\n",
        "\n",
        "**Time Estimate**: 2-3 hours\n",
        "\n",
        "**Grading Criteria**:\n",
        "- Code completion and correctness (40%)\n",
        "- Model evaluation and interpretation (30%)\n",
        "- Data cleaning and preprocessing quality (20%)\n",
        "- Documentation and comments (10%)\n",
        "\n",
        "---\n",
        "\n",
        "## Dataset Options\n",
        "\n",
        "Choose **ONE** of the following datasets for this lab:\n",
        "\n",
        "### Option 1: Iris Dataset (Recommended for Beginners)\n",
        "- **Source**: Built-in sklearn dataset\n",
        "- **Size**: 150 samples, 4 features\n",
        "- **Task**: Classify iris species (3 classes)\n",
        "- **Features**: Sepal length, sepal width, petal length, petal width\n",
        "- **Difficulty**: ‚≠ê Easy\n",
        "\n",
        "### Option 2: Wine Quality Dataset\n",
        "- **Source**: UCI Machine Learning Repository (wine quality)\n",
        "- **Size**: ~1,600 samples, 11 features\n",
        "- **Task**: Predict wine quality (regression) or classify quality levels\n",
        "- **Features**: Alcohol, acidity, pH, residual sugar, etc.\n",
        "- **Difficulty**: ‚≠ê‚≠ê Medium\n",
        "\n",
        "### Option 3: Breast Cancer Wisconsin Dataset\n",
        "- **Source**: Built-in sklearn dataset\n",
        "- **Size**: 569 samples, 30 features\n",
        "- **Task**: Classify tumors as benign or malignant\n",
        "- **Features**: Various cell measurements\n",
        "- **Difficulty**: ‚≠ê‚≠ê Medium\n",
        "\n",
        "### Option 4: California Housing Dataset\n",
        "- **Source**: Built-in sklearn dataset\n",
        "- **Size**: ~20,000 samples, 8 features\n",
        "- **Task**: Predict median house value (regression)\n",
        "- **Features**: Location, income, age, rooms, etc.\n",
        "- **Difficulty**: ‚≠ê‚≠ê‚≠ê Advanced\n",
        "\n",
        "---\n",
        "\n",
        "## Part 1: Setup and Data Loading\n",
        "\n",
        "### Step 1.1: Import Libraries\n",
        "\n",
        "First, import all necessary libraries:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
        "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
        "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score,\n",
        "                            mean_squared_error, r2_score, confusion_matrix,\n",
        "                            classification_report)\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import silhouette_score\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set plotting style\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"Libraries imported successfully!\")\n",
        "print(\"Ready to start the mini-lab!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 1.2: Load Your Chosen Dataset\n",
        "\n",
        "**TODO**: Uncomment and complete the code for your chosen dataset. Remove or comment out the other options.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# TODO: Choose ONE dataset and uncomment the corresponding code block\n",
        "# ============================================================================\n",
        "\n",
        "# OPTION 1: Iris Dataset (Recommended for beginners)\n",
        "from sklearn.datasets import load_iris\n",
        "data = load_iris()\n",
        "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "df['target'] = data.target\n",
        "df['target_name'] = data.target_names[data.target]\n",
        "print(\"Dataset: Iris\")\n",
        "print(f\"Shape: {df.shape}\")\n",
        "print(f\"Features: {list(df.columns[:-2])}\")\n",
        "print(f\"Target classes: {list(data.target_names)}\")\n",
        "\n",
        "# OPTION 2: Wine Quality Dataset\n",
        "# from sklearn.datasets import fetch_openml\n",
        "# wine = fetch_openml(name='wine-quality-red', version=1, as_frame=True)\n",
        "# df = wine.frame\n",
        "# print(\"Dataset: Wine Quality\")\n",
        "# print(f\"Shape: {df.shape}\")\n",
        "\n",
        "# OPTION 3: Breast Cancer Wisconsin Dataset\n",
        "# from sklearn.datasets import load_breast_cancer\n",
        "# data = load_breast_cancer()\n",
        "# df = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "# df['target'] = data.target\n",
        "# df['target_name'] = data.target_names[data.target]\n",
        "# print(\"Dataset: Breast Cancer Wisconsin\")\n",
        "# print(f\"Shape: {df.shape}\")\n",
        "\n",
        "# OPTION 4: California Housing Dataset\n",
        "# from sklearn.datasets import fetch_california_housing\n",
        "# housing = fetch_california_housing(as_frame=True)\n",
        "# df = housing.frame\n",
        "# print(\"Dataset: California Housing\")\n",
        "# print(f\"Shape: {df.shape}\")\n",
        "\n",
        "# Display first few rows\n",
        "print(\"\\nFirst 5 rows:\")\n",
        "print(df.head())\n",
        "print(\"\\nDataset info:\")\n",
        "print(df.info())\n",
        "print(\"\\nBasic statistics:\")\n",
        "print(df.describe())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 1.3: Exploratory Data Analysis\n",
        "\n",
        "**TODO**: Explore your dataset. Create visualizations to understand the data distribution, relationships, and any potential issues.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# TODO: Perform exploratory data analysis\n",
        "# - Check for missing values\n",
        "# - Visualize feature distributions\n",
        "# - Check for outliers\n",
        "# - Examine relationships between features\n",
        "# - Visualize target variable distribution (if applicable)\n",
        "# ============================================================================\n",
        "\n",
        "# Check for missing values\n",
        "print(\"Missing values:\")\n",
        "print(df.isnull().sum())\n",
        "print(f\"\\nTotal missing values: {df.isnull().sum().sum()}\")\n",
        "\n",
        "# TODO: Create visualizations\n",
        "# Example: Distribution plots for numerical features\n",
        "# Example: Box plots to check for outliers\n",
        "# Example: Correlation heatmap\n",
        "# Example: Target variable distribution (for classification)\n",
        "\n",
        "# Hint: Use df.hist(), sns.boxplot(), sns.heatmap(), sns.countplot(), etc.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Part 2: Data Cleaning and Preprocessing\n",
        "\n",
        "### Step 2.1: Handle Missing Values and Outliers\n",
        "\n",
        "**TODO**: Clean your dataset by handling missing values and outliers appropriately.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# TODO: Clean the dataset\n",
        "# 1. Handle missing values (imputation or removal)\n",
        "# 2. Handle outliers (if necessary)\n",
        "# 3. Encode categorical variables (if any)\n",
        "# ============================================================================\n",
        "\n",
        "# Create a copy for cleaning\n",
        "df_clean = df.copy()\n",
        "\n",
        "# TODO: Handle missing values\n",
        "# If there are missing values, decide on strategy:\n",
        "# - For numerical: mean/median imputation, or remove rows\n",
        "# - For categorical: mode imputation, or create \"missing\" category\n",
        "\n",
        "# Example (uncomment and modify as needed):\n",
        "# from sklearn.impute import SimpleImputer\n",
        "# imputer = SimpleImputer(strategy='mean')  # or 'median', 'most_frequent'\n",
        "# df_clean[['feature1', 'feature2']] = imputer.fit_transform(df_clean[['feature1', 'feature2']])\n",
        "\n",
        "# TODO: Handle outliers (if needed)\n",
        "# Options: Remove outliers, cap values, or transform\n",
        "\n",
        "# TODO: Encode categorical variables (if any)\n",
        "# Use pd.get_dummies() or LabelEncoder\n",
        "\n",
        "print(\"Data cleaning completed!\")\n",
        "print(f\"Original shape: {df.shape}\")\n",
        "print(f\"Cleaned shape: {df_clean.shape}\")\n",
        "print(\"\\nCleaned data info:\")\n",
        "print(df_clean.info())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 2.2: Prepare Features and Target\n",
        "\n",
        "**TODO**: Separate features (X) and target (y) for your supervised learning task.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# TODO: Prepare features and target for supervised learning\n",
        "# 1. Select features (X) - exclude target columns\n",
        "# 2. Select target (y)\n",
        "# 3. Split into training and testing sets\n",
        "# ============================================================================\n",
        "\n",
        "# TODO: Define your supervised learning task\n",
        "# For classification: predict target class\n",
        "# For regression: predict continuous value\n",
        "\n",
        "# Example for Iris dataset:\n",
        "# X = df_clean[['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']]\n",
        "# y = df_clean['target']  # or df_clean['target_name'] if using names\n",
        "\n",
        "# TODO: Replace with your actual feature and target selection\n",
        "X = df_clean.iloc[:, :-2]  # Modify this based on your dataset\n",
        "y = df_clean['target']     # Modify this based on your dataset\n",
        "\n",
        "print(f\"Features shape: {X.shape}\")\n",
        "print(f\"Target shape: {y.shape}\")\n",
        "print(f\"\\nFeature names: {list(X.columns)}\")\n",
        "print(f\"\\nTarget distribution:\")\n",
        "print(y.value_counts() if hasattr(y, 'value_counts') else pd.Series(y).value_counts())\n",
        "\n",
        "# Split data into training and testing sets\n",
        "# TODO: Choose appropriate test_size (typically 0.2 or 0.3)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y if len(np.unique(y)) < 10 else None\n",
        ")\n",
        "\n",
        "print(f\"\\nTraining set: {X_train.shape[0]} samples\")\n",
        "print(f\"Test set: {X_test.shape[0]} samples\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 2.3: Feature Scaling\n",
        "\n",
        "**TODO**: Scale features if necessary for your chosen models.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# TODO: Scale features if needed\n",
        "# Some models require scaling (SVM, k-NN, neural networks, regularized regression)\n",
        "# Tree-based models (Decision Trees, Random Forest) don't require scaling\n",
        "# ============================================================================\n",
        "\n",
        "# TODO: Decide if scaling is needed for your models\n",
        "# If using Logistic Regression, SVM, or k-NN: SCALE\n",
        "# If using Decision Trees or Random Forest: SCALING OPTIONAL\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Convert back to DataFrame for easier handling (optional)\n",
        "X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns, index=X_train.index)\n",
        "X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns, index=X_test.index)\n",
        "\n",
        "print(\"Features scaled successfully!\")\n",
        "print(f\"Scaled training data shape: {X_train_scaled.shape}\")\n",
        "print(f\"Scaled test data shape: {X_test_scaled.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Part 3: Supervised Learning\n",
        "\n",
        "### Step 3.1: Frame Your Supervised Learning Task\n",
        "\n",
        "**TODO**: Clearly define your supervised learning task in the markdown cell below.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Your Supervised Learning Task\n",
        "\n",
        "**TODO**: Describe your supervised learning task here:\n",
        "\n",
        "- **Task Type**: [ ] Classification  [ ] Regression\n",
        "- **Target Variable**: _______________________\n",
        "- **Features Used**: _______________________\n",
        "- **Business/Research Question**: What problem are you trying to solve?\n",
        "- **Success Metric**: How will you measure success? (e.g., accuracy > 0.90, RMSE < 1000)\n",
        "\n",
        "**Your description here:**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 3.2: Train Model 1\n",
        "\n",
        "**TODO**: Train your first supervised learning model. Choose an appropriate model based on your task type.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# TODO: Train Model 1\n",
        "# Choose one of the following based on your task:\n",
        "# - Classification: LogisticRegression, DecisionTreeClassifier, RandomForestClassifier\n",
        "# - Regression: LinearRegression, DecisionTreeRegressor, RandomForestRegressor\n",
        "# ============================================================================\n",
        "\n",
        "# TODO: Initialize and train your first model\n",
        "# Example for classification:\n",
        "# model1 = LogisticRegression(random_state=42, max_iter=1000)\n",
        "# model1.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Example for regression:\n",
        "# model1 = LinearRegression()\n",
        "# model1.fit(X_train_scaled, y_train)\n",
        "\n",
        "# TODO: Replace with your chosen model\n",
        "model1 = None  # Initialize your model here\n",
        "# model1.fit(...)  # Train your model here\n",
        "\n",
        "# Make predictions\n",
        "# y_train_pred1 = model1.predict(X_train_scaled)\n",
        "# y_test_pred1 = model1.predict(X_test_scaled)\n",
        "\n",
        "# TODO: Calculate and print evaluation metrics\n",
        "# For classification: accuracy, precision, recall, F1-score\n",
        "# For regression: MSE, RMSE, MAE, R¬≤\n",
        "\n",
        "print(\"Model 1 training completed!\")\n",
        "# print(f\"Training Accuracy: {accuracy_score(y_train, y_train_pred1):.4f}\")\n",
        "# print(f\"Test Accuracy: {accuracy_score(y_test, y_test_pred1):.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 3.3: Train Model 2\n",
        "\n",
        "**TODO**: Train a second, different supervised learning model for comparison.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# TODO: Train Model 2 (different algorithm from Model 1)\n",
        "# Choose a different model to compare performance\n",
        "# ============================================================================\n",
        "\n",
        "# TODO: Initialize and train your second model\n",
        "# Example for classification:\n",
        "# model2 = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "# model2.fit(X_train, y_train)  # Tree-based models don't need scaling\n",
        "\n",
        "# Example for regression:\n",
        "# model2 = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "# model2.fit(X_train, y_train)\n",
        "\n",
        "# TODO: Replace with your chosen model\n",
        "model2 = None  # Initialize your model here\n",
        "# model2.fit(...)  # Train your model here\n",
        "\n",
        "# Make predictions\n",
        "# y_train_pred2 = model2.predict(X_train_scaled)  # or X_train if no scaling needed\n",
        "# y_test_pred2 = model2.predict(X_test_scaled)   # or X_test if no scaling needed\n",
        "\n",
        "# TODO: Calculate and print evaluation metrics\n",
        "\n",
        "print(\"Model 2 training completed!\")\n",
        "# print(f\"Training Accuracy: {accuracy_score(y_train, y_train_pred2):.4f}\")\n",
        "# print(f\"Test Accuracy: {accuracy_score(y_test, y_test_pred2):.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 3.4: Model Evaluation and Comparison\n",
        "\n",
        "**TODO**: Evaluate both models and compare their performance.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# TODO: Evaluate and compare both models\n",
        "# 1. Calculate comprehensive metrics for both models\n",
        "# 2. Create visualizations (confusion matrix, feature importance, etc.)\n",
        "# 3. Compare performance and identify the better model\n",
        "# ============================================================================\n",
        "\n",
        "# TODO: Create a comparison table\n",
        "results_comparison = {\n",
        "    'Model': ['Model 1', 'Model 2'],\n",
        "    # 'Training Accuracy': [accuracy_score(y_train, y_train_pred1), accuracy_score(y_train, y_train_pred2)],\n",
        "    # 'Test Accuracy': [accuracy_score(y_test, y_test_pred1), accuracy_score(y_test, y_test_pred2)],\n",
        "    # Add more metrics as appropriate\n",
        "}\n",
        "\n",
        "results_df = pd.DataFrame(results_comparison)\n",
        "print(\"Model Comparison:\")\n",
        "print(results_df)\n",
        "\n",
        "# TODO: Create visualizations\n",
        "# For classification:\n",
        "# - Confusion matrices for both models\n",
        "# - ROC curves (if binary classification)\n",
        "# - Feature importance plots (if applicable)\n",
        "\n",
        "# For regression:\n",
        "# - Predicted vs Actual scatter plots\n",
        "# - Residual plots\n",
        "# - Feature importance plots (if applicable)\n",
        "\n",
        "# Example visualization code (uncomment and modify):\n",
        "# fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
        "# \n",
        "# # Confusion matrix for Model 1\n",
        "# cm1 = confusion_matrix(y_test, y_test_pred1)\n",
        "# sns.heatmap(cm1, annot=True, fmt='d', cmap='Blues', ax=axes[0])\n",
        "# axes[0].set_title('Model 1 - Confusion Matrix')\n",
        "# \n",
        "# # Confusion matrix for Model 2\n",
        "# cm2 = confusion_matrix(y_test, y_test_pred2)\n",
        "# sns.heatmap(cm2, annot=True, fmt='d', cmap='Blues', ax=axes[1])\n",
        "# axes[1].set_title('Model 2 - Confusion Matrix')\n",
        "# \n",
        "# plt.tight_layout()\n",
        "# plt.show()\n",
        "\n",
        "# TODO: Interpret results\n",
        "print(\"\\nInterpretation:\")\n",
        "print(\"Which model performed better? Why?\")\n",
        "print(\"What insights can you draw from the results?\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Part 4: Unsupervised Learning\n",
        "\n",
        "### Step 4.1: Frame Your Unsupervised Learning Task\n",
        "\n",
        "**TODO**: Define your unsupervised learning task.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Your Unsupervised Learning Task\n",
        "\n",
        "**TODO**: Describe your unsupervised learning task here:\n",
        "\n",
        "- **Task Type**: [ ] Clustering  [ ] Dimensionality Reduction  [ ] Both\n",
        "- **Research Question**: What patterns are you trying to discover?\n",
        "- **Expected Number of Clusters** (if clustering): _______________________\n",
        "- **Goal**: What insights do you hope to gain?\n",
        "\n",
        "**Your description here:**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 4.2: Prepare Data for Unsupervised Learning\n",
        "\n",
        "**TODO**: Prepare features for unsupervised learning (no target variable needed).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# TODO: Prepare data for unsupervised learning\n",
        "# Use the same features as supervised learning, but WITHOUT the target\n",
        "# ============================================================================\n",
        "\n",
        "# For unsupervised learning, we use features only (no target)\n",
        "X_unsupervised = X.copy()  # Use your feature matrix\n",
        "\n",
        "# Scale features (important for K-Means and PCA)\n",
        "scaler_unsup = StandardScaler()\n",
        "X_unsupervised_scaled = scaler_unsup.fit_transform(X_unsupervised)\n",
        "\n",
        "print(f\"Unsupervised learning data shape: {X_unsupervised_scaled.shape}\")\n",
        "print(f\"Features: {list(X.columns)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 4.3: K-Means Clustering\n",
        "\n",
        "**TODO**: Apply K-Means clustering to discover groups in your data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# TODO: Apply K-Means Clustering\n",
        "# 1. Determine optimal number of clusters (elbow method or silhouette analysis)\n",
        "# 2. Train K-Means with optimal k\n",
        "# 3. Visualize clusters\n",
        "# 4. Interpret cluster characteristics\n",
        "# ============================================================================\n",
        "\n",
        "# TODO: Determine optimal number of clusters using elbow method\n",
        "k_range = range(2, 11)\n",
        "inertias = []\n",
        "silhouette_scores = []\n",
        "\n",
        "for k in k_range:\n",
        "    kmeans_temp = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
        "    kmeans_temp.fit(X_unsupervised_scaled)\n",
        "    inertias.append(kmeans_temp.inertia_)\n",
        "    silhouette_scores.append(silhouette_score(X_unsupervised_scaled, kmeans_temp.labels_))\n",
        "\n",
        "# Visualize elbow method\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(k_range, inertias, 'bo-', linewidth=2, markersize=8)\n",
        "plt.xlabel('Number of Clusters (k)')\n",
        "plt.ylabel('Within-Cluster Sum of Squares (Inertia)')\n",
        "plt.title('Elbow Method')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(k_range, silhouette_scores, 'go-', linewidth=2, markersize=8)\n",
        "plt.xlabel('Number of Clusters (k)')\n",
        "plt.ylabel('Silhouette Score')\n",
        "plt.title('Silhouette Analysis')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# TODO: Choose optimal k based on elbow method and silhouette analysis\n",
        "optimal_k = 3  # Replace with your chosen k\n",
        "\n",
        "print(f\"Optimal number of clusters: {optimal_k}\")\n",
        "print(f\"Best silhouette score: {max(silhouette_scores):.4f}\")\n",
        "\n",
        "# TODO: Train K-Means with optimal k\n",
        "kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\n",
        "cluster_labels = kmeans.fit_predict(X_unsupervised_scaled)\n",
        "\n",
        "print(f\"\\nClustering completed!\")\n",
        "print(f\"Cluster distribution:\")\n",
        "unique, counts = np.unique(cluster_labels, return_counts=True)\n",
        "for cluster, count in zip(unique, counts):\n",
        "    print(f\"  Cluster {cluster}: {count} samples ({count/len(cluster_labels)*100:.1f}%)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Visualize clusters\n",
        "# If you have 2D or can reduce to 2D, create scatter plots\n",
        "# Otherwise, use PCA to reduce dimensions first (see next section)\n",
        "\n",
        "# Example: If dataset has 2 features or you want to plot first 2 features\n",
        "if X_unsupervised_scaled.shape[1] >= 2:\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    scatter = plt.scatter(X_unsupervised_scaled[:, 0], X_unsupervised_scaled[:, 1],\n",
        "                         c=cluster_labels, cmap='viridis', s=50, alpha=0.6)\n",
        "    plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1],\n",
        "               c='red', marker='x', s=200, linewidths=3, label='Centroids')\n",
        "    plt.xlabel('Feature 1')\n",
        "    plt.ylabel('Feature 2')\n",
        "    plt.title(f'K-Means Clustering (k={optimal_k})')\n",
        "    plt.legend()\n",
        "    plt.colorbar(scatter, label='Cluster')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# TODO: Analyze cluster characteristics\n",
        "# Add cluster labels to original dataframe for analysis\n",
        "df_with_clusters = df_clean.copy()\n",
        "df_with_clusters['cluster'] = cluster_labels\n",
        "\n",
        "print(\"\\nCluster Characteristics:\")\n",
        "print(df_with_clusters.groupby('cluster').mean())\n",
        "\n",
        "# TODO: Interpret clusters\n",
        "print(\"\\nInterpretation:\")\n",
        "print(\"What do the clusters represent?\")\n",
        "print(\"How do clusters differ from each other?\")\n",
        "print(\"Do clusters align with any known categories in your data?\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 4.4: Principal Component Analysis (PCA)\n",
        "\n",
        "**TODO**: Apply PCA to reduce dimensionality and visualize data in lower dimensions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# TODO: Apply Principal Component Analysis (PCA)\n",
        "# 1. Determine number of components to retain (e.g., 95% variance)\n",
        "# 2. Apply PCA transformation\n",
        "# 3. Visualize data in 2D using first two principal components\n",
        "# 4. Interpret principal components\n",
        "# ============================================================================\n",
        "\n",
        "# Apply PCA to all components first\n",
        "pca_full = PCA()\n",
        "X_pca_full = pca_full.fit_transform(X_unsupervised_scaled)\n",
        "\n",
        "# Calculate cumulative variance explained\n",
        "cumulative_variance = np.cumsum(pca_full.explained_variance_ratio_)\n",
        "\n",
        "# Visualize variance explained\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.bar(range(1, len(pca_full.explained_variance_ratio_) + 1),\n",
        "        pca_full.explained_variance_ratio_, alpha=0.7, label='Individual')\n",
        "plt.plot(range(1, len(cumulative_variance) + 1), cumulative_variance,\n",
        "         'ro-', label='Cumulative')\n",
        "plt.axhline(y=0.95, color='g', linestyle='--', label='95% threshold')\n",
        "plt.xlabel('Principal Component')\n",
        "plt.ylabel('Variance Explained')\n",
        "plt.title('Scree Plot')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(range(1, len(cumulative_variance) + 1), cumulative_variance,\n",
        "         'bo-', linewidth=2, markersize=8)\n",
        "plt.axhline(y=0.95, color='r', linestyle='--', label='95% threshold')\n",
        "plt.xlabel('Number of Components')\n",
        "plt.ylabel('Cumulative Variance Explained')\n",
        "plt.title('Cumulative Variance Explained')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# TODO: Determine number of components for 95% variance\n",
        "n_components_95 = np.argmax(cumulative_variance >= 0.95) + 1\n",
        "print(f\"Number of components explaining 95% variance: {n_components_95}\")\n",
        "print(f\"Variance explained by first {n_components_95} components: {cumulative_variance[n_components_95-1]:.2%}\")\n",
        "\n",
        "# Apply PCA with 2 components for visualization\n",
        "pca_2d = PCA(n_components=2, random_state=42)\n",
        "X_pca_2d = pca_2d.fit_transform(X_unsupervised_scaled)\n",
        "\n",
        "print(f\"\\nVariance explained by PC1: {pca_2d.explained_variance_ratio_[0]:.2%}\")\n",
        "print(f\"Variance explained by PC2: {pca_2d.explained_variance_ratio_[1]:.2%}\")\n",
        "print(f\"Total variance explained by 2 components: {sum(pca_2d.explained_variance_ratio_):.2%}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Visualize data in 2D using PCA\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# Plot 1: PCA projection colored by original target (if available)\n",
        "plt.subplot(1, 2, 1)\n",
        "if 'target' in df_clean.columns:\n",
        "    scatter = plt.scatter(X_pca_2d[:, 0], X_pca_2d[:, 1], c=df_clean['target'],\n",
        "                         cmap='viridis', s=50, alpha=0.6)\n",
        "    plt.colorbar(scatter, label='Target')\n",
        "    plt.xlabel(f'PC1 ({pca_2d.explained_variance_ratio_[0]:.2%} variance)')\n",
        "    plt.ylabel(f'PC2 ({pca_2d.explained_variance_ratio_[1]:.2%} variance)')\n",
        "    plt.title('PCA Projection (colored by target)')\n",
        "else:\n",
        "    plt.scatter(X_pca_2d[:, 0], X_pca_2d[:, 1], s=50, alpha=0.6)\n",
        "    plt.xlabel(f'PC1 ({pca_2d.explained_variance_ratio_[0]:.2%} variance)')\n",
        "    plt.ylabel(f'PC2 ({pca_2d.explained_variance_ratio_[1]:.2%} variance)')\n",
        "    plt.title('PCA Projection')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 2: PCA projection colored by clusters\n",
        "plt.subplot(1, 2, 2)\n",
        "scatter = plt.scatter(X_pca_2d[:, 0], X_pca_2d[:, 1], c=cluster_labels,\n",
        "                     cmap='viridis', s=50, alpha=0.6)\n",
        "plt.colorbar(scatter, label='Cluster')\n",
        "plt.xlabel(f'PC1 ({pca_2d.explained_variance_ratio_[0]:.2%} variance)')\n",
        "plt.ylabel(f'PC2 ({pca_2d.explained_variance_ratio_[1]:.2%} variance)')\n",
        "plt.title('PCA Projection (colored by K-Means clusters)')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# TODO: Interpret principal components\n",
        "print(\"\\nPrincipal Component Analysis:\")\n",
        "print(\"First Principal Component (PC1):\")\n",
        "print(f\"  Variance explained: {pca_2d.explained_variance_ratio_[0]:.2%}\")\n",
        "# TODO: Analyze which features contribute most to PC1\n",
        "\n",
        "print(\"\\nSecond Principal Component (PC2):\")\n",
        "print(f\"  Variance explained: {pca_2d.explained_variance_ratio_[1]:.2%}\")\n",
        "# TODO: Analyze which features contribute most to PC2\n",
        "\n",
        "# Feature contributions to principal components\n",
        "if hasattr(pca_2d, 'components_'):\n",
        "    feature_contributions = pd.DataFrame(\n",
        "        pca_2d.components_.T,\n",
        "        columns=['PC1', 'PC2'],\n",
        "        index=X.columns\n",
        "    )\n",
        "    print(\"\\nFeature contributions to principal components:\")\n",
        "    print(feature_contributions)\n",
        "\n",
        "print(\"\\nInterpretation:\")\n",
        "print(\"What patterns do you see in the PCA projection?\")\n",
        "print(\"Do clusters separate well in the PCA space?\")\n",
        "print(\"What do the principal components represent?\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Part 5: Interpretation and Insights\n",
        "\n",
        "### Step 5.1: Compare Supervised and Unsupervised Results\n",
        "\n",
        "**TODO**: Compare what you learned from supervised vs. unsupervised learning.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Insights and Comparison\n",
        "\n",
        "**TODO**: Answer the following questions:\n",
        "\n",
        "1. **Supervised Learning Insights:**\n",
        "   - Which supervised model performed better? Why?\n",
        "   - What features were most important for prediction?\n",
        "   - What are the limitations of your models?\n",
        "\n",
        "2. **Unsupervised Learning Insights:**\n",
        "   - How many clusters did you discover? Do they make sense?\n",
        "   - What characteristics define each cluster?\n",
        "   - How much variance is explained by the first 2 principal components?\n",
        "\n",
        "3. **Comparison:**\n",
        "   - Do the clusters from K-Means align with the target classes from supervised learning?\n",
        "   - What additional insights did unsupervised learning provide?\n",
        "   - How do PCA projections relate to your supervised learning results?\n",
        "\n",
        "4. **Business/Research Implications:**\n",
        "   - What actionable insights can you derive from your analysis?\n",
        "   - What recommendations would you make based on your findings?\n",
        "\n",
        "**Your answers here:**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# TODO: Create a comprehensive comparison visualization\n",
        "# Compare supervised predictions with unsupervised clusters\n",
        "# ============================================================================\n",
        "\n",
        "# TODO: If you have target labels, compare clusters with actual classes\n",
        "if 'target' in df_clean.columns:\n",
        "    # Create cross-tabulation\n",
        "    comparison_df = pd.DataFrame({\n",
        "        'Cluster': cluster_labels,\n",
        "        'Actual_Class': df_clean['target'].values\n",
        "    })\n",
        "    \n",
        "    crosstab = pd.crosstab(comparison_df['Cluster'], comparison_df['Actual_Class'])\n",
        "    print(\"Cluster vs Actual Class Comparison:\")\n",
        "    print(crosstab)\n",
        "    \n",
        "    # Visualize\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.heatmap(crosstab, annot=True, fmt='d', cmap='Blues')\n",
        "    plt.title('Cluster Assignment vs Actual Class')\n",
        "    plt.xlabel('Actual Class')\n",
        "    plt.ylabel('Cluster')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    print(\"\\nInterpretation:\")\n",
        "    print(\"How well do clusters align with actual classes?\")\n",
        "    print(\"Are there any clusters that correspond to specific classes?\")\n",
        "\n",
        "# TODO: Create summary visualization combining all results\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
        "\n",
        "# Plot 1: Original data (if 2D or using first 2 features)\n",
        "if X_unsupervised_scaled.shape[1] >= 2:\n",
        "    axes[0, 0].scatter(X_unsupervised_scaled[:, 0], X_unsupervised_scaled[:, 1],\n",
        "                      s=50, alpha=0.6)\n",
        "    axes[0, 0].set_title('Original Data (First 2 Features)')\n",
        "    axes[0, 0].set_xlabel('Feature 1')\n",
        "    axes[0, 0].set_ylabel('Feature 2')\n",
        "    axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 2: K-Means clusters\n",
        "axes[0, 1].scatter(X_pca_2d[:, 0], X_pca_2d[:, 1], c=cluster_labels,\n",
        "                  cmap='viridis', s=50, alpha=0.6)\n",
        "axes[0, 1].set_title('K-Means Clusters (PCA Projection)')\n",
        "axes[0, 1].set_xlabel('PC1')\n",
        "axes[0, 1].set_ylabel('PC2')\n",
        "axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 3: PCA colored by target (if available)\n",
        "if 'target' in df_clean.columns:\n",
        "    scatter = axes[1, 0].scatter(X_pca_2d[:, 0], X_pca_2d[:, 1], c=df_clean['target'],\n",
        "                               cmap='viridis', s=50, alpha=0.6)\n",
        "    axes[1, 0].set_title('PCA Projection (colored by target)')\n",
        "    axes[1, 0].set_xlabel('PC1')\n",
        "    axes[1, 0].set_ylabel('PC2')\n",
        "    axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# Plot 4: Variance explained\n",
        "axes[1, 1].plot(range(1, len(cumulative_variance) + 1), cumulative_variance,\n",
        "               'bo-', linewidth=2, markersize=8)\n",
        "axes[1, 1].axhline(y=0.95, color='r', linestyle='--', label='95% threshold')\n",
        "axes[1, 1].set_xlabel('Number of Components')\n",
        "axes[1, 1].set_ylabel('Cumulative Variance Explained')\n",
        "axes[1, 1].set_title('PCA Variance Explained')\n",
        "axes[1, 1].legend()\n",
        "axes[1, 1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Part 6: Summary and Reflection\n",
        "\n",
        "### Step 6.1: Key Findings Summary\n",
        "\n",
        "**TODO**: Summarize your key findings in the markdown cell below.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Summary of Key Findings\n",
        "\n",
        "**TODO**: Write a brief summary (2-3 paragraphs) of your key findings:\n",
        "\n",
        "1. **Supervised Learning Results:**\n",
        "   - Best performing model and its metrics\n",
        "   - Key features that drive predictions\n",
        "\n",
        "2. **Unsupervised Learning Results:**\n",
        "   - Number of clusters discovered and their characteristics\n",
        "   - Dimensionality reduction insights\n",
        "\n",
        "3. **Overall Insights:**\n",
        "   - What did you learn about the data?\n",
        "   - How do supervised and unsupervised results complement each other?\n",
        "\n",
        "**Your summary here:**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 6.2: Challenges and Learnings\n",
        "\n",
        "**TODO**: Reflect on challenges faced and what you learned.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Reflection Questions\n",
        "\n",
        "**TODO**: Answer the following reflection questions:\n",
        "\n",
        "1. **Challenges:**\n",
        "   - What was the most challenging part of this lab?\n",
        "   - How did you overcome it?\n",
        "\n",
        "2. **Learnings:**\n",
        "   - What new concepts or techniques did you learn?\n",
        "   - What would you do differently if you were to repeat this analysis?\n",
        "\n",
        "3. **Applications:**\n",
        "   - How could you apply these techniques to real-world problems?\n",
        "   - What additional analysis would you like to perform?\n",
        "\n",
        "**Your reflection here:**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Submission Checklist\n",
        "\n",
        "Before submitting, ensure you have:\n",
        "\n",
        "- [ ] Completed all TODO sections\n",
        "- [ ] Trained and evaluated at least 2 supervised learning models\n",
        "- [ ] Applied K-Means clustering with appropriate k selection\n",
        "- [ ] Applied PCA and visualized results\n",
        "- [ ] Created meaningful visualizations\n",
        "- [ ] Provided interpretations and insights\n",
        "- [ ] Documented your code with comments\n",
        "- [ ] Answered all reflection questions\n",
        "- [ ] Verified code runs without errors\n",
        "\n",
        "---\n",
        "\n",
        "## Additional Resources\n",
        "\n",
        "- Scikit-learn Documentation: https://scikit-learn.org/stable/\n",
        "- Pandas Documentation: https://pandas.pydata.org/docs/\n",
        "- Matplotlib Gallery: https://matplotlib.org/stable/gallery/\n",
        "- Seaborn Tutorial: https://seaborn.pydata.org/tutorial.html\n",
        "\n",
        "---\n",
        "\n",
        "## Grading Rubric\n",
        "\n",
        "| Criteria | Points | Description |\n",
        "|----------|--------|-------------|\n",
        "| Code Completion | 40 | All TODO sections completed, code runs without errors |\n",
        "| Model Evaluation | 30 | Proper evaluation metrics, meaningful comparisons |\n",
        "| Data Cleaning | 20 | Appropriate handling of missing values, outliers, preprocessing |\n",
        "| Documentation | 10 | Clear comments, interpretations, and reflections |\n",
        "\n",
        "**Total: 100 points**\n",
        "\n",
        "---\n",
        "\n",
        "Good luck with your mini-lab! üöÄ\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
