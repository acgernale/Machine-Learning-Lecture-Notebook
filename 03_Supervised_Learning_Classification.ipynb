{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Supervised Learning: Classification Models\n",
        "## Lecture Notebook for BS Data Science Students\n",
        "\n",
        "---\n",
        "\n",
        "## Learning Objectives\n",
        "\n",
        "By the end of this notebook, you will be able to:\n",
        "1. Understand what classification is and when to use it\n",
        "2. Implement and evaluate logistic regression for classification\n",
        "3. Understand decision trees and random forests\n",
        "4. Comprehend support vector machines (SVM)\n",
        "5. Apply k-nearest neighbors (k-NN) algorithm\n",
        "6. Evaluate classification models using appropriate metrics\n",
        "\n",
        "---\n",
        "\n",
        "## 3.1 Introduction to Classification\n",
        "\n",
        "### What is Classification?\n",
        "\n",
        "**Classification** is a supervised learning technique used to predict discrete categorical labels or classes. Unlike regression, which predicts continuous numerical values, classification assigns data points to predefined categories. The output of a classification model is a class label (e.g., \"spam\" or \"not spam\", \"disease\" or \"no disease\"), not a numerical value.\n",
        "\n",
        "Classification models learn to distinguish between different classes by identifying patterns in the training data that are associated with each class. The model builds a decision boundary (or multiple boundaries for multi-class problems) that separates different classes in the feature space. Once trained, the model can classify new, unseen data points by determining which side of the decision boundary they fall on.\n",
        "\n",
        "Classification is one of the most common machine learning tasks, with applications spanning virtually every domain. The ability to automatically categorize data enables automation, decision support systems, and intelligent filtering across industries.\n",
        "\n",
        "### Key Components of Classification\n",
        "\n",
        "**Features (X)**: Input variables used for prediction. These can be numerical (e.g., age, height, test scores) or categorical (e.g., color, location, product type). Features are the attributes that help distinguish between different classes.\n",
        "\n",
        "**Classes/Labels (y)**: Discrete categories to predict. In binary classification, there are two classes (e.g., positive/negative, yes/no, spam/not spam). In multi-class classification, there are three or more classes (e.g., animal species, product categories, sentiment levels).\n",
        "\n",
        "**Decision Boundary**: The surface that separates different classes in the feature space. For linear classifiers, this is a hyperplane; for non-linear classifiers, it can be a complex curved surface. The decision boundary is learned during training and determines how new data points are classified.\n",
        "\n",
        "**Class Probabilities**: Some classification models output probabilities for each class rather than just a class label. These probabilities indicate the model's confidence in its predictions and can be useful for decision-making when different misclassification costs exist.\n",
        "\n",
        "### The Classification Process\n",
        "\n",
        "The classification learning process follows these steps:\n",
        "\n",
        "1. **Model learns patterns from labeled training data**: The algorithm receives training examples with both features and their corresponding class labels.\n",
        "\n",
        "2. **Algorithm identifies decision boundaries**: The learning algorithm determines boundaries that best separate different classes based on the training examples. The complexity of these boundaries depends on the algorithm used.\n",
        "\n",
        "3. **For new data, model assigns class label**: When presented with new, unlabeled data, the model determines which side of the decision boundary the data point falls on and assigns the corresponding class label.\n",
        "\n",
        "4. **Some models provide probability estimates**: Many classification algorithms can also output the probability that a data point belongs to each class, which provides additional information beyond just the predicted class.\n",
        "\n",
        "### Common Types of Classification\n",
        "\n",
        "**Binary Classification**: The simplest form, involving exactly two classes. Examples include:\n",
        "- Email spam detection (spam vs. not spam)\n",
        "- Medical diagnosis (disease vs. no disease)\n",
        "- Credit approval (approve vs. deny)\n",
        "- Fraud detection (fraudulent vs. legitimate)\n",
        "\n",
        "**Multi-class Classification**: Involves three or more classes. Examples include:\n",
        "- Image recognition (identifying different objects)\n",
        "- Handwritten digit recognition (0-9)\n",
        "- Sentiment analysis (positive, negative, neutral)\n",
        "- Animal species classification\n",
        "\n",
        "**Multi-label Classification**: An advanced scenario where each instance can belong to multiple classes simultaneously (e.g., a news article can be tagged with multiple topics). This is beyond the scope of this introductory course but important to mention.\n",
        "\n",
        "### Uses and Applications\n",
        "\n",
        "Classification models are applied extensively across domains:\n",
        "\n",
        "- **Email Filtering**: Automatically identifying spam emails to protect users from unwanted messages and phishing attempts.\n",
        "\n",
        "- **Medical Diagnosis**: Assisting doctors in diagnosing diseases based on symptoms, test results, and patient history. For example, identifying cancerous cells in medical images or predicting disease risk.\n",
        "\n",
        "- **Image Recognition**: Classifying objects, people, or scenes in photographs. Used in security systems, autonomous vehicles, and social media applications.\n",
        "\n",
        "- **Sentiment Analysis**: Determining whether text expresses positive, negative, or neutral sentiment. Used in social media monitoring, product reviews, and customer feedback analysis.\n",
        "\n",
        "- **Credit Risk Assessment**: Banks and financial institutions use classification to determine whether to approve loan applications based on applicant characteristics and credit history.\n",
        "\n",
        "- **Customer Churn Prediction**: Identifying customers likely to cancel subscriptions or stop using services, enabling proactive retention efforts.\n",
        "\n",
        "- **Quality Control**: Manufacturing companies use classification to identify defective products automatically.\n",
        "\n",
        "The versatility and practical importance of classification make it a fundamental skill for data scientists.\n",
        "\n",
        "---\n",
        "\n",
        "## 3.2 Logistic Regression\n",
        "\n",
        "### What is Logistic Regression?\n",
        "\n",
        "**Logistic Regression** is a classification algorithm (despite its name containing \"regression\") that models the probability of a binary outcome using the logistic (sigmoid) function. While linear regression predicts continuous values, logistic regression predicts probabilities that are then converted to class labels.\n",
        "\n",
        "The key insight of logistic regression is that it uses the logistic function to map any real-valued number to a value between 0 and 1, which can be interpreted as a probability. This makes it ideal for binary classification problems where we want to know not just the class, but also the confidence (probability) of that prediction.\n",
        "\n",
        "Logistic regression is one of the most widely-used classification algorithms because it's simple, interpretable, fast to train, and provides probability estimates. It serves as an excellent baseline for classification problems and is often the first algorithm tried in practice.\n",
        "\n",
        "### The Logistic Function\n",
        "\n",
        "The core of logistic regression is the **sigmoid function** (also called the logistic function):\n",
        "\n",
        "**P(y=1) = 1 / (1 + e^(-z))**\n",
        "\n",
        "Where z = β₀ + β₁x₁ + β₂x₂ + ... + βₙxₙ\n",
        "\n",
        "The sigmoid function has an S-shaped curve that:\n",
        "- Maps any real number to a value between 0 and 1\n",
        "- Is symmetric around 0.5\n",
        "- Has steep slopes near z=0 and flatter slopes at the extremes\n",
        "- Ensures probabilities are always valid (between 0 and 1)\n",
        "\n",
        "When z is very negative, P(y=1) approaches 0. When z is very positive, P(y=1) approaches 1. When z=0, P(y=1) = 0.5, which is the decision threshold.\n",
        "\n",
        "### Key Components\n",
        "\n",
        "**Log-odds (logit)**: The natural logarithm of the odds ratio. The logit function is the inverse of the sigmoid:\n",
        "\n",
        "**logit(p) = ln(p / (1-p)) = β₀ + β₁x₁ + ... + βₙxₙ**\n",
        "\n",
        "This shows that logistic regression is linear in the log-odds space, which is why it's called \"linear\" classification.\n",
        "\n",
        "**Sigmoid Curve**: The S-shaped probability curve that ensures all predictions are valid probabilities. The steepness of the curve is controlled by the coefficients - larger coefficients create steeper transitions.\n",
        "\n",
        "**Decision Threshold**: Typically set at 0.5, meaning if P(y=1) ≥ 0.5, predict class 1; otherwise predict class 0. However, this threshold can be adjusted based on the problem requirements. For example, in medical diagnosis where false negatives are critical, you might use a lower threshold (e.g., 0.3) to catch more positive cases.\n",
        "\n",
        "### How Logistic Regression Works\n",
        "\n",
        "The training process involves:\n",
        "\n",
        "1. **Maximum Likelihood Estimation (MLE)**: Unlike linear regression which uses least squares, logistic regression uses MLE to find optimal coefficients. MLE finds the coefficient values that maximize the probability of observing the training data.\n",
        "\n",
        "2. **Coefficient Interpretation**: Each coefficient βᵢ represents the change in log-odds for a one-unit increase in feature xᵢ, holding other features constant. The exponential of βᵢ (e^βᵢ) represents the odds ratio, showing how the odds of the positive class change.\n",
        "\n",
        "3. **Decision Boundary Formation**: The decision boundary occurs where P(y=1) = 0.5, which corresponds to z = 0, or β₀ + β₁x₁ + ... + βₙxₙ = 0. This creates a linear decision boundary in the feature space.\n",
        "\n",
        "4. **Multi-class Extension**: For multi-class problems, logistic regression can be extended using:\n",
        "   - **One-vs-Rest (OvR)**: Train one binary classifier per class\n",
        "   - **Multinomial Logistic Regression**: Directly model multi-class probabilities using softmax function\n",
        "\n",
        "### Advantages and Limitations\n",
        "\n",
        "**Advantages**:\n",
        "- **Interpretable**: Coefficients have clear meaning (log-odds, odds ratios)\n",
        "- **Fast Training**: Efficient optimization algorithms available\n",
        "- **No Assumptions about Feature Distributions**: Works with various feature types\n",
        "- **Provides Probabilities**: Useful for decision-making under uncertainty\n",
        "- **Regularization Available**: Can use L1 or L2 regularization to prevent overfitting\n",
        "- **Works Well as Baseline**: Often provides competitive performance\n",
        "\n",
        "**Limitations**:\n",
        "- **Assumes Linear Decision Boundary**: Cannot capture complex non-linear relationships without feature engineering\n",
        "- **Sensitive to Outliers**: Extreme feature values can disproportionately influence the model\n",
        "- **Requires Feature Scaling**: For regularization to work properly, features should be scaled\n",
        "- **May Underperform on Complex Problems**: Non-linear algorithms often perform better on complex datasets\n",
        "\n",
        "### Evaluation Metrics for Classification\n",
        "\n",
        "**Accuracy**: The proportion of correct predictions. Simple and intuitive, but can be misleading with imbalanced datasets.\n",
        "\n",
        "**Precision**: Of all instances predicted as positive, how many are actually positive. Important when false positives are costly.\n",
        "\n",
        "**Recall (Sensitivity)**: Of all actual positive instances, how many were correctly identified. Important when false negatives are costly.\n",
        "\n",
        "**F1-Score**: Harmonic mean of precision and recall. Provides a balanced measure when you need to consider both precision and recall.\n",
        "\n",
        "**ROC Curve and AUC**: Receiver Operating Characteristic curve plots true positive rate vs. false positive rate at different thresholds. AUC (Area Under Curve) provides a single metric summarizing performance across all thresholds.\n",
        "\n",
        "**Confusion Matrix**: A table showing true positives, false positives, true negatives, and false negatives. Provides detailed insight into model performance.\n",
        "\n",
        "---\n",
        "\n",
        "## 3.3 Decision Trees\n",
        "\n",
        "### What is a Decision Tree?\n",
        "\n",
        "**Decision Trees** are tree-like models that make decisions by asking a series of questions about features, leading to leaf nodes that represent class predictions. The tree structure mimics human decision-making processes, making them highly interpretable.\n",
        "\n",
        "A decision tree consists of nodes and branches:\n",
        "- **Root Node**: The topmost node containing all training data\n",
        "- **Internal Nodes**: Decision points that test feature values (e.g., \"Is age > 30?\")\n",
        "- **Branches**: Outcomes of decisions (e.g., \"yes\" or \"no\")\n",
        "- **Leaf Nodes**: Terminal nodes that contain final predictions (class labels)\n",
        "\n",
        "Decision trees are non-parametric models, meaning they don't assume any specific form for the underlying relationship between features and target. This flexibility allows them to capture complex, non-linear relationships and interactions between features.\n",
        "\n",
        "### How Decision Trees Work\n",
        "\n",
        "The tree-building process follows these steps:\n",
        "\n",
        "1. **Start with root node**: Begin with all training data at the root.\n",
        "\n",
        "2. **Find best feature and threshold**: For each feature, find the threshold value that best splits the data. \"Best\" is measured using a splitting criterion (Gini impurity, entropy, etc.).\n",
        "\n",
        "3. **Create child nodes**: Split the data based on the chosen feature and threshold, creating child nodes for each outcome.\n",
        "\n",
        "4. **Recursively repeat**: Apply the same process to each child node until a stopping criterion is met (maximum depth reached, minimum samples per node, etc.).\n",
        "\n",
        "5. **Assign class labels**: Leaf nodes are assigned the majority class of training instances that reach that leaf.\n",
        "\n",
        "The key challenge is determining the \"best\" split at each node, which is done by maximizing information gain or minimizing impurity.\n",
        "\n",
        "### Splitting Criteria\n",
        "\n",
        "**Gini Impurity**: Measures the probability of misclassifying a randomly chosen element if it were labeled according to the class distribution in the node. Ranges from 0 (pure node, all same class) to 0.5 (maximum impurity, equal class distribution).\n",
        "\n",
        "**Gini = 1 - Σ(pᵢ)²** where pᵢ is the proportion of class i in the node.\n",
        "\n",
        "**Entropy/Information Gain**: Entropy measures the uncertainty or randomness in the node. Information gain is the reduction in entropy achieved by splitting.\n",
        "\n",
        "**Entropy = -Σ(pᵢ × log₂(pᵢ))**\n",
        "\n",
        "Higher information gain means the split better separates classes. The algorithm chooses splits that maximize information gain.\n",
        "\n",
        "**Chi-square**: A statistical test for independence. Used in some tree algorithms to test whether a split significantly improves class separation.\n",
        "\n",
        "### Advantages and Limitations\n",
        "\n",
        "**Advantages**:\n",
        "- **Highly Interpretable**: Easy to visualize and understand the decision process\n",
        "- **Handles Non-linear Relationships**: Can capture complex patterns without feature transformation\n",
        "- **Works with Mixed Data Types**: Handles both numerical and categorical features naturally\n",
        "- **No Feature Scaling Needed**: Tree-based algorithms are scale-invariant\n",
        "- **Feature Importance Available**: Can identify which features are most important\n",
        "- **Handles Missing Values**: Some implementations can handle missing data gracefully\n",
        "\n",
        "**Limitations**:\n",
        "- **Prone to Overfitting**: Trees can grow very deep and memorize training data\n",
        "- **Sensitive to Small Data Changes**: Small changes in data can result in very different trees\n",
        "- **Can Create Biased Trees**: With imbalanced classes, trees may favor majority class\n",
        "- **Greedy Algorithm**: May miss optimal splits by choosing locally optimal splits\n",
        "- **Not Suitable for Extrapolation**: Poor performance outside training data range\n",
        "\n",
        "### Preventing Overfitting\n",
        "\n",
        "Several techniques help prevent overfitting in decision trees:\n",
        "\n",
        "**Maximum Depth**: Limit how deep the tree can grow. Shallow trees are simpler and less prone to overfitting.\n",
        "\n",
        "**Minimum Samples per Split**: Require a minimum number of samples before allowing a split. Prevents splits on very small subsets.\n",
        "\n",
        "**Minimum Samples per Leaf**: Require a minimum number of samples in leaf nodes. Ensures predictions are based on sufficient data.\n",
        "\n",
        "**Pruning**: Remove branches that don't significantly improve performance:\n",
        "- **Pre-pruning**: Stop tree growth early based on criteria\n",
        "- **Post-pruning**: Grow full tree, then remove branches that don't improve validation performance\n",
        "\n",
        "**Cost Complexity Pruning**: Balance tree complexity against performance using a complexity parameter.\n",
        "\n",
        "---\n",
        "\n",
        "## 3.4 Random Forest\n",
        "\n",
        "### What is Random Forest?\n",
        "\n",
        "**Random Forest** is an ensemble method that combines multiple decision trees to improve prediction accuracy and reduce overfitting. The \"random\" aspect comes from two sources of randomness: bootstrap sampling of data and random feature selection at each split.\n",
        "\n",
        "Random Forest addresses the main weaknesses of individual decision trees:\n",
        "- Reduces overfitting by averaging predictions from multiple trees\n",
        "- Improves generalization by combining diverse trees\n",
        "- Provides more stable predictions than single trees\n",
        "- Maintains interpretability through feature importance\n",
        "\n",
        "The idea behind Random Forest is that while individual trees may overfit, averaging predictions from many diverse trees reduces variance and improves generalization. This is an example of the \"wisdom of crowds\" principle in machine learning.\n",
        "\n",
        "### Key Concepts\n",
        "\n",
        "**Ensemble Learning**: Combining multiple models to achieve better performance than any single model. Random Forest is a type of ensemble called \"bagging\" (bootstrap aggregating).\n",
        "\n",
        "**Bootstrap Aggregating (Bagging)**: Each tree is trained on a random sample (with replacement) of the training data. This creates diversity among trees and reduces overfitting.\n",
        "\n",
        "**Feature Randomness**: At each split, each tree considers only a random subset of features. This further increases diversity and prevents trees from always using the same strong features.\n",
        "\n",
        "**Voting**: For classification, the final prediction is the majority vote of all trees. For regression, it's the average prediction. This aggregation reduces variance and improves robustness.\n",
        "\n",
        "### How Random Forest Works\n",
        "\n",
        "The Random Forest algorithm follows these steps:\n",
        "\n",
        "1. **Create Bootstrap Samples**: Randomly sample the training data with replacement to create multiple datasets. Each dataset has the same size as the original but may contain duplicate samples.\n",
        "\n",
        "2. **Train Decision Trees**: For each bootstrap sample, train a decision tree. At each split in each tree, randomly select a subset of features to consider (typically √n features for classification, where n is total features).\n",
        "\n",
        "3. **Grow Trees Fully**: Unlike single decision trees which may be pruned, Random Forest trees are typically grown to maximum depth or until all leaves are pure.\n",
        "\n",
        "4. **Aggregate Predictions**: For new data, get predictions from all trees and combine them:\n",
        "   - **Classification**: Majority vote (most common class)\n",
        "   - **Regression**: Average of all predictions\n",
        "\n",
        "5. **Calculate Feature Importance**: Average the importance scores from all trees to get overall feature importance.\n",
        "\n",
        "### Advantages\n",
        "\n",
        "**Reduces Overfitting**: By averaging multiple trees, Random Forest reduces the variance that causes overfitting in individual trees.\n",
        "\n",
        "**Handles Missing Values**: Can handle missing data through surrogate splits or by using median/mode imputation.\n",
        "\n",
        "**Provides Feature Importance**: Aggregates feature importance across all trees, giving reliable importance scores.\n",
        "\n",
        "**Works Well with Default Parameters**: Often performs well without extensive hyperparameter tuning, making it easy to use.\n",
        "\n",
        "**Less Sensitive to Outliers**: Averaging across trees reduces the impact of outliers.\n",
        "\n",
        "**Handles Large Datasets**: Can be parallelized efficiently, making it suitable for large datasets.\n",
        "\n",
        "**No Feature Scaling Required**: Like decision trees, Random Forest doesn't require feature scaling.\n",
        "\n",
        "### Hyperparameters\n",
        "\n",
        "Key hyperparameters to tune:\n",
        "\n",
        "**n_estimators**: Number of trees in the forest. More trees generally improve performance but increase computation time. Typically 100-500 trees.\n",
        "\n",
        "**max_depth**: Maximum depth of trees. Deeper trees can capture more complex patterns but may overfit. None allows full growth.\n",
        "\n",
        "**min_samples_split**: Minimum samples required to split a node. Higher values create simpler trees.\n",
        "\n",
        "**min_samples_leaf**: Minimum samples required in a leaf node. Higher values create more conservative trees.\n",
        "\n",
        "**max_features**: Number of features to consider at each split. Common choices: 'sqrt' (√n), 'log2' (log₂n), or a specific number.\n",
        "\n",
        "**bootstrap**: Whether to use bootstrap sampling (default True). Can set to False to use all data for each tree.\n",
        "\n",
        "---\n",
        "\n",
        "## 3.5 Support Vector Machines (SVM)\n",
        "\n",
        "### What is SVM?\n",
        "\n",
        "**Support Vector Machines (SVM)** is a powerful classification algorithm that finds the optimal hyperplane (decision boundary) that maximizes the margin between classes. The \"support vectors\" are the data points closest to the decision boundary, and these points alone determine the boundary.\n",
        "\n",
        "SVM is based on the idea that the best decision boundary is the one that maximizes the separation (margin) between classes. This makes SVM particularly robust and less prone to overfitting, as it focuses on the most difficult-to-classify points (support vectors) rather than trying to fit all training points perfectly.\n",
        "\n",
        "SVM can handle both linear and non-linear classification problems through the use of kernel functions, making it versatile for various types of data.\n",
        "\n",
        "### Key Concepts\n",
        "\n",
        "**Support Vectors**: The data points closest to the decision boundary. These are the \"critical\" points that define the boundary. Only support vectors affect the model - other points can be removed without changing the decision boundary.\n",
        "\n",
        "**Margin**: The distance between the decision boundary and the nearest data points of each class. SVM aims to maximize this margin, creating the widest possible \"street\" between classes.\n",
        "\n",
        "**Hyperplane**: In n-dimensional space, a hyperplane is an (n-1)-dimensional flat surface that separates the space. For 2D data, it's a line; for 3D data, it's a plane.\n",
        "\n",
        "**Kernel Trick**: A method to handle non-linear relationships by mapping data to a higher-dimensional space where it becomes linearly separable, without explicitly computing the transformation.\n",
        "\n",
        "### How SVM Works\n",
        "\n",
        "The SVM algorithm works as follows:\n",
        "\n",
        "1. **Find Optimal Hyperplane**: SVM finds the hyperplane that maximizes the margin between classes. This is formulated as a constrained optimization problem.\n",
        "\n",
        "2. **Support Vectors Determine Boundary**: Only the support vectors (points on the margin boundaries) influence the decision boundary. This makes SVM memory-efficient.\n",
        "\n",
        "3. **Handle Non-linear Problems**: For non-linear problems, SVM uses kernel functions to map data to higher dimensions where it becomes linearly separable:\n",
        "   - **Linear Kernel**: For linearly separable data\n",
        "   - **Polynomial Kernel**: Captures polynomial relationships\n",
        "   - **RBF (Radial Basis Function) Kernel**: Most common, handles complex non-linear boundaries\n",
        "   - **Sigmoid Kernel**: Similar to neural network activation\n",
        "\n",
        "4. **Soft Margin**: Real data is rarely perfectly separable. Soft margin SVM allows some misclassification using slack variables, controlled by the C parameter.\n",
        "\n",
        "### Types of SVM\n",
        "\n",
        "**Hard Margin SVM**: Assumes data is perfectly linearly separable. Rarely used in practice because real data almost always has some overlap or noise.\n",
        "\n",
        "**Soft Margin SVM**: Allows some misclassification to handle noisy or overlapping data. The C parameter controls the trade-off between maximizing margin and minimizing misclassification. Higher C means less tolerance for misclassification.\n",
        "\n",
        "**Kernel SVM**: Uses kernel functions to handle non-linear relationships. The RBF kernel is most popular and works well for a wide variety of problems.\n",
        "\n",
        "### Advantages and Limitations\n",
        "\n",
        "**Advantages**:\n",
        "- **Effective in High Dimensions**: Works well even when number of features exceeds number of samples\n",
        "- **Memory Efficient**: Uses only support vectors, not all training data\n",
        "- **Versatile**: Different kernels handle various problem types\n",
        "- **Works Well with Clear Margin**: Excellent when classes are well-separated\n",
        "- **Regularization Built-in**: The margin maximization provides natural regularization\n",
        "\n",
        "**Limitations**:\n",
        "- **Poor Performance on Large Datasets**: Training time scales poorly with dataset size\n",
        "- **Sensitive to Feature Scaling**: Requires features to be on similar scales\n",
        "- **Doesn't Provide Probability Estimates Directly**: Requires additional calibration step\n",
        "- **Less Interpretable**: Harder to understand than tree-based methods\n",
        "- **Hyperparameter Sensitive**: Performance depends heavily on C and kernel parameters\n",
        "- **Not Suitable for Noisy Data**: Can be sensitive to outliers\n",
        "\n",
        "---\n",
        "\n",
        "## 3.6 k-Nearest Neighbors (k-NN)\n",
        "\n",
        "### What is k-NN?\n",
        "\n",
        "**k-Nearest Neighbors (k-NN)** is a simple, instance-based learning algorithm that classifies data points based on the majority class of their k nearest neighbors. Unlike most algorithms that build explicit models during training, k-NN is a \"lazy learner\" that stores all training data and computes predictions on-demand.\n",
        "\n",
        "The fundamental assumption of k-NN is that similar instances (close in feature space) belong to the same class. This is a reasonable assumption for many problems and makes k-NN intuitive and easy to understand.\n",
        "\n",
        "k-NN is non-parametric, meaning it doesn't assume any specific form for the underlying data distribution. This makes it flexible and able to capture complex decision boundaries.\n",
        "\n",
        "### Key Concepts\n",
        "\n",
        "**Instance-Based Learning**: k-NN doesn't learn a model during training. Instead, it memorizes all training examples and uses them directly for prediction. This is why it's called \"lazy learning\" - the work is done at prediction time.\n",
        "\n",
        "**Distance Metrics**: k-NN relies on distance calculations to find nearest neighbors:\n",
        "- **Euclidean Distance**: Straight-line distance in feature space (most common)\n",
        "- **Manhattan Distance**: Sum of absolute differences (useful for high-dimensional data)\n",
        "- **Minkowski Distance**: Generalization of Euclidean and Manhattan\n",
        "- **Hamming Distance**: For categorical data\n",
        "\n",
        "**k Parameter**: The number of neighbors to consider. This is the main hyperparameter:\n",
        "- Small k (e.g., k=1): More sensitive to noise, lower bias but higher variance\n",
        "- Large k (e.g., k=20): Smoother decision boundaries, higher bias but lower variance\n",
        "- Common practice: Use odd k for binary classification to avoid ties\n",
        "\n",
        "**Lazy Learning**: The algorithm doesn't perform any computation during training - it simply stores the data. All computation happens during prediction, which can be slow for large datasets.\n",
        "\n",
        "### How k-NN Works\n",
        "\n",
        "The k-NN algorithm follows these steps:\n",
        "\n",
        "1. **Store Training Data**: Simply memorize all training examples (no model building).\n",
        "\n",
        "2. **For New Data Point**: Calculate distances to all training points using chosen distance metric.\n",
        "\n",
        "3. **Identify k Nearest Neighbors**: Find the k training points closest to the new point.\n",
        "\n",
        "4. **For Classification**: Majority vote of neighbors' classes. For example, if k=5 and 3 neighbors are class A and 2 are class B, predict class A.\n",
        "\n",
        "5. **For Regression**: Average of neighbors' values (though k-NN is primarily used for classification).\n",
        "\n",
        "The simplicity of this algorithm is both its strength and weakness - it's easy to understand and implement, but can be computationally expensive for large datasets.\n",
        "\n",
        "### Choosing k\n",
        "\n",
        "Selecting the right k is crucial:\n",
        "\n",
        "**Small k (k=1 or k=3)**:\n",
        "- More sensitive to local patterns and noise\n",
        "- Lower bias (can capture complex boundaries)\n",
        "- Higher variance (predictions vary more with small data changes)\n",
        "- Risk of overfitting\n",
        "\n",
        "**Large k (k=15 or k=20)**:\n",
        "- Smoother decision boundaries\n",
        "- Higher bias (may miss local patterns)\n",
        "- Lower variance (more stable predictions)\n",
        "- Risk of underfitting\n",
        "\n",
        "**Best Practices**:\n",
        "- Use odd k for binary classification to avoid ties\n",
        "- Use cross-validation to select optimal k\n",
        "- Consider dataset size: larger datasets can use larger k\n",
        "- Start with k = √n (square root of number of samples) as a rule of thumb\n",
        "\n",
        "### Advantages and Limitations\n",
        "\n",
        "**Advantages**:\n",
        "- **Simple to Understand**: Intuitive algorithm that's easy to explain\n",
        "- **No Assumptions**: Doesn't assume any specific data distribution\n",
        "- **Works Well for Non-linear Problems**: Can capture complex decision boundaries\n",
        "- **Naturally Handles Multi-class**: No special modifications needed\n",
        "- **No Training Time**: Instant \"training\" (just storing data)\n",
        "- **Adapts to Local Patterns**: Can capture local structure in data\n",
        "\n",
        "**Limitations**:\n",
        "- **Computationally Expensive**: Must compute distances to all training points for each prediction\n",
        "- **Sensitive to Irrelevant Features**: All features contribute equally to distance\n",
        "- **Requires Feature Scaling**: Features on different scales will dominate distance calculations\n",
        "- **Performance Depends on Distance Metric**: Choice of metric significantly affects results\n",
        "- **Sensitive to Curse of Dimensionality**: Performance degrades in high-dimensional spaces\n",
        "- **Memory Intensive**: Must store all training data\n",
        "- **Slow Prediction**: Prediction time increases with dataset size\n",
        "\n",
        "---\n",
        "\n",
        "## Summary and Key Takeaways\n",
        "\n",
        "1. **Classification** predicts discrete class labels by learning decision boundaries in feature space.\n",
        "\n",
        "2. **Logistic Regression** provides interpretable, probabilistic predictions with linear decision boundaries.\n",
        "\n",
        "3. **Decision Trees** offer high interpretability and can capture non-linear relationships but are prone to overfitting.\n",
        "\n",
        "4. **Random Forest** combines multiple trees to reduce overfitting while maintaining interpretability through feature importance.\n",
        "\n",
        "5. **SVM** maximizes margin between classes and can handle non-linear problems through kernels, but requires feature scaling.\n",
        "\n",
        "6. **k-NN** is simple and intuitive but computationally expensive, requiring careful feature scaling and distance metric selection.\n",
        "\n",
        "7. **Evaluation Metrics** (accuracy, precision, recall, F1, ROC-AUC) must be chosen based on problem requirements and class imbalance.\n",
        "\n",
        "8. **Model Selection** depends on interpretability needs, dataset size, feature characteristics, and performance requirements.\n",
        "\n",
        "---\n",
        "\n",
        "## Further Reading\n",
        "\n",
        "- James, G., et al. (2013). *An Introduction to Statistical Learning*. Springer.\n",
        "- Scikit-learn Classification Documentation: https://scikit-learn.org/stable/supervised_learning.html#classification\n",
        "\n",
        "---\n",
        "\n",
        "## Practice Exercises\n",
        "\n",
        "1. Explain the difference between precision and recall. When would you prioritize one over the other?\n",
        "\n",
        "2. Why are decision trees prone to overfitting? What techniques can prevent this?\n",
        "\n",
        "3. How does Random Forest address the limitations of individual decision trees?\n",
        "\n",
        "4. What is the kernel trick in SVM? Why is it useful?\n",
        "\n",
        "5. Why does k-NN require feature scaling? What happens if features aren't scaled?\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import (accuracy_score, precision_score, recall_score, \n",
        "                            f1_score, confusion_matrix, classification_report,\n",
        "                            roc_curve, auc, roc_auc_score)\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set style\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"Libraries imported successfully!\")\n",
        "print(\"Ready to explore classification models!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Example 1: Logistic Regression\n",
        "\n",
        "Let's start with a logistic regression example for binary classification:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate sample data for binary classification\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_curve, auc\n",
        "\n",
        "# Create synthetic dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=2, n_redundant=0, \n",
        "                           n_informative=2, n_clusters_per_class=1, \n",
        "                           random_state=42, class_sep=0.8)\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Scale features (important for logistic regression with regularization)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train logistic regression\n",
        "lr = LogisticRegression(random_state=42, max_iter=1000)\n",
        "lr.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_train_pred = lr.predict(X_train_scaled)\n",
        "y_test_pred = lr.predict(X_test_scaled)\n",
        "y_test_proba = lr.predict_proba(X_test_scaled)[:, 1]  # Probability of class 1\n",
        "\n",
        "# Calculate metrics\n",
        "print(\"Logistic Regression Results:\")\n",
        "print(f\"Training Accuracy: {accuracy_score(y_train, y_train_pred):.4f}\")\n",
        "print(f\"Test Accuracy: {accuracy_score(y_test, y_test_pred):.4f}\")\n",
        "print(f\"\\nTest Set Metrics:\")\n",
        "print(f\"  Precision: {precision_score(y_test, y_test_pred):.4f}\")\n",
        "print(f\"  Recall: {recall_score(y_test, y_test_pred):.4f}\")\n",
        "print(f\"  F1-Score: {f1_score(y_test, y_test_pred):.4f}\")\n",
        "\n",
        "# Confusion Matrix\n",
        "cm = confusion_matrix(y_test, y_test_pred)\n",
        "print(f\"\\nConfusion Matrix:\")\n",
        "print(cm)\n",
        "\n",
        "# ROC Curve\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_test_proba)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "# Visualize\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
        "\n",
        "# Decision boundary\n",
        "ax = axes[0]\n",
        "h = 0.02\n",
        "x_min, x_max = X_test_scaled[:, 0].min() - 1, X_test_scaled[:, 0].max() + 1\n",
        "y_min, y_max = X_test_scaled[:, 1].min() - 1, X_test_scaled[:, 1].max() + 1\n",
        "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
        "                     np.arange(y_min, y_max, h))\n",
        "Z = lr.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "Z = Z.reshape(xx.shape)\n",
        "ax.contourf(xx, yy, Z, alpha=0.3, cmap=plt.cm.RdYlBu)\n",
        "ax.scatter(X_test_scaled[y_test==0, 0], X_test_scaled[y_test==0, 1], \n",
        "           c='blue', marker='o', label='Class 0', alpha=0.6)\n",
        "ax.scatter(X_test_scaled[y_test==1, 0], X_test_scaled[y_test==1, 1], \n",
        "           c='red', marker='s', label='Class 1', alpha=0.6)\n",
        "ax.set_xlabel('Feature 1')\n",
        "ax.set_ylabel('Feature 2')\n",
        "ax.set_title('Decision Boundary')\n",
        "ax.legend()\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "# Confusion Matrix Heatmap\n",
        "ax = axes[1]\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax)\n",
        "ax.set_xlabel('Predicted')\n",
        "ax.set_ylabel('Actual')\n",
        "ax.set_title('Confusion Matrix')\n",
        "\n",
        "# ROC Curve\n",
        "ax = axes[2]\n",
        "ax.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
        "ax.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random')\n",
        "ax.set_xlabel('False Positive Rate')\n",
        "ax.set_ylabel('True Positive Rate')\n",
        "ax.set_title('ROC Curve')\n",
        "ax.legend()\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nCoefficients: {lr.coef_[0]}\")\n",
        "print(f\"Intercept: {lr.intercept_[0]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Example 2: Decision Trees\n",
        "\n",
        "Demonstrating decision trees with visualization:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
        "\n",
        "# Create dataset\n",
        "X_tree, y_tree = make_classification(n_samples=300, n_features=2, n_redundant=0,\n",
        "                                     n_informative=2, n_clusters_per_class=1,\n",
        "                                     random_state=42)\n",
        "\n",
        "X_train_t, X_test_t, y_train_t, y_test_t = train_test_split(\n",
        "    X_tree, y_tree, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Train decision trees with different depths\n",
        "depths = [2, 5, 10]\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "for idx, depth in enumerate(depths):\n",
        "    dt = DecisionTreeClassifier(max_depth=depth, random_state=42)\n",
        "    dt.fit(X_train_t, y_train_t)\n",
        "    \n",
        "    train_acc = accuracy_score(y_train_t, dt.predict(X_train_t))\n",
        "    test_acc = accuracy_score(y_test_t, dt.predict(X_test_t))\n",
        "    \n",
        "    # Plot decision boundary\n",
        "    h = 0.02\n",
        "    x_min, x_max = X_tree[:, 0].min() - 1, X_tree[:, 0].max() + 1\n",
        "    y_min, y_max = X_tree[:, 1].min() - 1, X_tree[:, 1].max() + 1\n",
        "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
        "                         np.arange(y_min, y_max, h))\n",
        "    Z = dt.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "    Z = Z.reshape(xx.shape)\n",
        "    \n",
        "    axes[idx].contourf(xx, yy, Z, alpha=0.3, cmap=plt.cm.RdYlBu)\n",
        "    axes[idx].scatter(X_train_t[y_train_t==0, 0], X_train_t[y_train_t==0, 1],\n",
        "                     c='blue', marker='o', label='Class 0', alpha=0.6)\n",
        "    axes[idx].scatter(X_train_t[y_train_t==1, 0], X_train_t[y_train_t==1, 1],\n",
        "                     c='red', marker='s', label='Class 1', alpha=0.6)\n",
        "    axes[idx].set_title(f'Max Depth = {depth}\\nTrain Acc: {train_acc:.3f}, Test Acc: {test_acc:.3f}')\n",
        "    axes[idx].set_xlabel('Feature 1')\n",
        "    axes[idx].set_ylabel('Feature 2')\n",
        "    axes[idx].legend()\n",
        "    axes[idx].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Visualize tree structure\n",
        "dt_vis = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
        "dt_vis.fit(X_train_t, y_train_t)\n",
        "\n",
        "plt.figure(figsize=(20, 10))\n",
        "plot_tree(dt_vis, filled=True, feature_names=['Feature 1', 'Feature 2'],\n",
        "          class_names=['Class 0', 'Class 1'], fontsize=10)\n",
        "plt.title('Decision Tree Structure (Max Depth = 3)')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nFeature Importances:\")\n",
        "print(f\"Feature 1: {dt_vis.feature_importances_[0]:.4f}\")\n",
        "print(f\"Feature 2: {dt_vis.feature_importances_[1]:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Example 3: Random Forest\n",
        "\n",
        "Comparing Random Forest with a single decision tree:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create more complex dataset\n",
        "X_rf, y_rf = make_classification(n_samples=1000, n_features=10, n_informative=5,\n",
        "                                 n_redundant=2, n_clusters_per_class=1,\n",
        "                                 random_state=42)\n",
        "\n",
        "X_train_rf, X_test_rf, y_train_rf, y_test_rf = train_test_split(\n",
        "    X_rf, y_rf, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Train single decision tree\n",
        "dt_single = DecisionTreeClassifier(max_depth=10, random_state=42)\n",
        "dt_single.fit(X_train_rf, y_train_rf)\n",
        "\n",
        "# Train Random Forest\n",
        "rf = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)\n",
        "rf.fit(X_train_rf, y_train_rf)\n",
        "\n",
        "# Predictions\n",
        "dt_train_pred = dt_single.predict(X_train_rf)\n",
        "dt_test_pred = dt_single.predict(X_test_rf)\n",
        "rf_train_pred = rf.predict(X_train_rf)\n",
        "rf_test_pred = rf.predict(X_test_rf)\n",
        "\n",
        "# Compare performance\n",
        "print(\"Model Comparison:\")\n",
        "print(f\"\\nSingle Decision Tree:\")\n",
        "print(f\"  Training Accuracy: {accuracy_score(y_train_rf, dt_train_pred):.4f}\")\n",
        "print(f\"  Test Accuracy: {accuracy_score(y_test_rf, dt_test_pred):.4f}\")\n",
        "print(f\"  Test F1-Score: {f1_score(y_test_rf, dt_test_pred):.4f}\")\n",
        "\n",
        "print(f\"\\nRandom Forest (100 trees):\")\n",
        "print(f\"  Training Accuracy: {accuracy_score(y_train_rf, rf_train_pred):.4f}\")\n",
        "print(f\"  Test Accuracy: {accuracy_score(y_test_rf, rf_test_pred):.4f}\")\n",
        "print(f\"  Test F1-Score: {f1_score(y_test_rf, rf_test_pred):.4f}\")\n",
        "\n",
        "# Feature importance comparison\n",
        "feature_names = [f'Feature {i+1}' for i in range(10)]\n",
        "importance_df = pd.DataFrame({\n",
        "    'Feature': feature_names,\n",
        "    'Decision Tree': dt_single.feature_importances_,\n",
        "    'Random Forest': rf.feature_importances_\n",
        "})\n",
        "\n",
        "# Visualize feature importance\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
        "\n",
        "importance_df_sorted = importance_df.sort_values('Random Forest', ascending=True)\n",
        "axes[0].barh(importance_df_sorted['Feature'], importance_df_sorted['Decision Tree'])\n",
        "axes[0].set_xlabel('Importance')\n",
        "axes[0].set_title('Decision Tree Feature Importance')\n",
        "axes[0].grid(True, alpha=0.3, axis='x')\n",
        "\n",
        "axes[1].barh(importance_df_sorted['Feature'], importance_df_sorted['Random Forest'])\n",
        "axes[1].set_xlabel('Importance')\n",
        "axes[1].set_title('Random Forest Feature Importance')\n",
        "axes[1].grid(True, alpha=0.3, axis='x')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nObservations:\")\n",
        "print(\"- Random Forest typically has better generalization (lower overfitting)\")\n",
        "print(\"- Feature importances are more stable in Random Forest\")\n",
        "print(\"- Random Forest uses averaging to reduce variance\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Example 4: Support Vector Machine (SVM)\n",
        "\n",
        "Demonstrating SVM with different kernels:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create non-linearly separable data\n",
        "from sklearn.datasets import make_circles\n",
        "\n",
        "X_svm, y_svm = make_circles(n_samples=300, noise=0.1, factor=0.5, random_state=42)\n",
        "X_train_svm, X_test_svm, y_train_svm, y_test_svm = train_test_split(\n",
        "    X_svm, y_svm, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Scale features (important for SVM)\n",
        "scaler_svm = StandardScaler()\n",
        "X_train_svm_scaled = scaler_svm.fit_transform(X_train_svm)\n",
        "X_test_svm_scaled = scaler_svm.transform(X_test_svm)\n",
        "\n",
        "# Train SVMs with different kernels\n",
        "kernels = ['linear', 'poly', 'rbf']\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "for idx, kernel in enumerate(kernels):\n",
        "    svm = SVC(kernel=kernel, random_state=42, probability=True)\n",
        "    svm.fit(X_train_svm_scaled, y_train_svm)\n",
        "    \n",
        "    train_acc = accuracy_score(y_train_svm, svm.predict(X_train_svm_scaled))\n",
        "    test_acc = accuracy_score(y_test_svm, svm.predict(X_test_svm_scaled))\n",
        "    \n",
        "    # Plot decision boundary\n",
        "    h = 0.02\n",
        "    x_min, x_max = X_train_svm_scaled[:, 0].min() - 0.5, X_train_svm_scaled[:, 0].max() + 0.5\n",
        "    y_min, y_max = X_train_svm_scaled[:, 1].min() - 0.5, X_train_svm_scaled[:, 1].max() + 0.5\n",
        "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
        "                         np.arange(y_min, y_max, h))\n",
        "    Z = svm.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "    Z = Z.reshape(xx.shape)\n",
        "    \n",
        "    axes[idx].contourf(xx, yy, Z, alpha=0.3, cmap=plt.cm.RdYlBu)\n",
        "    axes[idx].scatter(X_train_svm_scaled[y_train_svm==0, 0], X_train_svm_scaled[y_train_svm==0, 1],\n",
        "                     c='blue', marker='o', label='Class 0', alpha=0.6)\n",
        "    axes[idx].scatter(X_train_svm_scaled[y_train_svm==1, 0], X_train_svm_scaled[y_train_svm==1, 1],\n",
        "                     c='red', marker='s', label='Class 1', alpha=0.6)\n",
        "    \n",
        "    # Highlight support vectors\n",
        "    axes[idx].scatter(svm.support_vectors_[:, 0], svm.support_vectors_[:, 1],\n",
        "                     s=100, facecolors='none', edgecolors='black', linewidths=2,\n",
        "                     label='Support Vectors')\n",
        "    \n",
        "    axes[idx].set_title(f'{kernel.upper()} Kernel\\nTrain: {train_acc:.3f}, Test: {test_acc:.3f}\\nSupport Vectors: {len(svm.support_vectors_)}')\n",
        "    axes[idx].set_xlabel('Feature 1')\n",
        "    axes[idx].set_ylabel('Feature 2')\n",
        "    axes[idx].legend()\n",
        "    axes[idx].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nObservations:\")\n",
        "print(\"- Linear kernel: Simple linear boundary, may not work for non-linear data\")\n",
        "print(\"- Polynomial kernel: Can capture non-linear relationships\")\n",
        "print(\"- RBF kernel: Most flexible, can handle complex boundaries\")\n",
        "print(\"- Support vectors are the critical points that define the decision boundary\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Example 5: k-Nearest Neighbors (k-NN)\n",
        "\n",
        "Demonstrating k-NN with different k values:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create dataset\n",
        "X_knn, y_knn = make_classification(n_samples=200, n_features=2, n_redundant=0,\n",
        "                                  n_informative=2, n_clusters_per_class=1,\n",
        "                                  random_state=42)\n",
        "\n",
        "X_train_knn, X_test_knn, y_train_knn, y_test_knn = train_test_split(\n",
        "    X_knn, y_knn, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Scale features (critical for k-NN)\n",
        "scaler_knn = StandardScaler()\n",
        "X_train_knn_scaled = scaler_knn.fit_transform(X_train_knn)\n",
        "X_test_knn_scaled = scaler_knn.transform(X_test_knn)\n",
        "\n",
        "# Test different k values\n",
        "k_values = [1, 5, 20]\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "for idx, k in enumerate(k_values):\n",
        "    knn = KNeighborsClassifier(n_neighbors=k)\n",
        "    knn.fit(X_train_knn_scaled, y_train_knn)\n",
        "    \n",
        "    train_acc = accuracy_score(y_train_knn, knn.predict(X_train_knn_scaled))\n",
        "    test_acc = accuracy_score(y_test_knn, knn.predict(X_test_knn_scaled))\n",
        "    \n",
        "    # Plot decision boundary\n",
        "    h = 0.02\n",
        "    x_min, x_max = X_train_knn_scaled[:, 0].min() - 1, X_train_knn_scaled[:, 0].max() + 1\n",
        "    y_min, y_max = X_train_knn_scaled[:, 1].min() - 1, X_train_knn_scaled[:, 1].max() + 1\n",
        "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
        "                         np.arange(y_min, y_max, h))\n",
        "    Z = knn.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "    Z = Z.reshape(xx.shape)\n",
        "    \n",
        "    axes[idx].contourf(xx, yy, Z, alpha=0.3, cmap=plt.cm.RdYlBu)\n",
        "    axes[idx].scatter(X_train_knn_scaled[y_train_knn==0, 0], X_train_knn_scaled[y_train_knn==0, 1],\n",
        "                     c='blue', marker='o', label='Class 0', alpha=0.6, s=50)\n",
        "    axes[idx].scatter(X_train_knn_scaled[y_train_knn==1, 0], X_train_knn_scaled[y_train_knn==1, 1],\n",
        "                     c='red', marker='s', label='Class 1', alpha=0.6, s=50)\n",
        "    axes[idx].set_title(f'k = {k}\\nTrain: {train_acc:.3f}, Test: {test_acc:.3f}')\n",
        "    axes[idx].set_xlabel('Feature 1')\n",
        "    axes[idx].set_ylabel('Feature 2')\n",
        "    axes[idx].legend()\n",
        "    axes[idx].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Compare k values\n",
        "k_range = range(1, 31)\n",
        "train_scores = []\n",
        "test_scores = []\n",
        "\n",
        "for k in k_range:\n",
        "    knn = KNeighborsClassifier(n_neighbors=k)\n",
        "    knn.fit(X_train_knn_scaled, y_train_knn)\n",
        "    train_scores.append(accuracy_score(y_train_knn, knn.predict(X_train_knn_scaled)))\n",
        "    test_scores.append(accuracy_score(y_test_knn, knn.predict(X_test_knn_scaled)))\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(k_range, train_scores, 'o-', label='Training Accuracy', linewidth=2)\n",
        "plt.plot(k_range, test_scores, 's-', label='Test Accuracy', linewidth=2)\n",
        "plt.xlabel('k (Number of Neighbors)')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('k-NN: Effect of k on Performance')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nObservations:\")\n",
        "print(\"- Small k (k=1): More complex boundary, may overfit\")\n",
        "print(\"- Large k (k=20): Smoother boundary, may underfit\")\n",
        "print(\"- Optimal k balances bias and variance\")\n",
        "print(\"- Feature scaling is critical for k-NN (distance-based algorithm)\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
