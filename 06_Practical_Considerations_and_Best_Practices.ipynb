{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Practical Considerations and Best Practices\n",
        "## Lecture Notebook for BS Data Science Students\n",
        "\n",
        "---\n",
        "\n",
        "## Learning Objectives\n",
        "\n",
        "By the end of this notebook, you will be able to:\n",
        "1. Understand data preprocessing techniques and their importance\n",
        "2. Perform feature engineering and feature selection\n",
        "3. Handle common pitfalls in machine learning\n",
        "4. Understand model interpretability and its importance\n",
        "5. Apply best practices throughout the ML pipeline\n",
        "6. Recognize and avoid common mistakes\n",
        "\n",
        "---\n",
        "\n",
        "## 6.1 Data Preprocessing\n",
        "\n",
        "### Handling Missing Values\n",
        "\n",
        "**Missing values** are common in real-world datasets and must be handled appropriately. The approach depends on:\n",
        "- **Amount of missing data**: Small vs. large proportion\n",
        "- **Pattern of missingness**: Random vs. systematic\n",
        "- **Type of feature**: Numerical vs. categorical\n",
        "- **Importance of feature**: Critical vs. auxiliary\n",
        "\n",
        "#### Strategies for Handling Missing Values\n",
        "\n",
        "**Deletion**:\n",
        "- **Listwise Deletion**: Remove rows with any missing values. Simple but can lose significant data.\n",
        "- **Pairwise Deletion**: Use available data for each analysis. Can lead to inconsistent sample sizes.\n",
        "- **When to Use**: When missing data is small (<5%) and appears random.\n",
        "\n",
        "**Imputation**:\n",
        "- **Mean/Median Imputation**: Replace missing numerical values with mean or median. Preserves sample size but underestimates variance.\n",
        "- **Mode Imputation**: For categorical variables, use most frequent category.\n",
        "- **Forward/Backward Fill**: For time series, use previous/next value.\n",
        "- **K-NN Imputation**: Use values from k nearest neighbors.\n",
        "- **Regression Imputation**: Predict missing values using other features.\n",
        "- **When to Use**: When missing data is substantial and deletion would lose important information.\n",
        "\n",
        "**Advanced Methods**:\n",
        "- **Multiple Imputation**: Create multiple imputed datasets and combine results.\n",
        "- **Model-Based Imputation**: Use sophisticated models to predict missing values.\n",
        "- **Indicator Variables**: Create binary features indicating missingness (may be informative).\n",
        "\n",
        "**Best Practices**:\n",
        "- Always investigate why data is missing (may be informative)\n",
        "- Consider creating \"missing\" as a category for categorical variables\n",
        "- Don't impute test set using training statistics - fit imputer on training data only\n",
        "- Document imputation strategy for reproducibility\n",
        "\n",
        "### Feature Scaling\n",
        "\n",
        "**Feature scaling** transforms features to similar scales. Many algorithms are sensitive to feature scale:\n",
        "\n",
        "**Algorithms Requiring Scaling**:\n",
        "- **Distance-based**: k-NN, SVM, K-Means (distance calculations affected)\n",
        "- **Gradient-based**: Neural networks, logistic regression with regularization\n",
        "- **Regularized models**: Ridge, Lasso, Elastic Net\n",
        "\n",
        "**Algorithms Not Requiring Scaling**:\n",
        "- **Tree-based**: Decision trees, Random Forest (splits based on thresholds)\n",
        "- **Naive Bayes**: Based on probabilities, not distances\n",
        "\n",
        "#### Scaling Methods\n",
        "\n",
        "**Standardization (Z-score normalization)**:\n",
        "- Formula: (x - μ) / σ\n",
        "- Result: Mean = 0, Standard deviation = 1\n",
        "- **When to Use**: When data follows normal distribution\n",
        "- **Advantages**: Preserves outliers, works well for many algorithms\n",
        "\n",
        "**Min-Max Normalization**:\n",
        "- Formula: (x - min) / (max - min)\n",
        "- Result: Range [0, 1]\n",
        "- **When to Use**: When you need bounded range\n",
        "- **Advantages**: Preserves relationships, intuitive scale\n",
        "- **Disadvantages**: Sensitive to outliers\n",
        "\n",
        "**Robust Scaling**:\n",
        "- Uses median and IQR instead of mean and std\n",
        "- **When to Use**: When data contains outliers\n",
        "- **Advantages**: Less affected by outliers\n",
        "\n",
        "**Best Practices**:\n",
        "- Fit scaler on training data only, then transform both train and test\n",
        "- Never fit on test data (data leakage)\n",
        "- Store scaling parameters for production use\n",
        "- Consider scaling even for algorithms that don't strictly require it (can help)\n",
        "\n",
        "### Encoding Categorical Variables\n",
        "\n",
        "Most machine learning algorithms require numerical inputs, so categorical variables must be encoded.\n",
        "\n",
        "#### Encoding Methods\n",
        "\n",
        "**One-Hot Encoding**:\n",
        "- Creates binary columns for each category\n",
        "- **Advantages**: No ordinal assumption, works for nominal categories\n",
        "- **Disadvantages**: Creates many columns for high-cardinality features, can cause multicollinearity\n",
        "- **When to Use**: Nominal categories with few unique values\n",
        "\n",
        "**Label Encoding**:\n",
        "- Assigns integer labels to categories (0, 1, 2, ...)\n",
        "- **Advantages**: Preserves single column, efficient\n",
        "- **Disadvantages**: Implies ordinality (may mislead algorithms)\n",
        "- **When to Use**: Ordinal categories or tree-based algorithms (less sensitive)\n",
        "\n",
        "**Target Encoding (Mean Encoding)**:\n",
        "- Replaces category with mean target value for that category\n",
        "- **Advantages**: Can capture predictive power, reduces dimensionality\n",
        "- **Disadvantages**: Risk of overfitting, requires careful cross-validation\n",
        "- **When to Use**: High-cardinality categorical features\n",
        "\n",
        "**Best Practices**:\n",
        "- Use one-hot encoding for nominal variables with few categories\n",
        "- Be cautious with high-cardinality features (consider target encoding or grouping)\n",
        "- For tree-based models, label encoding often sufficient\n",
        "- Always handle unseen categories in test data\n",
        "\n",
        "### Handling Imbalanced Data\n",
        "\n",
        "**Imbalanced datasets** have unequal class distributions (e.g., 95% class A, 5% class B). This causes problems:\n",
        "- Models biased toward majority class\n",
        "- Accuracy misleading (can achieve high accuracy by always predicting majority)\n",
        "- Minority class predictions poor\n",
        "\n",
        "#### Strategies\n",
        "\n",
        "**Resampling**:\n",
        "- **Oversampling**: Increase minority class samples (e.g., SMOTE - Synthetic Minority Oversampling)\n",
        "- **Undersampling**: Reduce majority class samples\n",
        "- **Combination**: Both oversample minority and undersample majority\n",
        "\n",
        "**Algorithm-Level**:\n",
        "- **Class Weights**: Penalize misclassifying minority class more heavily\n",
        "- **Threshold Tuning**: Adjust decision threshold (lower threshold for minority class)\n",
        "- **Cost-Sensitive Learning**: Assign different costs to different misclassifications\n",
        "\n",
        "**Evaluation Metrics**:\n",
        "- Don't use accuracy with imbalanced data\n",
        "- Use precision, recall, F1-score, ROC-AUC\n",
        "- Consider precision-recall curve instead of ROC curve\n",
        "\n",
        "**Best Practices**:\n",
        "- Always check class distribution first\n",
        "- Use appropriate metrics (not accuracy)\n",
        "- Try multiple approaches and compare\n",
        "- Consider business costs of different error types\n",
        "\n",
        "---\n",
        "\n",
        "## 6.2 Feature Engineering\n",
        "\n",
        "### Creating New Features\n",
        "\n",
        "**Feature engineering** is creating new features from existing ones. Often the most impactful step:\n",
        "\n",
        "**Mathematical Transformations**:\n",
        "- **Polynomial Features**: x², x³, x₁×x₂ (interactions)\n",
        "- **Logarithmic**: log(x) for skewed distributions\n",
        "- **Square Root**: √x for count data\n",
        "- **Ratios**: x₁/x₂ (e.g., income per person)\n",
        "\n",
        "**Temporal Features**:\n",
        "- **Time Components**: hour, day of week, month, season\n",
        "- **Time Since**: Days since last purchase, account creation\n",
        "- **Cyclical Encoding**: sin/cos for cyclical patterns (hours, months)\n",
        "\n",
        "**Binning**:\n",
        "- Convert continuous to categorical (e.g., age groups)\n",
        "- Can capture non-linear relationships\n",
        "- Reduces overfitting risk\n",
        "\n",
        "**Domain-Specific**:\n",
        "- **Business Logic**: Revenue per customer, conversion rates\n",
        "- **Domain Knowledge**: BMI from height/weight, speed from distance/time\n",
        "\n",
        "**Best Practices**:\n",
        "- Start with domain knowledge\n",
        "- Visualize relationships before creating features\n",
        "- Avoid creating too many features (curse of dimensionality)\n",
        "- Validate that new features improve model performance\n",
        "\n",
        "### Feature Selection\n",
        "\n",
        "**Feature selection** chooses the most relevant features, improving:\n",
        "- Model performance (removes noise)\n",
        "- Training speed (fewer features)\n",
        "- Interpretability (simpler models)\n",
        "- Generalization (reduces overfitting)\n",
        "\n",
        "#### Selection Methods\n",
        "\n",
        "**Filter Methods**:\n",
        "- **Correlation**: Remove highly correlated features\n",
        "- **Statistical Tests**: Chi-square, ANOVA F-test\n",
        "- **Mutual Information**: Measures dependency\n",
        "- **Advantages**: Fast, independent of model\n",
        "- **Disadvantages**: May miss feature interactions\n",
        "\n",
        "**Wrapper Methods**:\n",
        "- **Forward Selection**: Start empty, add best features iteratively\n",
        "- **Backward Elimination**: Start with all, remove worst iteratively\n",
        "- **Recursive Feature Elimination**: Remove least important iteratively\n",
        "- **Advantages**: Considers feature interactions\n",
        "- **Disadvantages**: Computationally expensive, risk of overfitting\n",
        "\n",
        "**Embedded Methods**:\n",
        "- **Lasso Regularization**: Automatically sets some coefficients to zero\n",
        "- **Tree-Based Importance**: Use feature importance from trees\n",
        "- **Advantages**: Efficient, model-specific\n",
        "- **Disadvantages**: Tied to specific algorithm\n",
        "\n",
        "**Best Practices**:\n",
        "- Start with filter methods for quick reduction\n",
        "- Use wrapper/embedded for final selection\n",
        "- Consider computational cost vs. performance gain\n",
        "- Validate selection on validation set\n",
        "\n",
        "### Feature Importance\n",
        "\n",
        "Understanding which features matter helps:\n",
        "- **Interpretability**: Explain model decisions\n",
        "- **Feature Selection**: Identify features to keep/remove\n",
        "- **Domain Insights**: Understand what drives predictions\n",
        "\n",
        "**Methods**:\n",
        "- **Coefficient Magnitude**: For linear models, larger coefficients = more important\n",
        "- **Permutation Importance**: Shuffle feature, measure performance drop\n",
        "- **SHAP Values**: Shapley Additive Explanations (model-agnostic)\n",
        "- **Tree Importance**: Built-in for tree-based models\n",
        "\n",
        "---\n",
        "\n",
        "## 6.3 Common Pitfalls and How to Avoid Them\n",
        "\n",
        "### Data Leakage\n",
        "\n",
        "**Data leakage** occurs when information from the future or test set leaks into training, creating unrealistically good performance.\n",
        "\n",
        "**Types**:\n",
        "- **Target Leakage**: Using features that wouldn't be available at prediction time\n",
        "- **Train-Test Contamination**: Using test data during training (e.g., scaling, imputation)\n",
        "- **Temporal Leakage**: Using future information to predict past\n",
        "\n",
        "**How to Avoid**:\n",
        "- **Temporal Ordering**: For time series, always train on past, test on future\n",
        "- **Proper Splitting**: Split data before any preprocessing\n",
        "- **Pipeline Approach**: Fit transformers on training data only\n",
        "- **Domain Knowledge**: Question whether features would be available in production\n",
        "- **Sanity Checks**: If performance seems too good, investigate leakage\n",
        "\n",
        "**Red Flags**:\n",
        "- Performance much better than expected\n",
        "- Features with suspiciously high correlation with target\n",
        "- Test performance better than validation performance\n",
        "- Features that wouldn't exist at prediction time\n",
        "\n",
        "### Overfitting and Underfitting\n",
        "\n",
        "**Overfitting**: Model learns training data too well, including noise.\n",
        "\n",
        "**Signs**:\n",
        "- Training performance much better than validation/test\n",
        "- Model too complex for data size\n",
        "- Performance degrades on new data\n",
        "\n",
        "**Solutions**:\n",
        "- **Regularization**: Add penalties for complexity (L1, L2)\n",
        "- **Reduce Model Complexity**: Fewer features, simpler models\n",
        "- **More Data**: Collect more training examples\n",
        "- **Cross-Validation**: Use to detect overfitting\n",
        "- **Early Stopping**: Stop training before overfitting\n",
        "\n",
        "**Underfitting**: Model too simple to capture patterns.\n",
        "\n",
        "**Signs**:\n",
        "- Poor performance on both training and test\n",
        "- Model too simple\n",
        "- High bias, low variance\n",
        "\n",
        "**Solutions**:\n",
        "- **Increase Model Complexity**: More features, deeper trees, more parameters\n",
        "- **Feature Engineering**: Create better features\n",
        "- **Reduce Regularization**: Lower regularization strength\n",
        "- **Longer Training**: For iterative algorithms, train longer\n",
        "\n",
        "### Ignoring Class Imbalance\n",
        "\n",
        "**Problem**: Using accuracy with imbalanced data gives misleading results.\n",
        "\n",
        "**Example**: 95% class A, 5% class B. Model predicting always A gets 95% accuracy but fails completely on class B.\n",
        "\n",
        "**Solutions**:\n",
        "- **Appropriate Metrics**: Use precision, recall, F1, ROC-AUC\n",
        "- **Resampling**: Balance classes through oversampling/undersampling\n",
        "- **Class Weights**: Penalize misclassifying minority class\n",
        "- **Threshold Tuning**: Adjust decision threshold based on business needs\n",
        "\n",
        "### Not Validating Properly\n",
        "\n",
        "**Common Mistakes**:\n",
        "- **Testing on Training Data**: Evaluating on same data used for training\n",
        "- **Multiple Testing**: Testing multiple models on test set, choosing best (overfitting to test set)\n",
        "- **Improper Splits**: Not respecting temporal order, data leakage\n",
        "- **No Validation Set**: Using test set for hyperparameter tuning\n",
        "\n",
        "**Best Practices**:\n",
        "- **Three-Way Split**: Train, validation, test sets\n",
        "- **Use Test Set Once**: Only for final evaluation\n",
        "- **Cross-Validation**: For model selection and hyperparameter tuning\n",
        "- **Temporal Awareness**: Respect time order for time series\n",
        "\n",
        "### Other Common Mistakes\n",
        "\n",
        "**Ignoring Data Quality**:\n",
        "- Not checking for outliers, errors, inconsistencies\n",
        "- Assuming data is clean\n",
        "- **Solution**: Always explore and clean data first\n",
        "\n",
        "**Feature Scaling Issues**:\n",
        "- Scaling test data using test statistics (should use training statistics)\n",
        "- Not scaling when required\n",
        "- **Solution**: Fit scalers on training data, transform both train and test\n",
        "\n",
        "**Categorical Encoding Mistakes**:\n",
        "- Using label encoding for nominal variables (implies order)\n",
        "- Not handling unseen categories in test data\n",
        "- **Solution**: Use appropriate encoding, handle test-time scenarios\n",
        "\n",
        "**Hyperparameter Tuning Mistakes**:\n",
        "- Tuning on test set\n",
        "- Not using cross-validation\n",
        "- Overfitting to validation set\n",
        "- **Solution**: Separate validation set, use cross-validation, keep test set separate\n",
        "\n",
        "---\n",
        "\n",
        "## 6.4 Model Interpretability\n",
        "\n",
        "### Why Interpretability Matters\n",
        "\n",
        "**Interpretability** is the ability to understand and explain how a model makes predictions.\n",
        "\n",
        "**Importance**:\n",
        "- **Trust**: Stakeholders need to trust model decisions\n",
        "- **Regulation**: Many industries require explainable models (finance, healthcare)\n",
        "- **Debugging**: Understanding failures helps improve models\n",
        "- **Fairness**: Detecting and preventing bias\n",
        "- **Insights**: Understanding what drives predictions provides business value\n",
        "\n",
        "**When Interpretability is Critical**:\n",
        "- **High-Stakes Decisions**: Medical diagnosis, loan approval, hiring\n",
        "- **Regulated Industries**: Finance, healthcare, legal\n",
        "- **Stakeholder Requirements**: Business users need explanations\n",
        "- **Bias Detection**: Ensuring fair, unbiased predictions\n",
        "\n",
        "### Interpretable Models\n",
        "\n",
        "**Naturally Interpretable**:\n",
        "- **Linear/Logistic Regression**: Coefficients show feature importance\n",
        "- **Decision Trees**: Visual, rule-based explanations\n",
        "- **Rule-Based Models**: IF-THEN rules\n",
        "\n",
        "**Trade-offs**:\n",
        "- Often simpler models\n",
        "- May sacrifice some performance for interpretability\n",
        "- Sufficient for many problems\n",
        "\n",
        "### Model-Agnostic Interpretability Methods\n",
        "\n",
        "**SHAP (SHapley Additive Explanations)**:\n",
        "- Explains individual predictions\n",
        "- Shows contribution of each feature\n",
        "- Works with any model\n",
        "- Based on game theory (Shapley values)\n",
        "\n",
        "**LIME (Local Interpretable Model-agnostic Explanations)**:\n",
        "- Approximates complex model locally with simple model\n",
        "- Explains individual predictions\n",
        "- Easy to understand\n",
        "- May not always be accurate\n",
        "\n",
        "**Partial Dependence Plots**:\n",
        "- Show effect of feature on prediction\n",
        "- Marginal effect holding other features constant\n",
        "- Visual and intuitive\n",
        "- Can show interactions\n",
        "\n",
        "**Feature Importance**:\n",
        "- Overall importance of each feature\n",
        "- Available for many models\n",
        "- Simple to understand\n",
        "- May miss interactions\n",
        "\n",
        "**Best Practices**:\n",
        "- Start with interpretable models when possible\n",
        "- Use model-agnostic methods for complex models\n",
        "- Explain both individual predictions and overall model behavior\n",
        "- Consider audience when choosing explanation method\n",
        "- Validate explanations make sense (domain knowledge)\n",
        "\n",
        "---\n",
        "\n",
        "## Summary and Key Takeaways\n",
        "\n",
        "1. **Data Preprocessing** (handling missing values, scaling, encoding) is crucial for model performance.\n",
        "\n",
        "2. **Feature Engineering** can significantly improve models - domain knowledge is key.\n",
        "\n",
        "3. **Feature Selection** reduces overfitting and improves interpretability.\n",
        "\n",
        "4. **Data Leakage** is a critical issue - always validate that features would be available at prediction time.\n",
        "\n",
        "5. **Overfitting and Underfitting** must be balanced through regularization, complexity control, and validation.\n",
        "\n",
        "6. **Class Imbalance** requires appropriate metrics and handling strategies.\n",
        "\n",
        "7. **Proper Validation** requires careful data splitting and respecting temporal order.\n",
        "\n",
        "8. **Interpretability** is essential for trust, regulation, and debugging - choose methods appropriate for your context.\n",
        "\n",
        "9. **Best Practices** throughout the pipeline prevent common mistakes and improve results.\n",
        "\n",
        "10. **Domain Knowledge** should guide preprocessing, feature engineering, and interpretation.\n",
        "\n",
        "---\n",
        "\n",
        "## Further Reading\n",
        "\n",
        "- Molnar, C. (2020). *Interpretable Machine Learning*. https://christophm.github.io/interpretable-ml-book/\n",
        "- Scikit-learn Preprocessing Documentation: https://scikit-learn.org/stable/modules/preprocessing.html\n",
        "- Scikit-learn Feature Selection Documentation: https://scikit-learn.org/stable/modules/feature_selection.html\n",
        "\n",
        "---\n",
        "\n",
        "## Practice Exercises\n",
        "\n",
        "1. Why is it important to fit scalers on training data only? What happens if you fit on the entire dataset?\n",
        "\n",
        "2. Explain the difference between one-hot encoding and label encoding. When would you use each?\n",
        "\n",
        "3. What is data leakage? Give an example and explain how to prevent it.\n",
        "\n",
        "4. How would you handle a classification problem with 99% class A and 1% class B?\n",
        "\n",
        "5. Why is model interpretability important? When might you choose a less interpretable model?\n",
        "\n",
        "6. Describe a scenario where feature engineering would significantly improve model performance.\n",
        "\n",
        "7. What are the signs of overfitting? How would you address it?\n",
        "\n",
        "8. Why shouldn't you use the test set for hyperparameter tuning?\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
