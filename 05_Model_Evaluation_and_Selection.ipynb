{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model Evaluation and Selection\n",
        "## Lecture Notebook Part 5 of 6\n",
        "\n",
        "---\n",
        "\n",
        "## Learning Objectives\n",
        "\n",
        "By the end of this notebook, you will be able to:\n",
        "1. Understand cross-validation and its importance\n",
        "2. Implement different cross-validation strategies\n",
        "3. Perform hyperparameter tuning using various methods\n",
        "4. Compare and select models effectively\n",
        "5. Understand the bias-variance tradeoff in model selection\n",
        "6. Apply best practices for model evaluation\n",
        "\n",
        "---\n",
        "\n",
        "## 5.1 Cross-Validation\n",
        "\n",
        "### What is Cross-Validation?\n",
        "\n",
        "**Cross-validation** is a resampling technique used to assess how well a model will generalize to an independent dataset. Instead of using a single train-test split, cross-validation divides the data into multiple folds and evaluates the model on each fold, providing a more robust estimate of model performance.\n",
        "\n",
        "The fundamental problem that cross-validation addresses is that a single train-test split can give misleading results. Depending on which data points end up in the training vs. test set, performance estimates can vary significantly. Cross-validation averages performance across multiple splits, giving a more reliable and stable estimate.\n",
        "\n",
        "Cross-validation is essential for:\n",
        "- **Model Selection**: Comparing different algorithms\n",
        "- **Hyperparameter Tuning**: Finding optimal parameter values\n",
        "- **Performance Estimation**: Getting reliable estimates of how well models will perform\n",
        "- **Detecting Overfitting**: Identifying when models perform well on training data but poorly on validation data\n",
        "\n",
        "### Types of Cross-Validation\n",
        "\n",
        "#### k-Fold Cross-Validation\n",
        "\n",
        "**k-Fold Cross-Validation** is the most common form. The data is divided into k equal-sized folds (typically k=5 or k=10). The algorithm:\n",
        "\n",
        "1. **Split data into k folds**: Randomly divide dataset into k groups of approximately equal size.\n",
        "\n",
        "2. **Train on k-1 folds, test on remaining fold**: For each of k iterations:\n",
        "   - Use k-1 folds for training\n",
        "   - Use remaining fold for testing\n",
        "   - Calculate performance metric\n",
        "\n",
        "3. **Repeat k times**: Each fold serves as test set exactly once.\n",
        "\n",
        "4. **Average results**: Compute mean and standard deviation of performance across all k iterations.\n",
        "\n",
        "**Advantages**:\n",
        "- More reliable than single train-test split\n",
        "- All data used for both training and testing\n",
        "- Reduces variance in performance estimates\n",
        "- Works well with moderate-sized datasets\n",
        "\n",
        "**Disadvantages**:\n",
        "- k times more computation than single split\n",
        "- Still requires sufficient data (each fold needs enough samples)\n",
        "\n",
        "**Choosing k**:\n",
        "- **k=5 or k=10**: Most common choices, good balance between reliability and computation\n",
        "- **Small k (k=3)**: Less computation but higher variance in estimates\n",
        "- **Large k (k=20)**: More reliable but more computation, and folds may be too small\n",
        "\n",
        "#### Stratified k-Fold Cross-Validation\n",
        "\n",
        "**Stratified k-Fold** maintains the same class distribution in each fold as in the original dataset. This is crucial for:\n",
        "- **Imbalanced datasets**: Ensures each fold has representative samples from all classes\n",
        "- **Small datasets**: Prevents folds with only one class\n",
        "- **Classification problems**: Maintains class balance across folds\n",
        "\n",
        "Stratified cross-validation is especially important when classes are imbalanced, as random splitting might create folds with very few (or zero) samples from minority classes.\n",
        "\n",
        "#### Leave-One-Out Cross-Validation (LOOCV)\n",
        "\n",
        "**LOOCV** is the extreme case where k equals the number of samples (k=n). Each iteration uses n-1 samples for training and 1 sample for testing.\n",
        "\n",
        "**Advantages**:\n",
        "- Uses maximum data for training (n-1 samples)\n",
        "- Deterministic (no randomness in fold creation)\n",
        "- Unbiased estimate (each sample tested exactly once)\n",
        "\n",
        "**Disadvantages**:\n",
        "- Very computationally expensive (n models to train)\n",
        "- High variance in estimates (each test set has only 1 sample)\n",
        "- Can be slow for large datasets\n",
        "\n",
        "**When to Use**: Small datasets where you want to use maximum data for training.\n",
        "\n",
        "#### Time Series Cross-Validation\n",
        "\n",
        "For time-dependent data, standard cross-validation can leak future information into the past. **Time Series Cross-Validation** respects temporal order:\n",
        "\n",
        "- **Train on past, test on future**: Always use earlier data to predict later data\n",
        "- **Expanding window**: Training set grows over time\n",
        "- **Sliding window**: Fixed-size training window that slides forward\n",
        "\n",
        "This prevents the model from \"cheating\" by using future information to predict the past.\n",
        "\n",
        "### Benefits of Cross-Validation\n",
        "\n",
        "**Better Performance Estimates**: Averaging across multiple folds provides more reliable estimates than a single split.\n",
        "\n",
        "**Reduces Overfitting Risk**: Models are evaluated on data they haven't seen during training, helping identify overfitting.\n",
        "\n",
        "**Helps with Hyperparameter Tuning**: Can evaluate different hyperparameter values across folds to find optimal settings.\n",
        "\n",
        "**Uses Data Efficiently**: All data is used for both training and validation, maximizing information usage.\n",
        "\n",
        "**Identifies Model Stability**: High variance across folds indicates the model is sensitive to data changes, suggesting potential issues.\n",
        "\n",
        "---\n",
        "\n",
        "## 5.2 Hyperparameter Tuning\n",
        "\n",
        "### What are Hyperparameters?\n",
        "\n",
        "**Hyperparameters** are parameters set before training begins, unlike model parameters (like coefficients in linear regression) which are learned during training. Hyperparameters control the learning process itself and significantly impact model performance.\n",
        "\n",
        "**Examples of Hyperparameters**:\n",
        "- **Regularization strength** (C, alpha, lambda): Controls overfitting in Ridge, Lasso, SVM\n",
        "- **Tree depth** (max_depth): Maximum depth of decision trees\n",
        "- **Number of neighbors** (k): For k-NN algorithm\n",
        "- **Learning rate**: Step size in gradient descent\n",
        "- **Number of trees** (n_estimators): For Random Forest\n",
        "- **Kernel parameters**: For SVM (gamma, degree)\n",
        "\n",
        "**Why Tuning Matters**: Poor hyperparameter choices can lead to:\n",
        "- Underfitting (model too simple)\n",
        "- Overfitting (model too complex)\n",
        "- Poor generalization performance\n",
        "- Wasted computational resources\n",
        "\n",
        "### Tuning Methods\n",
        "\n",
        "#### Grid Search\n",
        "\n",
        "**Grid Search** performs exhaustive search over a specified parameter grid. It:\n",
        "\n",
        "1. **Define parameter grid**: Specify values to try for each hyperparameter\n",
        "2. **Generate all combinations**: Create all possible combinations of parameter values\n",
        "3. **Evaluate each combination**: Train model and evaluate using cross-validation\n",
        "4. **Select best**: Choose combination with best performance\n",
        "\n",
        "**Advantages**:\n",
        "- Guaranteed to find best combination within grid\n",
        "- Simple to understand and implement\n",
        "- Works well when parameter space is small\n",
        "\n",
        "**Disadvantages**:\n",
        "- Computationally expensive (exponential in number of parameters)\n",
        "- May miss optimal values between grid points\n",
        "- Doesn't scale well with many hyperparameters\n",
        "\n",
        "**Example**: For SVM with C and gamma:\n",
        "- C: [0.1, 1, 10, 100]\n",
        "- gamma: [0.001, 0.01, 0.1, 1]\n",
        "- Total combinations: 4 × 4 = 16 models to train\n",
        "\n",
        "#### Random Search\n",
        "\n",
        "**Random Search** randomly samples hyperparameter combinations from specified distributions:\n",
        "\n",
        "1. **Define parameter distributions**: Specify ranges/distributions for each parameter\n",
        "2. **Randomly sample combinations**: Generate random parameter combinations\n",
        "3. **Evaluate sampled combinations**: Train and evaluate models\n",
        "4. **Select best**: Choose best performing combination\n",
        "\n",
        "**Advantages**:\n",
        "- More efficient than grid search (can find good solutions faster)\n",
        "- Better coverage of parameter space\n",
        "- Can specify distributions (not just discrete values)\n",
        "- Scales better with many hyperparameters\n",
        "\n",
        "**Disadvantages**:\n",
        "- May miss optimal combinations\n",
        "- Less systematic than grid search\n",
        "- Requires more iterations to be confident\n",
        "\n",
        "**When to Use**: When parameter space is large or when computational resources are limited.\n",
        "\n",
        "#### Bayesian Optimization\n",
        "\n",
        "**Bayesian Optimization** uses probabilistic models to guide search:\n",
        "\n",
        "1. **Build probabilistic model**: Model the relationship between hyperparameters and performance\n",
        "2. **Use acquisition function**: Choose next hyperparameters to evaluate based on model\n",
        "3. **Update model**: Incorporate new results\n",
        "4. **Iterate**: Continue until convergence or budget exhausted\n",
        "\n",
        "**Advantages**:\n",
        "- More efficient than random search\n",
        "- Learns from previous evaluations\n",
        "- Better for expensive evaluations\n",
        "- Can handle continuous and discrete parameters\n",
        "\n",
        "**Disadvantages**:\n",
        "- More complex to implement\n",
        "- Requires more setup\n",
        "- May get stuck in local optima\n",
        "\n",
        "**When to Use**: When model training is expensive and you want to minimize evaluations.\n",
        "\n",
        "### Best Practices\n",
        "\n",
        "**Use Cross-Validation**: Always use cross-validation (not single train-test split) for hyperparameter tuning to get reliable estimates.\n",
        "\n",
        "**Separate Validation Set**: Keep a separate test set that's never used during hyperparameter tuning. Only use it for final evaluation.\n",
        "\n",
        "**Start with Coarse Search**: Begin with wide parameter ranges, then refine around promising regions.\n",
        "\n",
        "**Consider Computational Cost**: Balance search thoroughness with available resources.\n",
        "\n",
        "**Monitor for Overfitting**: If validation performance much better than test performance, you may be overfitting to validation set.\n",
        "\n",
        "---\n",
        "\n",
        "## 5.3 Model Comparison and Selection\n",
        "\n",
        "### Comparing Models\n",
        "\n",
        "When comparing models, consider multiple factors:\n",
        "\n",
        "**Performance Metrics**: \n",
        "- Primary metric (e.g., accuracy, F1-score, RMSE)\n",
        "- Secondary metrics (e.g., precision, recall, training time)\n",
        "- Performance on different data subsets (train vs. validation vs. test)\n",
        "\n",
        "**Computational Cost**:\n",
        "- **Training Time**: How long does it take to train?\n",
        "- **Prediction Time**: How fast are predictions?\n",
        "- **Memory Usage**: How much memory does the model require?\n",
        "\n",
        "**Interpretability**:\n",
        "- Can stakeholders understand the model?\n",
        "- Are predictions explainable?\n",
        "- Is feature importance available?\n",
        "\n",
        "**Data Characteristics**:\n",
        "- Does model handle your data type well?\n",
        "- Can it handle missing values?\n",
        "- Does it require feature scaling?\n",
        "\n",
        "**Robustness**:\n",
        "- How sensitive is the model to data changes?\n",
        "- Does it handle outliers well?\n",
        "- Is performance stable across different data splits?\n",
        "\n",
        "### Bias-Variance Tradeoff Revisited\n",
        "\n",
        "The **bias-variance tradeoff** is fundamental to model selection:\n",
        "\n",
        "**Bias**: Error from overly simplistic assumptions. High bias causes underfitting.\n",
        "- **Low Bias**: Model can capture complex patterns\n",
        "- **High Bias**: Model too simple, misses patterns\n",
        "\n",
        "**Variance**: Error from sensitivity to small fluctuations in training set. High variance causes overfitting.\n",
        "- **Low Variance**: Model stable across different training sets\n",
        "- **High Variance**: Model changes significantly with small data changes\n",
        "\n",
        "**Tradeoff**:\n",
        "- **Simple Models** (e.g., linear regression): Low variance, high bias\n",
        "- **Complex Models** (e.g., deep neural networks): Low bias, high variance\n",
        "- **Goal**: Find balance that minimizes total error\n",
        "\n",
        "**Total Error = Bias² + Variance + Irreducible Error**\n",
        "\n",
        "The irreducible error is inherent to the problem and cannot be reduced.\n",
        "\n",
        "### Occam's Razor\n",
        "\n",
        "**Occam's Razor** principle: When two models perform similarly, prefer the simpler one.\n",
        "\n",
        "**Why Simpler is Often Better**:\n",
        "- **Better Generalization**: Simpler models often generalize better\n",
        "- **Easier to Interpret**: Stakeholders can understand and trust the model\n",
        "- **Less Prone to Overfitting**: Simpler models have fewer parameters to overfit\n",
        "- **Easier to Maintain**: Simpler models are easier to debug and update\n",
        "- **Faster**: Simpler models are usually faster to train and predict\n",
        "\n",
        "**When Complexity is Justified**:\n",
        "- Simple models clearly underperform\n",
        "- Problem genuinely requires complex relationships\n",
        "- You have sufficient data to support complexity\n",
        "- Computational resources allow for complex models\n",
        "\n",
        "### Model Selection Workflow\n",
        "\n",
        "1. **Define Problem and Metrics**: Clearly define what success looks like and how to measure it.\n",
        "\n",
        "2. **Prepare Data**: Clean, preprocess, and split data appropriately.\n",
        "\n",
        "3. **Try Multiple Algorithms**: Don't commit to one algorithm too early. Try several:\n",
        "   - Simple baseline (e.g., logistic regression, linear regression)\n",
        "   - More complex models (e.g., random forest, SVM)\n",
        "   - Ensemble methods if appropriate\n",
        "\n",
        "4. **Use Cross-Validation**: Evaluate all models using cross-validation for fair comparison.\n",
        "\n",
        "5. **Tune Hyperparameters**: Optimize hyperparameters for promising models.\n",
        "\n",
        "6. **Compare on Test Set**: Final comparison on held-out test set (used only once).\n",
        "\n",
        "7. **Consider Practical Factors**: Beyond performance, consider interpretability, speed, maintenance.\n",
        "\n",
        "8. **Select Final Model**: Choose model that best balances all factors.\n",
        "\n",
        "---\n",
        "\n",
        "## Summary and Key Takeaways\n",
        "\n",
        "1. **Cross-Validation** provides more reliable performance estimates than single train-test splits.\n",
        "\n",
        "2. **Stratified Cross-Validation** is essential for imbalanced classification problems.\n",
        "\n",
        "3. **Hyperparameter Tuning** significantly impacts model performance and should use cross-validation.\n",
        "\n",
        "4. **Grid Search** is exhaustive but expensive; **Random Search** is more efficient for large parameter spaces.\n",
        "\n",
        "5. **Model Comparison** should consider performance, computational cost, interpretability, and robustness.\n",
        "\n",
        "6. **Bias-Variance Tradeoff** guides model complexity selection - balance between underfitting and overfitting.\n",
        "\n",
        "7. **Occam's Razor** suggests preferring simpler models when performance is similar.\n",
        "\n",
        "8. **Proper Workflow** separates training, validation, and test sets, using each appropriately.\n",
        "\n",
        "---\n",
        "\n",
        "## Further Reading\n",
        "\n",
        "- James, G., et al. (2013). *An Introduction to Statistical Learning*. Springer.\n",
        "- Scikit-learn Model Selection Documentation: https://scikit-learn.org/stable/model_selection.html\n",
        "\n",
        "---\n",
        "\n",
        "## Practice Exercises\n",
        "\n",
        "1. Why is cross-validation important? What problems does it solve?\n",
        "\n",
        "2. When would you use stratified cross-validation instead of regular k-fold?\n",
        "\n",
        "3. Compare grid search and random search. When would you use each?\n",
        "\n",
        "4. Explain the bias-variance tradeoff. How does it relate to model selection?\n",
        "\n",
        "5. Why might a simpler model be preferred over a more complex one, even if the complex model performs slightly better?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import (train_test_split, cross_val_score, \n",
        "                                     KFold, StratifiedKFold, GridSearchCV, \n",
        "                                     RandomizedSearchCV)\n",
        "from sklearn.linear_model import LogisticRegression, Ridge\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, mean_squared_error, make_scorer\n",
        "from sklearn.datasets import make_classification, make_regression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set style\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"Libraries imported successfully!\")\n",
        "print(\"Ready to explore model evaluation and selection!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Example 1: k-Fold Cross-Validation\n",
        "\n",
        "Demonstrating k-fold cross-validation:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate classification dataset\n",
        "X_cv, y_cv = make_classification(n_samples=1000, n_features=10, n_informative=5,\n",
        "                                 n_redundant=2, random_state=42)\n",
        "\n",
        "# Split into train and test (test set never used in CV)\n",
        "X_train_cv, X_test_cv, y_train_cv, y_test_cv = train_test_split(\n",
        "    X_cv, y_cv, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Scale features\n",
        "scaler_cv = StandardScaler()\n",
        "X_train_cv_scaled = scaler_cv.fit_transform(X_train_cv)\n",
        "X_test_cv_scaled = scaler_cv.transform(X_test_cv)\n",
        "\n",
        "# Train a model\n",
        "model_cv = LogisticRegression(random_state=42, max_iter=1000)\n",
        "\n",
        "# Perform k-fold cross-validation\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "cv_scores = cross_val_score(model_cv, X_train_cv_scaled, y_train_cv, \n",
        "                            cv=kf, scoring='accuracy')\n",
        "\n",
        "print(\"5-Fold Cross-Validation Results:\")\n",
        "print(f\"Fold scores: {cv_scores}\")\n",
        "print(f\"Mean CV Score: {cv_scores.mean():.4f} ± {cv_scores.std():.4f}\")\n",
        "print(f\"Min Score: {cv_scores.min():.4f}\")\n",
        "print(f\"Max Score: {cv_scores.max():.4f}\")\n",
        "\n",
        "# Compare with single train-test split\n",
        "model_single = LogisticRegression(random_state=42, max_iter=1000)\n",
        "model_single.fit(X_train_cv_scaled, y_train_cv)\n",
        "single_score = accuracy_score(y_test_cv, model_single.predict(X_test_cv_scaled))\n",
        "\n",
        "print(f\"\\nSingle Train-Test Split Score: {single_score:.4f}\")\n",
        "print(f\"\\nCV provides more reliable estimate with mean: {cv_scores.mean():.4f}\")\n",
        "\n",
        "# Visualize CV scores\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.bar(range(1, 6), cv_scores, alpha=0.7, label='Fold Scores')\n",
        "plt.axhline(y=cv_scores.mean(), color='r', linestyle='--', linewidth=2, \n",
        "           label=f'Mean: {cv_scores.mean():.4f}')\n",
        "plt.fill_between(range(0, 6), cv_scores.mean() - cv_scores.std(),\n",
        "                 cv_scores.mean() + cv_scores.std(), alpha=0.2, color='red',\n",
        "                 label=f'±1 std: {cv_scores.std():.4f}')\n",
        "plt.xlabel('Fold Number')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('5-Fold Cross-Validation Scores')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3, axis='y')\n",
        "plt.ylim([0.8, 1.0])\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Example 2: Stratified k-Fold Cross-Validation\n",
        "\n",
        "Demonstrating stratified cross-validation for imbalanced data:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create imbalanced dataset\n",
        "X_imbal, y_imbal = make_classification(n_samples=1000, n_features=10, n_informative=5,\n",
        "                                        n_redundant=2, weights=[0.9, 0.1], \n",
        "                                        random_state=42)\n",
        "\n",
        "print(f\"Class distribution: {np.bincount(y_imbal)}\")\n",
        "print(f\"Class 0: {np.sum(y_imbal == 0)} samples ({np.sum(y_imbal == 0)/len(y_imbal)*100:.1f}%)\")\n",
        "print(f\"Class 1: {np.sum(y_imbal == 1)} samples ({np.sum(y_imbal == 1)/len(y_imbal)*100:.1f}%)\")\n",
        "\n",
        "X_train_imbal, X_test_imbal, y_train_imbal, y_test_imbal = train_test_split(\n",
        "    X_imbal, y_imbal, test_size=0.2, random_state=42, stratify=y_imbal\n",
        ")\n",
        "\n",
        "scaler_imbal = StandardScaler()\n",
        "X_train_imbal_scaled = scaler_imbal.fit_transform(X_train_imbal)\n",
        "X_test_imbal_scaled = scaler_imbal.transform(X_test_imbal)\n",
        "\n",
        "model_imbal = LogisticRegression(random_state=42, max_iter=1000)\n",
        "\n",
        "# Regular k-fold (may have imbalanced folds)\n",
        "kf_regular = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "cv_regular = cross_val_score(model_imbal, X_train_imbal_scaled, y_train_imbal,\n",
        "                            cv=kf_regular, scoring='accuracy')\n",
        "\n",
        "# Stratified k-fold (maintains class distribution)\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "cv_stratified = cross_val_score(model_imbal, X_train_imbal_scaled, y_train_imbal,\n",
        "                               cv=skf, scoring='accuracy')\n",
        "\n",
        "print(\"\\nRegular K-Fold CV Scores:\")\n",
        "print(f\"  Scores: {cv_regular}\")\n",
        "print(f\"  Mean: {cv_regular.mean():.4f} ± {cv_regular.std():.4f}\")\n",
        "\n",
        "print(\"\\nStratified K-Fold CV Scores:\")\n",
        "print(f\"  Scores: {cv_stratified}\")\n",
        "print(f\"  Mean: {cv_stratified.mean():.4f} ± {cv_stratified.std():.4f}\")\n",
        "\n",
        "# Check class distribution in folds\n",
        "print(\"\\nClass distribution in first fold (Regular K-Fold):\")\n",
        "for train_idx, val_idx in kf_regular.split(X_train_imbal_scaled, y_train_imbal):\n",
        "    print(f\"  Train: {np.bincount(y_train_imbal[train_idx])}\")\n",
        "    print(f\"  Val: {np.bincount(y_train_imbal[val_idx])}\")\n",
        "    break\n",
        "\n",
        "print(\"\\nClass distribution in first fold (Stratified K-Fold):\")\n",
        "for train_idx, val_idx in skf.split(X_train_imbal_scaled, y_train_imbal):\n",
        "    print(f\"  Train: {np.bincount(y_train_imbal[train_idx])}\")\n",
        "    print(f\"  Val: {np.bincount(y_train_imbal[val_idx])}\")\n",
        "    break\n",
        "\n",
        "# Visualize\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "axes[0].bar(range(1, 6), cv_regular, alpha=0.7, label='Regular K-Fold')\n",
        "axes[0].axhline(y=cv_regular.mean(), color='r', linestyle='--', linewidth=2)\n",
        "axes[0].set_xlabel('Fold Number')\n",
        "axes[0].set_ylabel('Accuracy')\n",
        "axes[0].set_title('Regular K-Fold CV')\n",
        "axes[0].legend()\n",
        "axes[0].grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "axes[1].bar(range(1, 6), cv_stratified, alpha=0.7, color='green', label='Stratified K-Fold')\n",
        "axes[1].axhline(y=cv_stratified.mean(), color='r', linestyle='--', linewidth=2)\n",
        "axes[1].set_xlabel('Fold Number')\n",
        "axes[1].set_ylabel('Accuracy')\n",
        "axes[1].set_title('Stratified K-Fold CV')\n",
        "axes[1].legend()\n",
        "axes[1].grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nObservations:\")\n",
        "print(\"- Stratified K-Fold ensures each fold has similar class distribution\")\n",
        "print(\"- Important for imbalanced datasets to get reliable estimates\")\n",
        "print(\"- Regular K-Fold may create folds with very few minority class samples\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Example 3: Grid Search for Hyperparameter Tuning\n",
        "\n",
        "Demonstrating grid search with cross-validation:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate dataset\n",
        "X_gs, y_gs = make_classification(n_samples=1000, n_features=10, n_informative=5,\n",
        "                                 random_state=42)\n",
        "\n",
        "X_train_gs, X_test_gs, y_train_gs, y_test_gs = train_test_split(\n",
        "    X_gs, y_gs, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "scaler_gs = StandardScaler()\n",
        "X_train_gs_scaled = scaler_gs.fit_transform(X_train_gs)\n",
        "X_test_gs_scaled = scaler_gs.transform(X_test_gs)\n",
        "\n",
        "# Define parameter grid for SVM\n",
        "param_grid = {\n",
        "    'C': [0.1, 1, 10, 100],\n",
        "    'gamma': [0.001, 0.01, 0.1, 1],\n",
        "    'kernel': ['rbf', 'linear']\n",
        "}\n",
        "\n",
        "print(f\"Total combinations: {len(param_grid['C']) * len(param_grid['gamma']) * len(param_grid['kernel'])}\")\n",
        "\n",
        "# Perform grid search with cross-validation\n",
        "svm = SVC(random_state=42)\n",
        "grid_search = GridSearchCV(svm, param_grid, cv=5, scoring='accuracy', \n",
        "                          n_jobs=-1, verbose=1)\n",
        "grid_search.fit(X_train_gs_scaled, y_train_gs)\n",
        "\n",
        "print(f\"\\nBest parameters: {grid_search.best_params_}\")\n",
        "print(f\"Best cross-validation score: {grid_search.best_score_:.4f}\")\n",
        "\n",
        "# Evaluate on test set\n",
        "best_model = grid_search.best_estimator_\n",
        "test_score = accuracy_score(y_test_gs, best_model.predict(X_test_gs_scaled))\n",
        "print(f\"Test set score: {test_score:.4f}\")\n",
        "\n",
        "# Visualize results\n",
        "results_df = pd.DataFrame(grid_search.cv_results_)\n",
        "print(f\"\\nTop 5 parameter combinations:\")\n",
        "top_5 = results_df.nlargest(5, 'mean_test_score')[['param_C', 'param_gamma', \n",
        "                                                    'param_kernel', 'mean_test_score', \n",
        "                                                    'std_test_score']]\n",
        "print(top_5.to_string(index=False))\n",
        "\n",
        "# Plot results for RBF kernel\n",
        "rbf_results = results_df[results_df['param_kernel'] == 'rbf']\n",
        "pivot_table = rbf_results.pivot_table(values='mean_test_score', \n",
        "                                      index='param_C', \n",
        "                                      columns='param_gamma')\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.heatmap(pivot_table, annot=True, fmt='.3f', cmap='viridis', cbar_kws={'label': 'CV Score'})\n",
        "plt.title('Grid Search Results (RBF Kernel)')\n",
        "plt.xlabel('Gamma')\n",
        "plt.ylabel('C')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Example 4: Random Search for Hyperparameter Tuning\n",
        "\n",
        "Comparing random search with grid search:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from scipy.stats import uniform, loguniform\n",
        "\n",
        "# Same dataset\n",
        "X_rs = X_train_gs_scaled\n",
        "y_rs = y_train_gs\n",
        "\n",
        "# Define parameter distributions for random search\n",
        "param_distributions = {\n",
        "    'C': loguniform(0.01, 100),  # Log-uniform distribution\n",
        "    'gamma': loguniform(0.001, 1),\n",
        "    'kernel': ['rbf', 'linear']\n",
        "}\n",
        "\n",
        "# Perform random search\n",
        "svm_rs = SVC(random_state=42)\n",
        "random_search = RandomizedSearchCV(svm_rs, param_distributions, \n",
        "                                   n_iter=20, cv=5, scoring='accuracy',\n",
        "                                   random_state=42, n_jobs=-1, verbose=1)\n",
        "random_search.fit(X_rs, y_rs)\n",
        "\n",
        "print(f\"\\nRandom Search Results:\")\n",
        "print(f\"Best parameters: {random_search.best_params_}\")\n",
        "print(f\"Best cross-validation score: {random_search.best_score_:.4f}\")\n",
        "\n",
        "# Compare with grid search\n",
        "print(f\"\\nComparison:\")\n",
        "print(f\"Grid Search best score: {grid_search.best_score_:.4f}\")\n",
        "print(f\"Random Search best score: {random_search.best_score_:.4f}\")\n",
        "print(f\"\\nGrid Search evaluated {len(grid_search.cv_results_['params'])} combinations\")\n",
        "print(f\"Random Search evaluated {len(random_search.cv_results_['params'])} combinations\")\n",
        "\n",
        "# Visualize random search results\n",
        "rs_results_df = pd.DataFrame(random_search.cv_results_)\n",
        "rs_rbf = rs_results_df[rs_results_df['param_kernel'] == 'rbf']\n",
        "\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.scatter(rs_rbf['param_C'], rs_rbf['param_gamma'], \n",
        "           c=rs_rbf['mean_test_score'], s=100, cmap='viridis', alpha=0.7)\n",
        "plt.colorbar(label='CV Score')\n",
        "plt.xscale('log')\n",
        "plt.yscale('log')\n",
        "plt.xlabel('C')\n",
        "plt.ylabel('Gamma')\n",
        "plt.title('Random Search Results (RBF Kernel)')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.scatter(range(len(rs_results_df)), rs_results_df['mean_test_score'].sort_values(ascending=False),\n",
        "           s=50, alpha=0.7)\n",
        "plt.axhline(y=random_search.best_score_, color='r', linestyle='--', \n",
        "           label=f'Best: {random_search.best_score_:.4f}')\n",
        "plt.xlabel('Iteration (sorted by score)')\n",
        "plt.ylabel('CV Score')\n",
        "plt.title('Random Search: Score Distribution')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nObservations:\")\n",
        "print(\"- Random search can find good solutions with fewer evaluations\")\n",
        "print(\"- More efficient for large parameter spaces\")\n",
        "print(\"- Can explore continuous parameter spaces better\")\n",
        "print(\"- May miss optimal combinations but often finds good ones faster\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Example 5: Model Comparison\n",
        "\n",
        "Comparing multiple models using cross-validation:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate dataset\n",
        "X_compare, y_compare = make_classification(n_samples=1000, n_features=10, \n",
        "                                          n_informative=5, random_state=42)\n",
        "\n",
        "X_train_comp, X_test_comp, y_train_comp, y_test_comp = train_test_split(\n",
        "    X_compare, y_compare, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "scaler_comp = StandardScaler()\n",
        "X_train_comp_scaled = scaler_comp.fit_transform(X_train_comp)\n",
        "X_test_comp_scaled = scaler_comp.transform(X_test_comp)\n",
        "\n",
        "# Define models to compare\n",
        "models = {\n",
        "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
        "    'Decision Tree': DecisionTreeClassifier(random_state=42, max_depth=5),\n",
        "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
        "    'SVM (RBF)': SVC(kernel='rbf', random_state=42)\n",
        "}\n",
        "\n",
        "# Perform cross-validation for each model\n",
        "cv_results = {}\n",
        "skf_comp = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "for name, model in models.items():\n",
        "    if name == 'SVM (RBF)':\n",
        "        # SVM needs scaling\n",
        "        scores = cross_val_score(model, X_train_comp_scaled, y_train_comp,\n",
        "                                cv=skf_comp, scoring='accuracy')\n",
        "    else:\n",
        "        scores = cross_val_score(model, X_train_comp, y_train_comp,\n",
        "                                cv=skf_comp, scoring='accuracy')\n",
        "    cv_results[name] = scores\n",
        "    print(f\"{name}: {scores.mean():.4f} ± {scores.std():.4f}\")\n",
        "\n",
        "# Visualize comparison\n",
        "results_df_comp = pd.DataFrame(cv_results)\n",
        "results_df_comp = results_df_comp.reindex(columns=sorted(results_df_comp.columns))\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "results_df_comp.boxplot(ax=plt.gca())\n",
        "plt.ylabel('CV Accuracy Score')\n",
        "plt.title('Model Comparison using 5-Fold Cross-Validation')\n",
        "plt.xticks(rotation=45)\n",
        "plt.grid(True, alpha=0.3, axis='y')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Bar plot with error bars\n",
        "means = results_df_comp.mean()\n",
        "stds = results_df_comp.std()\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "x_pos = np.arange(len(means))\n",
        "plt.bar(x_pos, means, yerr=stds, alpha=0.7, capsize=5)\n",
        "plt.xlabel('Model')\n",
        "plt.ylabel('CV Accuracy Score')\n",
        "plt.title('Model Comparison (Mean ± Std)')\n",
        "plt.xticks(x_pos, means.index, rotation=45)\n",
        "plt.grid(True, alpha=0.3, axis='y')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Final evaluation on test set\n",
        "print(\"\\nFinal Test Set Evaluation:\")\n",
        "for name, model in models.items():\n",
        "    if name == 'SVM (RBF)':\n",
        "        model.fit(X_train_comp_scaled, y_train_comp)\n",
        "        test_pred = model.predict(X_test_comp_scaled)\n",
        "    else:\n",
        "        model.fit(X_train_comp, y_train_comp)\n",
        "        test_pred = model.predict(X_test_comp)\n",
        "    test_acc = accuracy_score(y_test_comp, test_pred)\n",
        "    print(f\"{name}: {test_acc:.4f}\")\n",
        "\n",
        "print(\"\\nObservations:\")\n",
        "print(\"- Cross-validation provides fair comparison across models\")\n",
        "print(\"- Consider both mean performance and variance (stability)\")\n",
        "print(\"- Test set should only be used once for final evaluation\")\n",
        "print(\"- Choose model based on performance, interpretability, and computational cost\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
